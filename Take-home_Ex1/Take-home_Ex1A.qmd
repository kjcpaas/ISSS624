---
title: "Take Home Exercise 1A: Data Wrangling"
author: "Kristine Joy Paas"
date: "24 November 2023"
date-modified: "last-modified"
format: html
execute: 
  echo: true
  eval: true
  warning: false
---

# Overview

The aim of this study is to uncover spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.

The main modes of analysis to be used here are Local Indicators of Spatial Association (GLISA) and Emerging Hot Spot Analysis (EHSA).

In doing these study, we will be looking at bus trips started during the hours below.

| Peak hour period             | Bus tap on time |
|------------------------------|-----------------|
| Weekday morning peak         | 6am to 9am      |
| Weekday evening peak         | 5pm to 8pm      |
| Weekend/holiday morning peak | 11am to 2pm     |
| Weekend/holiday evening peak | 4pm to 7pm      |

More details about the study can be found [here](https://isss624-ay2023-24nov.netlify.app/take-home_ex01).

In this part of the study, we will do **data wrangling** on the data sets so that they are transformed into a form that can be used for geovisualization and spatial analysis.

# Setup

## Preparing the data sets

### Geospatial

This data sets are in `shp` format.

-   Bus Stop Locations, available publicly from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html)

### Aspatial

These data sets are in `csv` format.

-   Master Plan 2019 Subzone Boundary (Web), originally from [data.gov.sg](https://data.gov.sg/) but used the one provided on [E-learn](https://elearn.smu.edu.sg/d2l/home/357628).
-   Passenger Volume By Origin Destination Bus Stops from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html) via API (need to [request for access](https://datamall.lta.gov.sg/content/datamall/en/request-for-api.html))
    -   August 2023

    -   September 2023

    -   October 2023 - we will focus on this as the **main data set**

## Preparing the `data/` directory

Before starting our analysis, we have to organize the data sets in a directory.

-   Geospatial data will be located under `data/geospatial`

-   Aspatial data will be located under `data/aspatial`

-   `data/rds` to be created to store data that we can reuse and to make our code reproduceable.

::: {.callout-note collapse="true" appearance="minimal"}
### Show file structure

``` bash
Take-home_Ex1
â””â”€â”€ data
    â”œâ”€â”€ aspatial
    â”‚   â”œâ”€â”€ origin_destination_bus_202308.csv
    â”‚   â”œâ”€â”€ origin_destination_bus_202309.csv
    â”‚   â””â”€â”€ origin_destination_bus_202310.csv
    â”œâ”€â”€ geospatial
    â”‚   â”œâ”€â”€ BusStop.cpg
    â”‚   â”œâ”€â”€ BusStop.dbf
    â”‚   â”œâ”€â”€ BusStop.lyr
    â”‚   â”œâ”€â”€ BusStop.prj
    â”‚   â”œâ”€â”€ BusStop.sbn
    â”‚   â”œâ”€â”€ BusStop.sbx
    â”‚   â”œâ”€â”€ BusStop.shp
    â”‚   â”œâ”€â”€ BusStop.shp.xml
    â”‚   â”œâ”€â”€ BusStop.shx
    â”‚   â”œâ”€â”€ MPSZ-2019.cpg
    â”‚   â”œâ”€â”€ MPSZ-2019.dbf
    â”‚   â”œâ”€â”€ MPSZ-2019.prj
    â”‚   â”œâ”€â”€ MPSZ-2019.qmd
    â”‚   â”œâ”€â”€ MPSZ-2019.shp
    â”‚   â””â”€â”€ MPSZ-2019.shx
    â””â”€â”€ rds
```
:::

## Setting Up the R Environment

After preparing the data sets, we can finally proceed to load the R packages needed for this study.

::: {.callout-note collapse="true" appearance="simple"}
### R packages used

-   [**tmap**](https://cran.r-project.org/web/packages/tmap/index.html): for thematic mapping

-   [**sf**](https://cran.r-project.org/web/packages/sf/index.html): for geospatial data handling

-   [**tidyverse**](https://cran.r-project.org/web/packages/tidyverse/index.html): for non-spatial data handling

-   [**knitr**](https://cran.r-project.org/web/packages/knitr/):for prettifying presentation

-   [**sfdep**](https://sfdep.josiahparry.com/): for spatial analysis
:::

## Environment Settings

We will also set the default settings on for this document

-   `tmap_mode` to **plot**: for plotting simple maps

-   `tmap_style` to **natural**: for my preferred mapping style

-   set **seed** for reproducibility of results

## Running the setup

We will label this code chunk as the setup chunk so the R runs it even after the environment restarts.

```{r}
#| label: setup
pacman::p_load(sf, tmap, tidyverse, knitr, sfdep)
tmap_mode("plot")
tmap_style("natural")
set.seed(1234)
```

# Methodology

After setting up the data sets and the R environment, we can finally proceed with data wrangling.

## Goal data sets

To enable the visualization and analysis in latter part of the study, we need to have the following data sets:

-   **Honeycomb** geometry, a tessellation of hexagons covering the bus stops in Singapor

-   **Hourly bus trips started** from each hexagon cell

    -   1 for weekend, 1 for weekend/holidays

    -   Required columns: `HEX_ID`, `HOUR_OF_DAY`, `TRIPS`

    -   Must contain geometry of the hexagon

    -   Can be used to generate a time series cube

As the wrangling process is expected to have a lot of intermediate steps, **Save**, **Load**, and **Data clear** points are available to make our data wrangling more efficient.

::: callout-tip
## Save point

This is where data is written as `rds` files using `write_rds()` for important data sets that will be used in later analysis. Examples are:

-   The end goal of data wrangling: **Hourly bus trips started** **from each hexagon cell** data sets

-   Critical outputs of expensive calculations
:::

::: callout-note
## Load point

This is where data is loaded from `rds` files using `read_rds()`. They were previously generated by the save point.

**TIP**: Skip to the load points to progress without running the code above it
:::

::: callout-warning
## Data clear point

This is where data that will not be used anymore are cleared. The data in RStudio environment will pile up and set `#| eval: false` in code chunks if you want skip the clearing. For example, the code below won't be run.

```{r}
#| eval: false
message <- "This code chunk executed"
```
:::

# Generating hexagons from *BusStop* data

As per the specifications of this study, we must use a honeycomb grid, a tesselation of hexagons to replace the `mpsz` data set.

::: callout-tip
## Why hexagons?

Some benefits of using a hexagons are:

-   A hexagon is the polygon with the most number of sides that can tessellate (or tile). Hence it is the most "circular" of the polygons that can be tessellated.

-   Distances of the centroid from one hexagon to the next are consistent all around the hexagon, making it easy to find neighbors.

More information about hexagons in the context of spatial analysis can be found in <https://desktop.arcgis.com/en/arcmap/latest/tools/spatial-statistics-toolbox/h-whyhexagons.htm>
:::

## Importing the Singapore boundary data

We will use the **Master Plan 2019 Subzone Boundary (Web)** data set that has been used in class. This is a `shp` file, that we will import by using `st_read()`. We will use this to **ensure that the bus stops are within Singapore.**

```{r}
mpsz <- st_read(dsn = "data/geospatial",
                    layer = "MPSZ-2019")
```

::: {.callout-caution appearance="simple"}
### Correcting the projection

This data frame using the global GPS standard projection, [**WGS84**](https://gisgeography.com/wgs84-world-geodetic-system/). We need to convert this to [**SVY21**](https://app.sla.gov.sg/sirent/About/PlaneCoordinateSystem) that is more appropriate for Singapore ðŸ‡¸ðŸ‡¬ context.

```{r}
mpsz <- mpsz %>% st_transform(crs=3414)
```
:::

Now that the data frame has the correct projection, let's do a quick plot to visually check if we have the boundaries that we need.

```{r}
#| code-fold: true
#| code-summary: "**Show the code**"
tmap_style("natural")
tm_shape(mpsz) +
  tm_fill("lightgreen", title = "Singapore Boundary") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Map of Singapore",
            main.title.position = "center",
            main.title.size = 1.0,
            legend.height = 0.35, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha = 0.2)
```

## Importing the BusStop data set

The `BusStop` data set is a in `shp` format. We can import it by using `st_read()` from the `sf` package.

```{r}
busstops <- st_read(dsn = "data/geospatial",
                    layer = "BusStop")
```

::: {.callout-caution collapse="true" appearance="simple"}
### Correcting the projection

We want to use [SVY21](https://app.sla.gov.sg/sirent/About/PlaneCoordinateSystem) as the projection for this study as it is the projection used for local Singaporean context.

After the import, it shows that the **Projected CRS**is **SVY21**. However, checking the CRS with `st_crs()` tells a different story.

```{r}
st_crs(busstops)
```

As we can see EPSG value is **9001**, which correspond to [**WGS84**](https://gisgeography.com/wgs84-world-geodetic-system/). We have to fix the projection by transforming to EPSG value of **3414**, which corresponds to **SVY21**.

```{r}
busstops <- st_transform(busstops, crs = 3414)
```
:::

Next, let's take a look at the available columns to identify which columns we can use for analysis.

```{r}
kable(head(busstops))
```

::: callout-note
From this initial look in the data, **`BUS_STOP_N`** and **`LOC_DESC`** can potentially be used to match records in the passenger volume data set.
:::

## Generating hexagons from Singapore boundary data

Following the steps from <https://urbandatapalette.com/post/2021-08-tessellation-sf/>, we will use [st_make_grid()](https://search.r-project.org/CRAN/refmans/sf/html/st_make_grid.html) to generate the hexagons for analysis.

We need to provide a value for `cellsize` in the function, which is defined as *"for hexagonal cells the distance between opposite edges"*. We need to create hexagons whose apothem is **250m**, resulting in a cell size of **500m**.

::: {.callout-tip collapse="true" appearance="simple"}
### Why is cell size 500 m?

[Apothem](https://www.merriam-webster.com/dictionary/apothem) is defined as *the perpendicular from the center of a regular polygon to one of the sides**.***

The specification is this study requires hexagons to be **250 m** from the center of the hexagon to the center of one of it's edge.

![](images/apothem.png){fig-align="center"}

As such, this corresponds to the length of 2 opposite apothems, which is **500 m.**

The edge length is **not** the same as apothem. It is **288.675**m.

$$
250m/cos(30) = 288.675m
$$
:::

We will use the `mpsz` data to ensure that the honeycomb grid perfectly covers the Singapore boundaries

```{r}
honeycomb <-
  st_make_grid(mpsz,
               cellsize = 500,
               what = "polygon",
               square = FALSE) %>%
  st_sf()
```

::: {.callout-caution appearance="minimal"}
We have to use `st_sf()` to convert the result to a data frame that can be used for the succeeding steps.
:::

Checking the generated hexagons reveals that it covers all the bus stops.

```{r}
#| code-fold: true
#| code-summary: "**Show the code**"
tm_shape(honeycomb) +
  tm_fill(col = "white", title = "Hexagons") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Singapore with honeycomb grid",
            main.title.position = "center",
            main.title.size = 1.0,
            legend.height = 0.35, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="8star", size = 2, bg.color = "white", bg.alpha = 0.5) +
  tm_scale_bar(bg.color = "white", bg.alpha = 0.5) +
  tm_shape(mpsz) +
  tm_fill("green", title = "Singapore Boundary", alpha = 0.5) +
  tm_shape(busstops) +
  tm_dots(col = "red", size = 0.005, title = "Bus Stops")
```

::: {.callout-tip appearance="minimal"}
Checking the scale reveals that the generated hexagons are of the expected size, **500 m from one edge to the opposite edge** as there are **10 hexagons within a 5 km** **distance**.
:::

::: {.callout-important appearance="simple" collapse="true"}
### About those points outside Singapore

The map shows that there are bus stops in our data set that our outside Singapore bounds (green area). We can remove these points from our `busstops` data by following the filtering steps from <https://urbandatapalette.com/post/2021-08-tessellation-sf/>.

We will `st_intersects()` to see which points in `busstops` intersect with `mpsz`, and filter those that intersect.

```{r}
busstops$n_collisions = lengths(st_intersects(busstops, mpsz))
busstops <-
  filter(busstops, n_collisions > 0) %>%
  select(, -n_collisions) # Remove n_collisions as we do not need it anymore
```

Plotting again shows that all bus stops are now within Singapore bounds.

```{r}
#| code-fold: true
#| code-summary: "**Show the code**"
tm_shape(honeycomb) +
  tm_fill(col = "white", title = "Hexagons") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Honeycomb grid without bus stops outside of Singapore",
            main.title.position = "center",
            main.title.size = 1.0,
            legend.height = 0.35, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="8star", size = 2, bg.color = "white", bg.alpha = 0.5) +
  tm_scale_bar(bg.color = "white", bg.alpha = 0.5) +
  tm_shape(mpsz) +
  tm_fill("green", title = "Singapore Boundary", alpha = 0.5) +
  tm_shape(busstops) +
  tm_dots(col = "red", size = 0.005, title = "Bus Stops")
```
:::

## Filtering hexagons with bus stops

The honeycomb grid generated from [Generating hexagons from Singapore boundary data] need to be filtered such that the hexagons remaining correspond to only those with bus stops.

We can do this by following the filtering steps from <https://urbandatapalette.com/post/2021-08-tessellation-sf/>. We will use `st_intersects()` to identify **which hexagons intersect with bus stop locations**.

```{r}
honeycomb$n_collisions = lengths(st_intersects(honeycomb, busstops))
honeycomb <- filter(honeycomb, n_collisions > 0)
```

Let's generate the map again to check if we have the hexagons that correspond to bus stop locations.

```{r}
#| code-fold: true
#| code-summary: "**Show the code**"
tm_shape(mpsz) +
  tm_fill("green", title = "Singapore Boundary", alpha = 0.5) +
  tm_shape(honeycomb) +
  tm_fill(col = "white", title = "Hexagons", alpha = 1) +
  tm_borders(alpha = 0.2) +
  tm_layout(main.title = "Honeycomb grid corresponding to Singapore bus stops",
            main.title.position = "center",
            main.title.size = 1.0,
            legend.height = 0.35, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="8star", size = 2, bg.color = "white", bg.alpha = 0.5) +
  tm_scale_bar(bg.color = "white", bg.alpha = 0.5) +
  tm_shape(busstops) +
  tm_dots(col = "red", size = 0.001, title = "Bus Stops") +
  tm_grid(alpha = 0.2)
```

## Assigning ids to each hexagon

Here is the structure of our honeycomb data:

```{r}
kable(head(honeycomb, n=3))
```

::: {.callout-caution collapse="true" appearance="simple"}
### Remove n-collisions

We do not need `n-collisions` anymore so we can remove it.

```{r}
honeycomb <- honeycomb %>% select(, -n_collisions)
```
:::

This data is still incomplete as we need to associate the hexagons to aspatial data, which is critical to the next steps in our data wrangling.

For this purpose, we will assign `HEX_ID` with format `H0000`.

```{r}
honeycomb$HEX_ID = sprintf("H%04d", seq_len(nrow(honeycomb)))
kable(head(honeycomb))
```

::: {.callout-tip collapse="true"}
## Save point

`honeycomb` is the **geometry** that we will use for analysis. It will be used for tasks such as identifying neighbors and calculating spatial weights.

Is it also one of the [Goal data sets] we need. Hence, we will save it.

```{r}
write_rds(honeycomb, "data/rds/honeycomb202310.rds")
```
:::

::: {.callout-warning collapse="true"}
## Data clear point

We do not need `mpsz` anymore as we have generated hexagons already. For further analysis, we will overlay the hexagons to the Singapore map with `tmap_mode("plot")` to use interactive maps for closer inspection.

```{r}
#| eval: true
rm(mpsz)
```
:::

# Extracting Hourly \# of Bus Trips Originating from Hexagons {#extracting-hourly-of-bus-trips-originating-from-hexagons}

The goal for this part of data wrangling is to have information on **how many trips started from each hexagon for every given hour of the day**.

We will use the *Passenger Volume By Origin Destination Bus Stops* from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html) via API for the months of **August, September, October 2023**.

::: {.callout-note appearance="simple"}
For demonstrating the steps, we will use the **October 2023** data set. The same steps will be applied to the other data sets later on.

If you want to run the code for **August 2023** and **September 2023**, replace *202310*, with *202308* or *202309**.*** Our code can be used to analyze this dataset from any month.
:::

## Importing the data set

The data set is an aspatial data in `csv` format so we will use `read_csv()` to import the data.

```{r}
odbus <- read_csv("data/aspatial/origin_destination_bus_202310.csv")
kable(head(odbus))
```

::: {.callout-note appearance="simple"}
The relevant columns for our data study are **`DAY_TYPE`**, **`TIME_PER_HOUR`**, **`ORIGIN_PT_CODE`**, `TOTAL_TRIPS`

We do not need the **`DESTINATION_PT_CODE`** as we are only interested on **when passengers get on the bus**.

Furthermore, the **`ORIGIN_PT_CODE`** can be correlated to the `BUS_STOP_N` column of `busstops` data.
:::

::: {.callout-note collapse="true" appearance="simple"}
### Recap of `busstops` data

```{r}
kable(head(busstops))
```
:::

## Cleaning the data

Before going deep in the wrangling, we will clean up the data so that we are left with a lightweight data set that R can process more easily. We will retain and rename columns below to make them more understandable and easier to join with other data sets.

-   `DAY_TYPE`

-   `TIME_PER_HOUR` -\> `HOUR_OF_DAY`

-   `ORIGIN_PT_CODE` -\> `BUS_STOP_N`

-   `TOTAL_TRIPS` -\> `TRIPS`

We will also rename the columns to make them more understandable and will make joining with other data sets easier.

```{r}
trips <- odbus %>%
  select(c(ORIGIN_PT_CODE, DAY_TYPE, TIME_PER_HOUR, TOTAL_TRIPS)) %>%
  rename(BUS_STOP_N = ORIGIN_PT_CODE) %>%
  rename(HOUR_OF_DAY = TIME_PER_HOUR) %>%
  rename(TRIPS = TOTAL_TRIPS)
kable(head(trips))
```

::: {.callout-note appearance="minimal"}
`select()` is used to select the columns we need.

`rename()` is used to rename the columns.
:::

::: {.callout-warning collapse="true"}
### Data clear point

We do not need `odbus` anymore as we will be working with the more lightweight `trips` from this point on.

```{r}
#| eval: true
rm(odbus)
```
:::

## Mapping the bus stops to hexagon

In [Filtering hexagons with bus stops] we were able to overlay the bus locations to our generated hexagon. While this is enough for visualization, it is **not enough for the rest of the data processing** we need.

From [Cleaning the data], we have the `BUS_STOP_N` in the that we can use to associate with `busstops`.

We need to create an **aspatial** table that contain `BUS_STOP_N` and `HEX_ID` of the hexagon containing them. We will use `st_intersection()`.

::: {.callout-tip collapse="true" appearance="simple"}
### Why aspatial?

We want to use generate a simple mapping here as this table will serve as a "glue" between the other aspatial data sets and our geospatial data, `honeycomb`.
:::

```{r}
bs_hex <- st_intersection(busstops, honeycomb) %>%
  st_drop_geometry() %>%
  select(c(BUS_STOP_N, HEX_ID))
kable(head(bs_hex))
```

::: {.callout-note appearance="minimal"}
`st_intersection()` - find which hexagon contains the bus stop

`st_drop_geometry()` - to make data aspatial

`select()` - to retain only the relevant columns: `BUS_STOP_N` and `HEX_ID`
:::

## Adding `HEX_ID` information to bus `trips` data

To achieve our goal of having the hourly \# of bus trips per location, we need to add `HEX_ID` to `trips` data. This is so we can answer, *how many bus trip originate from a certain hexagon?*

To do this, we will do an `inner_join()` to join the `trips` data with `bs_hex`.

::: {.callout-tip collapse="true"}
### Why \`inner_join()\` instead of \`left_join()\`?

We will use `inner_join` as there are `BUS_STOP_N` values in `trips` data that are not in `bs_hex`.

```{r}
trips$BUS_STOP_N[!(trips$BUS_STOP_N %in% bs_hex$BUS_STOP_N)] %>%
  unique() %>% length()
```

There are **57** bus stops in `trips` that are not in `bs_hex`. **5** of this can be attributed the bus stops we removed in [About those points outside Singapore]. Others may be due to the *BusStops* data set not having complete information.

Nonetheless, we have to **remove** these bus stops from our analysis as **we do not have geospatial data** to associate to the hexagons.

Therefore, we will use `inner_join` to keep only the observations in `trips` with the matching bus stops in `bs_hex`.
:::

```{r}
trips <- inner_join(trips, bs_hex)
kable(head(trips))
```

## Aggregating `TRIPS` based on `HEX_ID`

Next, we will add the `TRIPS` for all the bus stops within a hexagon. We will group via `HEX_ID`, `DAY_TYPE`, and `HOUR_OF_DAY`.

```{r}
trips <- trips %>%
  group_by(
    HEX_ID,
    DAY_TYPE,
    HOUR_OF_DAY) %>%
  summarise(TRIPS = sum(TRIPS))
kable(head(trips))
```

::: {.callout-tip collapse="true"}
## Save point

`trips` was processed from *Passenger Volume By Origin Destination Bus Stops*, which has almost **6 million** observations.

We now have a more lightweight dataset with almost **60,000** observations, which is about **100x smaller**.

Let's save this data as an `rds` file so we don't need to reprocess again later on.

```{r}
write_rds(trips, "data/rds/trips202310.rds")
```
:::

# Generating time series cube-friendly data

::: {.callout-note collapse="true"}
## Load point

We can run the rest of the document from this point by loading this data.

```{r}
trips <- read_rds("data/rds/trips202310.rds")
honeycomb <- read_rds("data/rds/honeycomb202310.rds")
```
:::

When doing **Emerging Hotspot Analysis** (EHSA), we need to create a time series cube. To do that we must pass the following criteria:

-   It must have a row for each combination of `HEX_ID` (location) and `HOUR_OF_DAY` (time)

-   There are **no missing** values in `TRIPS` column

::: {.callout-important collapse="true" appearance="simple"}
## Is our *trips* **data time series cube-friendly? The answer is NO.**

```{r}
spacetime(trips, honeycomb,
          .loc_col = "HEX_ID",
          .time_col = "HOUR_OF_DAY") %>%
  is_spacetime_cube()
```

We do not pass the first requirement for generating a time series cube

> It must have a row for each combination of `HEX_ID` (location) and `HOUR_OF_DAY` (time)
:::

## Generating the combinations

::: {.callout-tip collapse="true" appearance="simple"}
### How many combinations are there? The answer is 36,456.

To satisfy the requirement of:

> It must have a row for each combination of `HEX_ID` (location) and `HOUR_OF_DAY` (time)

We need to find out how many such combinations exist.

-   There are **1519** **hexagons** in our `honeycomb`

-   There are **24 hours** in a day

Therefore, there are $1519 \times 24 = 36,456$ combinations. We will use this value to verify if we have the correct space time cube.
:::

To generate the combinations, we will use `expand.grid()` and for us to provide the list possible values for `HEX_ID` and `HOUR_OF_DAY`.

```{r}
combos <- expand_grid(
  HEX_ID = honeycomb$HEX_ID,
  HOUR_OF_DAY = 0:23
)
kable(combos[20:29,])
```

`combos` also has **36,456** rows, aligned with our expectations.

```{r}
nrow(combos)
```

With this generated, we can use this as a glue to generate our time series cube.

## Splitting the data

As we want to do separate analysis for weekdays and weekends, we will split the data. We will also remove the `DAY_TYPE` column as we do not need it anymore. To do this, we have to `ungroup()` before removing as we use `DAY_TYPE` as filter.

::: panel-tabset
### Weekday

```{r}
trips_wkdy <- trips %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  ungroup() %>%
  select(, -DAY_TYPE)
kable(trips_wkdy[20:29,])
```

### Weekend/Holidays

```{r}
trips_wknd <- trips %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  ungroup() %>%
  select(, -DAY_TYPE)
kable(trips_wknd[20:29,])
```
:::

::: {.callout-note collapse="true"}
### Checking if split covers the full data

Let's check the total rows in `trips_wkdy` and `trips_wknd` add up to the number of rows in `trips`.

```{r}
nrow(trips_wkdy) + nrow(trips_wknd) == nrow(trips)
```

There are no lost data so we can proceed to the next step.
:::

::: {.callout-warning collapse="true"}
### Data clear point

We do not need `trips` anymore as we will be using `trips_wkdy` and `trips_wknd` from this point.

```{r}
#| eval: true
rm(trips)
```
:::

## Filling in the all the combos

Now that we have separate data frames for weekday and weekend/holiday, we need to make sure that our data frame as all the combination in `combos`. We can do that by joining `trips_wkxx` with `combos`.

::: panel-tabset
### Weekday

```{r}
trips_cube_wkdy <- left_join(combos, trips_wkdy)
kable(head(trips_cube_wkdy, n = 24))
```

Check if the output has the same rows as `combos`.

```{r}
nrow(trips_cube_wkdy) == nrow(combos)
```

### Weekend/Holiday

```{r}
trips_cube_wknd <- left_join(combos, trips_wknd)
kable(head(trips_cube_wknd, n = 24))
```

Check if the output has the same rows as `combos`.

```{r}
nrow(trips_cube_wknd) == nrow(combos)
```
:::

::: {.callout-tip appearance="simple"}
The data frames generated now passes

> It must have a row for each combination of `HEX_ID` (location) and `HOUR_OF_DAY` (time)
:::

::: {.callout-important appearance="simple"}
The data frames generated violate

> There are **no missing** values in `TRIPS` column

This is because there are some `HOUR_OF_DAY` where the value of `TRIPS` is `NA`. We need to fill in these missing values.
:::

::: {.callout-warning collapse="true"}
### Data clear point

We do not need `trips_wkxx` anymore as we will be using `trips_cube_wkxx` from this point on.

```{r}
rm(trips_wkdy)
rm(trips_wknd)
```
:::

## Filling in missing values

Lastly, we need to fill in the missing values in `TRIPS`. This can be done by filtering the rows with `NA` and setting those to **0**.

::: panel-tabset
### Weekday

```{r}
trips_cube_wkdy$TRIPS[is.na(trips_cube_wkdy$TRIPS)] <- 0
kable(head(trips_cube_wkdy, n = 24))
```

### Weekend/Holiday

```{r}
trips_cube_wknd$TRIPS[is.na(trips_cube_wknd$TRIPS)] <- 0
kable(head(trips_cube_wknd, n = 24))
```
:::

::: {.callout-tip collapse="true" appearance="simple"}
### Is our data frame **time series cube-friendly? The answer is YES.**

Let us check of our data frame can be used to create spacetime cubes.

#### Weekend

```{r}
spacetime(trips_cube_wkdy, honeycomb,
          .loc_col = "HEX_ID",
          .time_col = "HOUR_OF_DAY") %>%
  is_spacetime_cube()
```

#### Weekend/Holiday

```{r}
spacetime(trips_cube_wknd, honeycomb,
          .loc_col = "HEX_ID",
          .time_col = "HOUR_OF_DAY") %>%
  is_spacetime_cube()
```
:::

::: {.callout-tip collapse="true"}
### Save point

`trips_cube_wkxx` data is part the [Goal data sets] we need. Hence, we will save them.

```{r}
write_rds(trips_cube_wkdy, "data/rds/trips_cube_wkdy202310.rds")
write_rds(trips_cube_wknd, "data/rds/trips_cube_wknd202310.rds")
```
:::
