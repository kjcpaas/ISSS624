---
title: "Take Home Exercise 1: Geospatial Analysis for the Public Good"
author: "Kristine Joy Paas"
date: "24 November 2023"
date-modified: "last-modified"
format: html
execute: 
  echo: true
  eval: true
  warning: false
---

# Overview

The aim of this study is to uncover spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.

The main modes of analysis to be used here are Local Indicators of Spatial Association (GLISA) and Emerging Hot Spot Analysis (EHSA).

In doing these study, we will be looking at bus trips started during the hours below.

| Peak hour period             | Bus tap on time |
|------------------------------|-----------------|
| Weekday morning peak         | 6am to 9am      |
| Weekday afternoon peak       | 5pm to 8pm      |
| Weekend/holiday morning peak | 11am to 2pm     |
| Weekend/holiday evening peak | 4pm to 7pm      |

More details about the study can be found [here](https://isss624-ay2023-24nov.netlify.app/take-home_ex01).

# Setup

## Preparing the data sets

### Geospatial

These data sets are in `shp` format.

-   Master Plan 2019 Subzone Boundary (Web), originally from [data.gov.sg](https://data.gov.sg/) but used the one provided on [E-learn](https://elearn.smu.edu.sg/d2l/home/357628).

-   Bus Stop Locations, available publicly from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html)

### Aspatial

These data sets are in `csv` format.

-   Passenger Volume By Origin Destination Bus Stops from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html) via API (need to [request for access](https://datamall.lta.gov.sg/content/datamall/en/request-for-api.html))

    -   August 2023

    -   September 2023

    -   October 2023 - we will focus on this as the **main data set**

## Preparing the `data/` directory

Before starting our analysis, we have to organize the data sets in a directory.

-   Geospatial data will be located under `data/geospatial`

-   Aspatial data will be located under `data/aspatial`

-   `data/rds` to be created to store data that we can reuse and to make our code reproduceable.

Finally, we are left with the following file structure:

``` bash
Take-home_Ex1
â”œâ”€â”€ Take-home_Ex1.qmd
â””â”€â”€ data
    â”œâ”€â”€ aspatial
    â”‚   â”œâ”€â”€ origin_destination_bus_202308.csv
    â”‚   â”œâ”€â”€ origin_destination_bus_202309.csv
    â”‚   â””â”€â”€ origin_destination_bus_202310.csv
    â”œâ”€â”€ geospatial
    â”‚   â”œâ”€â”€ BusStop.cpg
    â”‚   â”œâ”€â”€ BusStop.dbf
    â”‚   â”œâ”€â”€ BusStop.lyr
    â”‚   â”œâ”€â”€ BusStop.prj
    â”‚   â”œâ”€â”€ BusStop.sbn
    â”‚   â”œâ”€â”€ BusStop.sbx
    â”‚   â”œâ”€â”€ BusStop.shp
    â”‚   â”œâ”€â”€ BusStop.shp.xml
    â”‚   â”œâ”€â”€ BusStop.shx
    â”‚   â”œâ”€â”€ MPSZ-2019.cpg
    â”‚   â”œâ”€â”€ MPSZ-2019.dbf
    â”‚   â”œâ”€â”€ MPSZ-2019.prj
    â”‚   â”œâ”€â”€ MPSZ-2019.qmd
    â”‚   â”œâ”€â”€ MPSZ-2019.shp
    â”‚   â””â”€â”€ MPSZ-2019.shx
    â””â”€â”€ rds
```

## Setting Up the R Environment

After preparing the data sets, we can finally proceed to load the R packages needed for this study.

-   [**tmap**](https://cran.r-project.org/web/packages/tmap/index.html): for thematic mapping

-   [**sf**](https://cran.r-project.org/web/packages/sf/index.html): for geospatial data handling

-   [**tidyverse**](https://cran.r-project.org/web/packages/tidyverse/index.html): for non-spatial data handling

-   [**sfdep**](https://sfdep.josiahparry.com/): for spatial analysis

```{r}
pacman::p_load(sf, spdep, tmap, tidyverse)
```

# Data Wrangling

After setting up the data sets and the R environment, we can finally proceed with data wrangling.

## Importing data into R environment

### BusStop data set

The `BusStop` data set is a in `shp` format. We can import it by using `st_read()` from the `sf` package.

```{r}
busstops <- st_read(dsn = "data/geospatial",
                    layer = "BusStop")
```

::: callout-tip
As this already uses [**SVY21**](https://app.sla.gov.sg/sirent/About/PlaneCoordinateSystem) datum that is appropriate for Singapore ðŸ‡¸ðŸ‡¬ local context, we can proceed with using data on its own.
:::

Next, let's take a look at the available columns to identify which columns we can use for analysis. We will decide this later after looking at other data sets.

```{r}
glimpse(busstops)
```

Lastly, let's do a quick plot to see a visual glimpse of the data.

```{r}
plot(busstops['BUS_STOP_N'])
```

This is not enough information to do the analysis as we are missing information on which regions of Singapore ðŸ‡¸ðŸ‡¬ the bus stops are located.

### Singapore boundary data

To visualize where the bus stops in Singapore ðŸ‡¸ðŸ‡¬, we need data that shows boundaries covering the country. We will use the **Master Plan 2019 Subzone Boundary (Web)** data set that has been used in class.

As this is also a `shp` file, we will import it the same way as in [BusStop data set].

```{r}
mpsz <- st_read(dsn = "data/geospatial",
                    layer = "MPSZ-2019")
```

::: callout-important
This data frame using the global GPS standard projection, [**WGS84**](https://gisgeography.com/wgs84-world-geodetic-system/). We need to convert this to [**SVY21**](https://app.sla.gov.sg/sirent/About/PlaneCoordinateSystem) that is more appropriate for Singapore ðŸ‡¸ðŸ‡¬ context, and for consistency with the bus stop data.
:::

```{r}
mpsz <- mpsz %>% st_transform(crs=3414)
head(mpsz)
```

Let's do a quick mapping of the boundary map and the bus stops for a visual check.

```{r}
tmap_style("natural")
tm_shape(mpsz) +
  tm_fill("lightgreen", title = "Singapore Boundary") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Bus Stop Locations in Singapore",
            main.title.position = "center",
            main.title.size = 1.0,
            legend.height = 0.35, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2) +
  tm_shape(busstops) +
  tm_dots(col = "red", size = 0.001, title = "Bus Stops")
```

::: callout-important
Some bus stops on the North are outside of the border. These are the bus stops that for bus routes (e.g. CWx, SJE) the cross the border to **Johor Bahru** in Malaysia ðŸ‡²ðŸ‡¾
:::

### Bus commuter data

Finally, we import the bus commuter data. We will use the **Passenger Volume By Origin Destination Bus Stops** data sets to provide data about bus commuter volumes.

These files are in `csv` file format so we will use `read_csv` to import them.

::: callout-important
We aim to analyze data for 3 months. However, we will focus on the **September 2023** for now to simplify the steps.
:::

```{r}
odbus202310 <- read_csv("data/aspatial/origin_destination_bus_202310.csv")
glimpse(odbus202310)
```

## Agregating bus trips based on origin

*Revise to reduce the data size and to reduce noise.*

## Extracting bus trips started during peak areas

::: callout-important
For the **odbus202310** data set, the `TIME_PER_HOUR` column contains the **hour of day (in 24-hr format)** when the trip was started.

Hence, if `TIME_PER_HOUR` is 18, it means that the trip was started between 6pm to just before 7pm.

We will keep this in mind when extracting the data on this part.
:::

### Weekday morning peak (6 to 9 am)

As `ORIGIN_PT_CODE` and `DESTINATION_PT_CODE` contain a discrete set of values, we can convert them to `factor` data type to make it easier to work with.

We are only interested in the origin bus stops so we will only convert that.

```{r}
odbus202310$ORIGIN_PT_CODE <- as.factor(odbus202310$ORIGIN_PT_CODE)
```

Next, we will filter and aggregate the records using the following functions:

-   `filter()` - to filter out udesired records

-   `group_by()` - to group based on a column value

-   `summarise()` - to aggregate values of elements within the group

-   `unique` - to ensure that there are no duplicates

```{r}
odbus202310_weekday6_9 <- odbus202310 %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 6 &
           TIME_PER_HOUR < 9) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS)) %>%
  unique()
glimpse(odbus202310_weekday6_9)
```

::: callout-tip
Let's save this data set for later use:

```{r}
write_rds(odbus202310_weekday6_9, "data/rds/odbus202310_weekday6_9.rds")
```
:::

We will do the same method for the rest of the peak times.

### Weekday afternoon peak (5 to 8 pm)

```{r}
odbus202310_weekday17_20 <- odbus202310 %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 17 &
           TIME_PER_HOUR < 20) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS)) %>%
  unique()
glimpse(odbus202310_weekday17_20)
```

::: callout-tip
Let's save this data set for later use:

```{r}
write_rds(odbus202310_weekday17_20, "data/rds/odbus202310_weekday17_20.rds")
```
:::

### Weekend/Holiday morning peak (11 am to 2 pm)

```{r}
odbus202310_weekend11_14 <- odbus202310 %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 11 &
           TIME_PER_HOUR < 14) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS)) %>%
  unique()
glimpse(odbus202310_weekend11_14)
```

::: callout-tip
Let's save this data set for later use:

```{r}
write_rds(odbus202310_weekend11_14, "data/rds/odbus202310_weekend11_14.rds")
```
:::

### Weekend/Holiday evening peak (4 to 7 pm)

```{r}
odbus202310_weekend16_19 <- odbus202310 %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 16 &
           TIME_PER_HOUR < 19) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS)) %>%
  unique()
glimpse(odbus202310_weekend16_19)
```

::: callout-tip
Let's save this data set for later use:

```{r}
write_rds(odbus202310_weekend16_19, "data/rds/odbus202310_weekend16_19.rds")
```
:::

## Combining them all together

::: callout-tip
Run the code chunk below if you want to replay the code chunks from this part.

```{r}
odbus202310_weekday6_9 <- read_rds("data/rds/odbus202310_weekday6_9.rds")
odbus202310_weekday17_20 <- read_rds("data/rds/odbus202310_weekday17_20.rds")
odbus202310_weekend11_14 <- read_rds("data/rds/odbus202310_weekend11_14.rds")
odbus202310_weekend16_19 <- read_rds("data/rds/odbus202310_weekend16_19.rds")
```
:::

### Adding geometry to the data

The extracted bus commuter data has no geometry so we have no way to plot it in a map as it is now. We can do that by joining it with `busstops` data. We do this by doing a `left_join` on `busstops.BUS_STOP_N` = `odbus202310_weekday6_9.ORIGIN_PT_CODE`.

We also only need the `BUS_STOP_N` (to be renamed to `ORIGIN_BS`) and `TRIPS` so we will select only those.

```{r}
odbus202310_weekday6_9_sf <- left_join(busstops, odbus202310_weekday6_9,
                                    by = c("BUS_STOP_N" = "ORIGIN_PT_CODE")) %>%
  select(c(1,4)) %>%
  rename(ORIGIN_BS = BUS_STOP_N)
glimpse(odbus202310_weekday6_9_sf)
```

### Setting the correct values

Upon inspection, we see that there are `NA` values in `TRIPS`. This is because the number of rows of `busstops` do not necessarily match the number of rows in the `odbus` data frames. This makes sense as some **bus stops may not have passengers** in those times.

We will correct this by filtering the rows will `NA` values and setting the values to 0.

```{r}
odbus202310_weekday6_9_sf$TRIPS[is.na(odbus202310_weekday6_9_sf$TRIPS)] <- 0
glimpse(odbus202310_weekday6_9_sf)
```

After this correction, the `NA` is gone. Let's save this in an `rds` file for reuse later.

```{r}
write_rds(odbus202310_weekday6_9_sf, "data/rds/odbus202310_weekday6_9_sf.rds")
```

### Applying to the other data sets

We will do the same methods to do data wrangling on the rest of the `odbus` data frames.

```{r}
odbus202310_weekday17_20_sf <- left_join(busstops, odbus202310_weekday17_20,
                                    by = c("BUS_STOP_N" = "ORIGIN_PT_CODE")) %>%
  select(c(1,4)) %>%
  rename(ORIGIN_BS = BUS_STOP_N)

odbus202310_weekday17_20_sf$TRIPS[is.na(odbus202310_weekday17_20_sf$TRIPS)] <- 0
write_rds(odbus202310_weekday17_20_sf, "data/rds/odbus202310_weekday17_20_sf.rds")
```

```{r}
odbus202310_weekend11_14_sf <- left_join(busstops, odbus202310_weekend11_14,
                                    by = c("BUS_STOP_N" = "ORIGIN_PT_CODE")) %>%
  select(c(1,4)) %>%
  rename(ORIGIN_BS = BUS_STOP_N)

odbus202310_weekend11_14_sf$TRIPS[is.na(odbus202310_weekend11_14_sf$TRIPS)] <- 0
write_rds(odbus202310_weekend11_14_sf, "data/rds/odbus202310_weekend11_14_sf.rds")
```

```{r}
odbus202310_weekend16_19_sf <- left_join(busstops, odbus202310_weekend16_19,
                                    by = c("BUS_STOP_N" = "ORIGIN_PT_CODE")) %>%
  select(c(1,4)) %>%
  rename(ORIGIN_BS = BUS_STOP_N)

odbus202310_weekend16_19_sf$TRIPS[is.na(odbus202310_weekend16_19_sf$TRIPS)] <- 0
write_rds(odbus202310_weekend16_19_sf, "data/rds/odbus202310_weekend16_19_sf.rds")
```

::: callout-caution
We don't need the filtered aspatial `odbus` datasets anymore so we can remove them from the environment. This is to make it easier to inspect the rest of the data we will use later on.

```{r}
rm(
  odbus202310_weekday6_9,
  odbus202310_weekday17_20,
  odbus202310_weekend11_14,
  odbus202310_weekend16_19
  )
```
:::

# Generating a honeycomb grid

A honeycomb grid contains a grid of tessellated or tiled **hexagons**.

In this part, we will generate a honeycomb grid that covers the Singapore ðŸ‡¸ðŸ‡¬ map. We will use the `mpsz` data set to provide the boundaries.

## Why hexagons?

Some benefits of using a hexagons are:

-   A hexagon is the polygon with the most number of sides that can tessellate (or tile). Hence it is the most "circular" of the polygons that can be tessellated.

-   Distances of the centroid from one hexagon to the next are consistent all around the hexagon, making it easy to find neighbors.

More information about hexagons in the context of spatial analysis can be found in <https://desktop.arcgis.com/en/arcmap/latest/tools/spatial-statistics-toolbox/h-whyhexagons.htm>

::: callout-tip
As in the map in [Singapore boundary data], the subzones have different shapes and sizes. The analysis will benefit from using a consistently-shaped regions because our analysis requires a lot of neighbor calculations.
:::

## Generating hexagons

For this study, we will create hexagons with [apothem](https://www.merriam-webster.com/dictionary/apothem) of **250m.** This is the distance of the line segment from the **center** to the **midpoint of the** **edge**.

::: callout-tip
The edge length is **288.675**m.

$$
250m/cos(30) = 288.675m
$$
:::

Following the steps on <https://urbandatapalette.com/post/2021-08-tessellation-sf/>, we will use [st_make_grid()](https://search.r-project.org/CRAN/refmans/sf/html/st_make_grid.html) to generate the hexagons for analysis.

We need to provide a value for `cellsize` in the function, which is defined as *"for hexagonal cells the distance between opposite edges"*.

This is equivalent to $2 \times L_{apothem} = 2 \times 250m$, which is **500m**.

```{r}
sg_honeycomb <- st_make_grid(mpsz,
                       cellsize = 500,
                       what = "polygon",
                       square = FALSE) %>%
  st_sf()
```

::: callout-important
We have to apply \`st_sf()\` to convert the result to a data frame that can be used for the succeeding steps.
:::

Let's check if the honeycomb grid fits with Singapore.

```{r}
tm_shape(sg_honeycomb) +
  tm_fill(col = "white", title = "Hexagons") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Singapore with honeycomb grid",
            main.title.position = "center",
            main.title.size = 1.0,
            legend.height = 0.35, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="8star", size = 2, bg.color = "white", bg.alpha = 0.5) +
  tm_scale_bar(bg.color = "white", bg.alpha = 0.5) +
  tm_shape(mpsz) +
  tm_fill("green", title = "Singapore Boundary", alpha = 0.5)
```

::: callout-note
The grid has been generated correctly because:

-   It covers exactly the whole country of Singapore.

-   Looking at the scale, there are 10 hexagons within a 5 km length. This means each hexagon has an apothem of 250m.
:::

## Fitting grid exactly to the region

The grid above has hexagons outside of Singapore bounds. We need to filter the grids such that we are left with only those that intersect with Singapore boundary.

We will use `st_intersects()` and `filter()` to filter out the hexagons that intersect Singapore.

```{r}
sg_honeycomb$n_collisions = lengths(st_intersects(sg_honeycomb, mpsz))
sg_honeycomb <- filter(sg_honeycomb, n_collisions > 0)
```

Let's generate a map again if the cleaning generated our expected result.

```{r}
tm_shape(sg_honeycomb) +
  tm_fill(col = "white", title = "Hexagons") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Singapore with honeycomb grid",
            main.title.position = "center",
            main.title.size = 1.0,
            legend.height = 0.35, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2) +
  tm_shape(mpsz) +
  tm_fill("green", title = "Singapore Boundary", alpha = 0.5) +
  tm_shape(busstops) +
  tm_dots(col = "red", size = 0.001, title = "Bus Stops")
```

::: callout-important
There are some tiny islands outside of the Singapore mainland. Although they have no bus stops, we will keep them in the data frame for now as other hexagons also do not have bus stops in them (as they have no red dots inside).
:::

As we now have a perfectly fitting honeycomb grid, we will save this to an `rds` file for later reuse.

```{r}
write_rds(sg_honeycomb, "data/rds/sg_honeycomb.rds")
```

::: callout-caution
From this point forward, we will be using `sg_honeycomb` to do the analysis. Hence, we can already remove `mpsz` from the environment.

```{r}
rm(mpsz)
```
:::
