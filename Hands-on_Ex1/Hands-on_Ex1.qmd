---
title: "Hands-on Exercise 1: Geospatial Data Wrangling and Choropleth Mapping with R"
author: "Kristine Joy Paas"
date: "16 November 2023"
date-modified: "last-modified"
format: html
execute: 
  echo: true
  eval: true
  warning: false
---

# Overview

This hands-on exercise covers these chapters:

-   [Chapter 1: Data Wrangling with R](https://r4gdsa.netlify.app/chap01)

-   [Chapter 2: Choropleth Mapping with R](https://r4gdsa.netlify.app/chap02)

In this hands-on exercise, I learned about the following:

-   Public Data Sets like the ones on [data.gov.sg](https://data.gov.sg/), [LTADataMall](https://www.mytransport.sg/content/mytransport/home/dataMall.html), and [InsideAirbnb](http://insideairbnb.com/get-the-data.html).
-   How to import data sets into RStudio
-   Wrangling geospatial data in using different R packages like sf, tidyverse, etc.
-   Creating thematic/choropleth maps with `tmap`

# Getting Started

## Preparing the data sets

First, I downloaded the different data sets needed in this exercise.

### **Geospatial**

-   Master Plan 2014 Subzone Boundary (Web) from [data.gov.sg](https://data.gov.sg/)

-   Pre-Schools Location from [data.gov.sg](https://data.gov.sg/)

-   Cycling Path from [LTADataMall](https://www.mytransport.sg/content/mytransport/home/dataMall.html)

### **Aspatial**

-   Latest version of Singapore Airbnb listing data from [Inside Airbnb](http://insideairbnb.com/get-the-data.html)

-   Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 from [Department of Statistics, Singapore](https://www.singstat.gov.sg/)

Next, is putting them under the `Hands-on_Ex1` directory, with the following file structure:

``` bash
Hands-on_Ex1
├── Hands-on_Ex1.qmd
└── data
    ├── aspatial
    │   ├── listings.csv
    │   └── respopagesextod2011to2020.csv
    └── geospatial
        ├── CyclingPathGazette.cpg
        ├── CyclingPathGazette.dbf
        ├── CyclingPathGazette.lyr
        ├── CyclingPathGazette.prj
        ├── CyclingPathGazette.sbn
        ├── CyclingPathGazette.sbx
        ├── CyclingPathGazette.shp
        ├── CyclingPathGazette.shp.xml
        ├── CyclingPathGazette.shx
        ├── MP14_SUBZONE_WEB_PL.dbf
        ├── MP14_SUBZONE_WEB_PL.prj
        ├── MP14_SUBZONE_WEB_PL.sbn
        ├── MP14_SUBZONE_WEB_PL.sbx
        ├── MP14_SUBZONE_WEB_PL.shp
        ├── MP14_SUBZONE_WEB_PL.shp.xml
        ├── MP14_SUBZONE_WEB_PL.shx
        └── PreSchoolsLocation.kml
```

## Installing R packages

I used the code below to install the R packages used in the exercise:

```{r}
pacman::p_load(sf, tidyverse, tmap)
```

# Chapter 1: Data Wrangling with R

## Importing Geospatial Data

After setting up the data sets and the R packages, we can proceed with importing the geospatial data.

### **Master Plan 2014 Subzone Boundary (Web)**

To import the data set to RStudio, I used `st_read()` :

```{r}
mpsz <- st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL")
```

I encountered the error below along the way:

``` bash
Cannot open layer MasterPlan2014SubzoneBoundaryWebKML
```

This is because I originally downloaded the `kml` file instead of the `shp` file. After using the `shp` file, the `st_read()` succeeded.

> ℹ️ My biggest take-away for this is that `st_read` reads `shp` data set by default. (this would be debunked later)

After running the code, we should see the `mpsz` data in the environment.

![](images/mpsz_imported.png)

### Cycling Path Data

Equipped with my learning from the previous step, I was able to quickly figure out that importing this data set can be done by simply changing the `layer` parameter from the previous code:

```{r}
cyclingpath <- st_read(dsn = "data/geospatial", layer = "CyclingPathGazette")
```

However, the difference is that this geometry has polyline features, while the previous has polygon features.

### Pre-Schools Location Data

Unlike the others, this data set is in `kml` format instead of `shp` format. I used the following code to import:

```{r}
preschool <- st_read("data/geospatial/PreSchoolsLocation.kml")
```

> ℹ️ Contrary to my previous take-away, `st_read()` can read `kml` files by default. In fact, reading `shp` files require more parameters like `dsn` and `layer`.

## Checking Contents of Data Frames

### Checking the geometry of data frames

Using `st_geometry()` returns information about the geometry of the data frame.

```{r}
st_geometry(mpsz)
```

It gave the same geometric information as when importing the shape data but with additional details like the first 5 geometries.

### Getting overview of geospatial data

Using `glimpse()` gives useful information about the columns, data types, values. For example:

```{r}
glimpse(mpsz)
```

This will be very useful to scan for the available data and which columns are useful for analysis.

### Revealing complete information of feature objects

Using `head()` can give full information about objects in the data set. For example:

```{r}
head(mpsz, n=5)
```

This will return the first 5 objects, and the number of objects can be set by specifying a value for `n`.

Another function that is useful for this purpose is `tail()`, which returns items from the end of the data set. For example:

```{r}
tail(mpsz, n=2)
```

This returned the items on rows 322 and 323 instead of 1 and 2 if we were to use `head()`.

However, I wonder which use cases these functions would be useful as we can easily inspect the full data when looking at all the Environment Data in RStudio. 🤔

## Plotting Geospatial Data

```{r}
plot(mpsz, max.plot = 15)
```

I was pleasantly surprised to find that the rendering of the plots was **so fast**! It only took 1 second or so on my machine for 300+ features and 15 fields. This is really useful for quick look of the data.

Trying it on the cycling path data was also very fast though the result was not so useful for me as it needs to be overlayed with a map like above.

```{r}
plot(cyclingpath)
```

I wonder how long it would take once we have larger data sets. 🤔

As someone not originally from Singapore, I am still familiarizing myself with the countries geography so I'll plot the regions first.

```{r}
plot(mpsz['REGION_N'])
```

This can still be visualized better, especially the names in the legend got cut off. From the **Help** pages on RStudio, this function has a lot more parameters and I'll explore it once I have more time.

## Projections

### Correcting the EPSG code

```{r}
st_crs(mpsz)
```

```{r}
mpsz3414 <- st_set_crs(mpsz, 3414)
```

```{r}
st_crs(mpsz3414)
```

I am not familiar with what the `st_crs()` returns and I wouldn't have thought that EPSG needs correcting since I don't have the domain knowledge. This is one of my biggest take away for this exercise.

### Projection Transformation

When using `st_set_crs()`, I got the warning below:

``` bash
Warning: st_crs<- : replacing crs does not reproject data; use st_transform for that
```

As such, the `mpsz3414` data before may not be projected properly despite having the correct EPSG value.

Next, I will transform the pre-school data:

```{r}
preschool3414 <- st_transform(preschool, crs=3414)
st_geometry(preschool3414)
```

## Importing and Converting Aspatial Data

For aspatial data, I used `read_csv()` to import the data.

```{r}
listings <- read_csv("data/aspatial/listings.csv")
```

From this, we have candidate geospatial fields that we can use, longitude and latitude.

Checking the data contents of these field can confirm if we can really use the data.

```{r}
list(listings)
```

After confirming that longitude and latitude can be used as geospatial data, I transformed this data to geospatial data.

```{r}
listings_sf <- st_as_sf(listings, 
                       coords = c("longitude", "latitude"),
                       crs=4326) %>%
  st_transform(crs = 3414)
```

Checking the new data frame, it was confirmed that it was transformed to a geospatial data.

```{r}
glimpse(listings_sf)
```

## Geoprocessing

### Buffering

```{r}
buffer_cycling <- st_buffer(cyclingpath, 
                               dist=5, nQuadSegs = 30)
buffer_cycling$AREA <- st_area(buffer_cycling)
sum(buffer_cycling$AREA)
```

After seeing my classmate's question on Piazza, I was also curious about the effect of `nQuadSegs` to the area. From the documentation, this is the number of segments created per quadrant.

My understanding of this is since each quadrant has 90 degrees, having `nQuadSegs = 30` means 1 segment per 3 degrees. If this correct, my hypothesis is that the higher `nQuadSegs` is, the more accurate it is. This is because `nQuadSegs=1` would be a square, and it becomes a polygon with more sides the higher `nQuadSegs` is. The higher `nQuadSegs`, the smoother the polygon becomes and it gets closer to being a circle.

I'm testing the theory below and if my hypothesis is correct, the area should not differ much past `nQuadSegs=180`

```{r}
buffer_cycling0 <- st_buffer(cyclingpath, 
                               dist=5, nQuadSegs = 0)
buffer_cycling0$AREA <- st_area(buffer_cycling0)
sum(buffer_cycling0$AREA)
```

```{r}
buffer_cycling10 <- st_buffer(cyclingpath, 
                               dist=5, nQuadSegs = 10)
buffer_cycling10$AREA <- st_area(buffer_cycling10)
sum(buffer_cycling10$AREA)
```

```{r}
buffer_cycling45 <- st_buffer(cyclingpath, 
                               dist=5, nQuadSegs = 45)
buffer_cycling45$AREA <- st_area(buffer_cycling45)
sum(buffer_cycling45$AREA)
```

```{r}
buffer_cycling90 <- st_buffer(cyclingpath, 
                               dist=5, nQuadSegs = 90)
buffer_cycling90$AREA <- st_area(buffer_cycling90)
sum(buffer_cycling90$AREA)
```

```{r}
buffer_cycling180 <- st_buffer(cyclingpath, 
                               dist=5, nQuadSegs = 180)
buffer_cycling180$AREA <- st_area(buffer_cycling180)
sum(buffer_cycling180$AREA)
```

```{r}
buffer_cycling1800 <- st_buffer(cyclingpath, 
                               dist=5, nQuadSegs = 1800)
buffer_cycling1800$AREA <- st_area(buffer_cycling1800)
sum(buffer_cycling1800$AREA)
```

As we can see, from 180 to 1800 the area only changed by 3m^2^ but the differences are larger in lower values. My conclusion is that the higher `nQuadSegs`, the more accurate the value we will get. However, the calculation took much longer. The extremely small accuracy benefit may not be worth the trade-off in most cases.

The result when using `nQuadSegs` of 30 is already very close to the result when it is 1800.

$$
1774367/1774465 = 99.99\%
$$

### Point-in-polygon count

```{r}
mpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))
summary(mpsz3414$`PreSch Count`)
```

This means that there are subzones without pre-school while some have as many as 72.

```{r}
top_n(mpsz3414, 1, `PreSch Count`)
```

The subzone with the most pre-schools is **Tampines East**.

To calculate the density of pre-schools:

```{r}
mpsz3414$Area <- mpsz3414 %>%
  st_area()
mpsz3414 <- mpsz3414 %>%
  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)
```

Let's see the top 3 subzones with the highest pre-school density.

```{r}
top_n(mpsz3414, 3, `PreSch Density`)
```

Despite Tampines East having the most pre-schools, **Cecil** has the highest pre-school density. Tampines East might be much bigger than Cecil so its Pre-school Density is lower.

## Exploratory Data Analysis (EDA)

Below is the histogram for Pre-school Density:

```{r}
hist(mpsz3414$`PreSch Density`)
```

We improvef this graph by using `ggplot`.

```{r}
ggplot(data=mpsz3414, 
       aes(x= as.numeric(`PreSch Density`)))+
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  labs(title = "Are pre-school even distributed in Singapore?",
       subtitle= "There are many planning sub-zones with a single pre-school, on the other hand, \nthere are a few sub-zones with at least 20 pre-schools",
      x = "Pre-school density (per km sq)",
      y = "Frequency")
```

We can also create a scatter plot.

```{r}
ggplot(data=mpsz3414, 
       aes(y = `PreSch Count`, 
           x= as.numeric(`PreSch Density`)))+
  geom_point(color="black", 
             fill="light blue") +
  xlim(0, 40) +
  ylim(0, 40) +
  labs(title = "",
      x = "Pre-school density (per km sq)",
      y = "Pre-school count")
```

# Chapter 2: Choropleth mapping with R

## Data Wrangling

### Importing geospatial data

To import the MPSZ data set to RStudio, I used the same code chunk in the previous exercise.

```{r}
mpsz <- st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL")
```

### Importing aspatial data

The csv data is an aspatial data so `read_csv()` must be used instead of `st_read()`:

```{r}
popdata <- read_csv("data/aspatial/respopagesextod2011to2020.csv")
```

### Data preparation

```{r}
popdata2020 <- popdata %>%
  filter(Time == 2020) %>%
  group_by(PA, SZ, AG) %>%
  summarise(`POP` = sum(`Pop`)) %>%
  ungroup()%>%
  pivot_wider(names_from=AG, 
              values_from=POP) %>%
  mutate(YOUNG = rowSums(.[3:6])
         +rowSums(.[12])) %>%
mutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+
rowSums(.[13:15]))%>%
mutate(`AGED`=rowSums(.[16:21])) %>%
mutate(`TOTAL`=rowSums(.[3:21])) %>%  
mutate(`DEPENDENCY` = (`YOUNG` + `AGED`)
/`ECONOMY ACTIVE`) %>%
  select(`PA`, `SZ`, `YOUNG`, 
       `ECONOMY ACTIVE`, `AGED`, 
       `TOTAL`, `DEPENDENCY`)
```

I don't fully understand this entire code chunk yet but I know it filtered for data from 2020 only and did some aggregations based on the age groups, PA, and AZ. New fields like `YOUNG` and `AGED` .

Next, I joined this data with the mpsz data via `SZ`. However, we still need to make sure that the `SZ` values are uppercase to match mpsz's `SUBZONE_N.`

```{r}
popdata2020 <- popdata2020 %>%
  mutate_at(.vars = vars(PA, SZ), 
          .funs = list(toupper)) %>%
  filter(`ECONOMY ACTIVE` > 0)
```

Then the 2 data sets can be joined.

```{r}
mpsz_pop2020 <- left_join(mpsz, popdata2020,
                          by = c("SUBZONE_N" = "SZ"))
```

Lastly, write the rds of the combined data set.

```{r}
write_rds(mpsz_pop2020, "data/rds/mpszpop2020.rds")
```

> ⚠️ This failed on the first try so I had to create the `rds` directory under `data/` before trying again.

## **Choropleth Mapping Geospatial Data Using *tmap***

### Using *qtm()*

```{r}
tmap_mode("plot")
qtm(mpsz_pop2020, 
    fill = "DEPENDENCY")
```

I tried to create an interactive map by using `view` instead of `plot` but an error about invalid polygons was returned.

### Using *tmap* elements

```{r}
tm_shape(mpsz_pop2020)+
  tm_fill("DEPENDENCY", 
          style = "quantile", 
          palette = "Blues",
          title = "Dependency ratio") +
  tm_layout(main.title = "Distribution of Dependency Ratio by planning subzone",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_borders(alpha = 0.5) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2) +
  tm_credits("Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\n and Population data from Department of Statistics DOS", 
             position = c("left", "bottom"))
```

From this part, I saw that with `tm_shape` as base, thematic maps can be created by doing `+` with the tmap actions. I will explore this later but I found a good reference on where to start: <https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html>.

### Classification methods

#### Quantile

```{r}
tm_shape(mpsz_pop2020)+
  tm_fill("DEPENDENCY",
          n = 5,
          style = "jenks") +
  tm_borders(alpha = 0.5)
```

#### Equal

```{r}
tm_shape(mpsz_pop2020)+
  tm_fill("DEPENDENCY",
          n = 5,
          style = "equal") +
  tm_borders(alpha = 0.5)
```

My main takeaway for this is that the quantile classification deals with outliers better. In this data set, there is an outlier subzone. If we use equal classification, the map looks homogeneous and does not provide much information as it cannot be seen how the values from one subzone to the other differ.

With quantile classification, these differences can be seen more easily despite the outlier value.

### Color Scheme

The color scheme can be changed by specifying the palette in `tm_fill()` like below:

```{r}
tm_shape(mpsz_pop2020)+
  tm_fill("DEPENDENCY",
          n = 6,
          style = "quantile",
          palette = "Blues") +
  tm_borders(alpha = 0.5)
```

More colors can be found when running `tmaptools::palette_explorer()` in the console. However, it requires `shiny` and `shinyjs` to work. This is a wonderful tool as it can also simulate how the color schemes look from the perspective of people with color blindness. As these maps aim to communicate, it is important for the color schemes chosen to not just be beautiful, but also inclusive.

### Map Layouts

```{r}
tm_shape(mpsz_pop2020)+
  tm_fill("DEPENDENCY", 
          style = "jenks", 
          palette = "Blues", 
          legend.hist = TRUE, 
          legend.is.portrait = TRUE,
          legend.hist.z = 0.1) +
  tm_layout(main.title = "Distribution of Dependency Ratio by planning subzone \n(Jenks classification)",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.45, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) +
  tm_borders(alpha = 0.5)
```

Adding legends to maps is very useful as it provides additional information. However, this feature should not be abused to add multiple visualizations as legend as it can cause more confusion if there is too much information to present.

Map style can also be changed and it is useful to enhance the visual presentation. After doing some research, I found the other available styles in <https://cran.r-project.org/web/packages/tmap/vignettes/tmap-changes.html>.

```{r}
tm_shape(mpsz_pop2020)+
  tm_fill("DEPENDENCY", 
          style = "quantile", 
          palette = "-Greens") +
  tm_borders(alpha = 0.5) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar(width = 0.15) +
  tm_grid(lwd = 0.1, alpha = 0.2) +
  tmap_style("natural")
```

In this example, I used the `natural` style to make it look like the map is surrounded by water (as it is naturally). Adding "furnitures" like compass and scale can also provide more perspective.

### Smaller Maps

```{r}
tm_shape(mpsz_pop2020)+ 
  tm_polygons(c("DEPENDENCY","AGED"),
          style = c("equal", "quantile"), 
          palette = list("Blues","Greens")) +
  tm_layout(legend.position = c("right", "bottom"))
```

I don't find this very useful as the maps rendered might be too small to inspect. However, the facets with the region can be useful to see the data based on region.

```{r}
tm_shape(mpsz_pop2020) +
  tm_fill("DEPENDENCY",
          style = "quantile",
          palette = "Greens",
          thres.poly = 0) + 
  tm_facets(by="REGION_N", 
            free.coords=TRUE, 
            drop.shapes=TRUE) +
  tm_layout(legend.show = FALSE,
            title.position = c("center", "center"), 
            title.size = 20) +
  tm_borders(alpha = 0.5)
```

### Mapping According to Criterion

```{r}
tm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N=="CENTRAL REGION", ])+
  tm_fill("DEPENDENCY", 
          style = "quantile", 
          palette = "Greens", 
          legend.hist = TRUE, 
          legend.is.portrait = TRUE,
          legend.hist.z = 0.1) +
  tm_layout(legend.outside = TRUE,
            legend.height = 0.45, 
            legend.width = 5.0,
            legend.position = c("right", "bottom"),
            frame = FALSE) +
  tm_borders(alpha = 0.5)
```

I find this very useful especially if we just want to map a subset of the data. This can be used when we want to highlight information on certain regions.

# Others

## Changing the website theme

After exploring Quarto docs, I found that we can [change the theme](https://quarto.org/docs/output-formats/html-themes.html). I decided on the [`zephyr`](https://bootswatch.com/zephyr/) theme as it looks most readable and aesthetic for me.

## Issues with using Github on Rstudio

It was recommended to name the project with the convention `<github_username>/ISSS624`. However, due to restrictions on my machine, I had to deviate from this and create my project elsewhere.

Hence, I couldn't use `usethis::use_github()` to setup my Github repository. However, as I use git Github intensively in my job, I did the setup manually myself to use the `git` functions on RStudio.

I used these steps for the manual setup.

1.  Create the repo manually on Github on <https://github.com/kjcpaas/ISSS624>

2.  Add the Github remote on RStudio project

    ``` bash
    > git remote add origin git@github.com:kjcpaas/ISSS624.git
    > git remote -v
    origin  git@github.com:kjcpaas/ISSS624.git (fetch)
    origin  git@github.com:kjcpaas/ISSS624.git (push)
    ```

3.  Set remote for head

    ``` bash
    > git remote set-head origin --auto
    > git gc
    ```

After all these, I was able to use the `git` functions on RStudio.

# Reflections

-   I thought it would be difficult to setup R and RStudio especially as the Cran Project site was down when I first attempted to set up. Good thing it was up the next day.

-   I was exhausted after doing the 2 chapters, and I must change how I do things next week. What I have been doing is understanding each part deeply first before moving on to the next. I think I should do chunks of work first before going back for deep understanding so I have more holistic view of the content and more sustainable for the duration of this course.

-   I find `tmap` very useful and I am looking forward to using it more as it has so many functions to visualize data. Only one's creativity limits the possibilities of using it.

-   This course is looking to be very demanding indeed with a lot of time spent on pre-class assignments. However, I have enjoyed the hands-on exercise so far and I am eager to learn more!
