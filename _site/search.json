[
  {
    "objectID": "Hands-on_Ex3/data/geospatial/MPSZ-2019.html",
    "href": "Hands-on_Ex3/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2.html",
    "href": "Hands-on_Ex2/Hands-on_Ex2.html",
    "title": "Hands-on Exercise 2: Work in Progress",
    "section": "",
    "text": "In this hands-on exercise, I learned about the following:\n\nPut summarry of learnings here"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2.html#preparing-the-data-sets",
    "href": "Hands-on_Ex2/Hands-on_Ex2.html#preparing-the-data-sets",
    "title": "Hands-on Exercise 2: Work in Progress",
    "section": "Preparing the data sets",
    "text": "Preparing the data sets\nAdd what data sets are used"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2.html#installing-r-packages",
    "href": "Hands-on_Ex2/Hands-on_Ex2.html#installing-r-packages",
    "title": "Hands-on Exercise 2: Work in Progress",
    "section": "Installing R packages",
    "text": "Installing R packages\nWhat R packages are needed?"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2B.html",
    "href": "Hands-on_Ex2/Hands-on_Ex2B.html",
    "title": "Hands-on Exercise 2B: Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "This hands-on exercise covers Chapter 9: Global Measures of Spatial Autocorrelation\nI learned about the following:\n\nSpatial correlation using Moran’s I and Geary’s C\nCorrelograms\n\n\n\nData sets used on this exercise were downloaded from E-learn.\n\n\n\nHunan county boundary layer (shp format)\n\n\n\n\n\nHunan’s local development indicators in 2012 (csv format)\n\nNext, is putting them under the Hands-on_Ex2 directory, with the following file structure:\nHands-on_Ex2\n└── data\n    ├── aspatial\n    │   └── Hunan_2012.csv\n    └── geospatial\n        ├── Hunan.dbf\n        ├── Hunan.prj\n        ├── Hunan.qpj\n        ├── Hunan.shp\n        └── Hunan.shx\n\n\n\n\nI used the code below to install the R packages used in the exercise:\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2B.html#preparing-the-data-sets",
    "href": "Hands-on_Ex2/Hands-on_Ex2B.html#preparing-the-data-sets",
    "title": "Hands-on Exercise 2B: Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "Data sets used on this exercise were downloaded from E-learn.\n\n\n\nHunan county boundary layer (shp format)\n\n\n\n\n\nHunan’s local development indicators in 2012 (csv format)\n\nNext, is putting them under the Hands-on_Ex2 directory, with the following file structure:\nHands-on_Ex2\n└── data\n    ├── aspatial\n    │   └── Hunan_2012.csv\n    └── geospatial\n        ├── Hunan.dbf\n        ├── Hunan.prj\n        ├── Hunan.qpj\n        ├── Hunan.shp\n        └── Hunan.shx"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2B.html#installing-r-packages",
    "href": "Hands-on_Ex2/Hands-on_Ex2B.html#installing-r-packages",
    "title": "Hands-on Exercise 2B: Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "I used the code below to install the R packages used in the exercise:\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2B.html#importing-data-sets",
    "href": "Hands-on_Ex2/Hands-on_Ex2B.html#importing-data-sets",
    "title": "Hands-on Exercise 2B: Global Measures of Spatial Autocorrelation",
    "section": "Importing data sets",
    "text": "Importing data sets\nI used st_read() to import the geospatial shp data.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\",\n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/Hands-on_Ex2/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the previous exercises, we transformed the data with EPSG:3414. However, that is not applicable for this data set as we are not working with Singapore 🇸🇬 data set.\n\n\nAs with the previous exercises, I used read_csv() to import aspatial csv data.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2B.html#joining-the-data-sets",
    "href": "Hands-on_Ex2/Hands-on_Ex2B.html#joining-the-data-sets",
    "title": "Hands-on Exercise 2B: Global Measures of Spatial Autocorrelation",
    "section": "Joining the data sets",
    "text": "Joining the data sets\nIn the exercise, we have to join the 2 data sets using this code:\n\nhunan &lt;- left_join(hunan, hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\n\n\nNote\n\n\n\nWe did not specify any columns to join by but left_join detected common column, County, so it joined the 2 data sets by this column.\nAt the end of this, we are left with 7 columns, which includes GDPPC from the aspatial data, which contains data for Gross Domestic Product per Capita."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2B.html#visualizing-regional-development-indicator",
    "href": "Hands-on_Ex2/Hands-on_Ex2B.html#visualizing-regional-development-indicator",
    "title": "Hands-on Exercise 2B: Global Measures of Spatial Autocorrelation",
    "section": "Visualizing Regional Development Indicator",
    "text": "Visualizing Regional Development Indicator\nNext, I plotted the GDPPC maps using equal interval classification and equal quantile classification.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2B.html#computing-contiguity-spatial-weights",
    "href": "Hands-on_Ex2/Hands-on_Ex2B.html#computing-contiguity-spatial-weights",
    "title": "Hands-on Exercise 2B: Global Measures of Spatial Autocorrelation",
    "section": "Computing Contiguity Spatial Weights",
    "text": "Computing Contiguity Spatial Weights\nFirst, I built the neighbor list using Queen contiguity-based neighbors. This means the regions must share a border (minimum a point) to be considered neighbors.\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2B.html#row-standardized-weights-matrix",
    "href": "Hands-on_Ex2/Hands-on_Ex2B.html#row-standardized-weights-matrix",
    "title": "Hands-on Exercise 2B: Global Measures of Spatial Autocorrelation",
    "section": "Row-standardized weights matrix",
    "text": "Row-standardized weights matrix\nNext, I assigned weights to each neighboring county with value 1/(# of neighbors). This could be done by using style=\"W\" to nb2listw().\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2B.html#global-spatial-autocorrelation-morans-i",
    "href": "Hands-on_Ex2/Hands-on_Ex2B.html#global-spatial-autocorrelation-morans-i",
    "title": "Hands-on Exercise 2B: Global Measures of Spatial Autocorrelation",
    "section": "Global Spatial Autocorrelation: Moran’s I",
    "text": "Global Spatial Autocorrelation: Moran’s I\n\nMoran’s I test\nNext, I used Moran’s I statistical testing using moran.test().\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Moran’s I value is 0.30075, which is greater than 0. This means that observations are clustered, and tend to be similar.\nThe p-value is also very close to 0, which indicates high confidence on the correlation.\n\n\n\n\nComputing Monte Carlo Moran’s I\nNext, a Monte Carlo simulation was performed for the Moran’s I statistic. 1000 simulations were performed by the code below:\n\nset.seed(1234)\nbperm = moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Moran’s I value is 0.30075, same result as that of the Moran’s I test. Similarly, it means that observations are clustered, and tend to be similar.\nThe p-value is also very close to 0, which indicates high confidence on the correlation.\n\n\n\n\nVisualizing Monte Carlo Moran’s I\nFirst, I examined the statistics of the Monte Carlo Moran’s I. I checked the mean, variance, and the quantiles.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\nNext, I also plotted the histogram.\n\n\n\n\n\n\nImportant\n\n\n\nI plotted using ggplot2 as an additional challenge from the exercise.\n\n\n\nmc_results_df &lt;- data.frame(moran_i = bperm$res)\nggplot(mc_results_df, aes(x = moran_i)) +\n  geom_histogram(bins = 20, fill = \"grey\", color = \"black\") +\n  geom_vline(xintercept = 0, color = \"red\") +\n  labs(x = \"Sumilated Moran's I\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom this Monte Carlo simulations, results are skewed to the left, meaning most of the Moran’s I simulations result in negative values. It means that in most simulation results, there is dispersion so there is no spatial correlation.\nThis is quite contradictory to the statistic from moran.test.\nHowever, as this is a simulation set using seed 1234, results could be different in other simulations because the sampling is different."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2B.html#global-spatial-autocorrelation-gearys",
    "href": "Hands-on_Ex2/Hands-on_Ex2B.html#global-spatial-autocorrelation-gearys",
    "title": "Hands-on Exercise 2B: Global Measures of Spatial Autocorrelation",
    "section": "Global Spatial Autocorrelation: Geary’s",
    "text": "Global Spatial Autocorrelation: Geary’s\nNext I used Geary’s method for spatial correlation.\n\nGeary’s C test\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\n\n\n\n\n\nTip\n\n\n\nGeary’s C statistic is 0.6907, which is less than 1. This means that observations are clustered, and tend to be similar. P-value is also very close to 0, suggesting high-confidence.\n\nIt is consistent with the conclusions in Moran’s I test.\n\n\n\n\nComputing Monte Carlo Geary’s C\nSimilarly, I did permutation test via Monte Carlo simulations.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Moran’s I value is 0.6907, same result as that of the Geary’s C test. Similarly, it means that observations are clustered, and tend to be similar.\nThe p-value is also very close to 0, which indicates high confidence on the correlation.\n\n\n\n\nVisualizing Monte Carlo Geary’s C\nFirst, I examined the statistics of the Monte Carlo Geary’s C. I checked the mean, variance, and the quantiles.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\nFinally, visualizing it.\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom this Monte Carlo simulations, results are quite balanced on 1, which makes it inconclusive as to the spatial clustering and dispersion.\nThis is quite contrary to the statistic resulting from geary.test(), which was more conclusive.\nHowever, as this is a simulation set using seed 1234, results could be different in other simulations because the sampling is different."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2B.html#compute-morans-i-correlogram",
    "href": "Hands-on_Ex2/Hands-on_Ex2B.html#compute-morans-i-correlogram",
    "title": "Hands-on Exercise 2B: Global Measures of Spatial Autocorrelation",
    "section": "Compute Moran’s I correlogram",
    "text": "Compute Moran’s I correlogram\nFirst, I generated the correlogram for Morans’s I.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\nThis did not provide me much information and I didn’t know how to interpret it so I printed the full result.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTip\n\n\n\nFrom my understanding, since Moran’s I values are greater than 0 and highest on lag 1, it means that the spatial correlation is most significant the closer the regions are."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2B.html#compute-gearys-c-correlogram",
    "href": "Hands-on_Ex2/Hands-on_Ex2B.html#compute-gearys-c-correlogram",
    "title": "Hands-on Exercise 2B: Global Measures of Spatial Autocorrelation",
    "section": "Compute Geary’s C correlogram",
    "text": "Compute Geary’s C correlogram\nNext, I generated the correlogram for Geary’s C.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\nNext was to print the results.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Geary’s C values are closest to 0 on the lag distance 1. Similar to Compute Moran’s I correlogram, the spatial correlation is strongest the closer the regions are.\nThe pattern is inverse of the Moran’s I correlogram, which makes sense as Moran’s I and Geary’s C trends are inverse of each other."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html",
    "href": "Take-home_Ex1/Take-home_Ex1.html",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "",
    "text": "The aim of this study is to uncover spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.\nThe main modes of analysis to be used here are Local Indicators of Spatial Association (GLISA) and Emerging Hot Spot Analysis (EHSA).\nIn doing these study, we will be looking at bus trips started during the hours below.\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday evening peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nMore details about the study can be found here.\nIn this part of the study, we will do data wrangling on the data sets so that they are transformed into a form that can be used for geovisualization and spatial analysis."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#preparing-the-data-sets",
    "href": "Take-home_Ex1/Take-home_Ex1.html#preparing-the-data-sets",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Preparing the data sets",
    "text": "Preparing the data sets\n\nGeospatial\nThis data sets are in shp format.\n\nBus Stop Locations, available publicly from LTA DataMall\n\n\n\nAspatial\nThese data sets are in csv format.\n\nMaster Plan 2019 Subzone Boundary (Web), originally from data.gov.sg but used the one provided on E-learn.\nPassenger Volume By Origin Destination Bus Stops from LTA DataMall via API (need to request for access)\n\nAugust 2023\nSeptember 2023\nOctober 2023 - we will focus on this as the main data set"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#preparing-the-data-directory",
    "href": "Take-home_Ex1/Take-home_Ex1.html#preparing-the-data-directory",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Preparing the data/ directory",
    "text": "Preparing the data/ directory\nBefore starting our analysis, we have to organize the data sets in a directory.\n\nGeospatial data will be located under data/geospatial\nAspatial data will be located under data/aspatial\ndata/rds to be created to store data that we can reuse and to make our code reproduceable.\n\n\n\n\n\n\n\nShow file structure\n\n\n\n\n\nTake-home_Ex1\n└── data\n    ├── aspatial\n    │   ├── origin_destination_bus_202308.csv\n    │   ├── origin_destination_bus_202309.csv\n    │   └── origin_destination_bus_202310.csv\n    ├── geospatial\n    │   ├── BusStop.cpg\n    │   ├── BusStop.dbf\n    │   ├── BusStop.lyr\n    │   ├── BusStop.prj\n    │   ├── BusStop.sbn\n    │   ├── BusStop.sbx\n    │   ├── BusStop.shp\n    │   ├── BusStop.shp.xml\n    │   ├── BusStop.shx\n    │   ├── MPSZ-2019.cpg\n    │   ├── MPSZ-2019.dbf\n    │   ├── MPSZ-2019.prj\n    │   ├── MPSZ-2019.qmd\n    │   ├── MPSZ-2019.shp\n    │   └── MPSZ-2019.shx\n    └── rds"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#setting-up-the-r-environment",
    "href": "Take-home_Ex1/Take-home_Ex1.html#setting-up-the-r-environment",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Setting Up the R Environment",
    "text": "Setting Up the R Environment\nAfter preparing the data sets, we can finally proceed to load the R packages needed for this study.\n\n\n\n\n\n\nR packages used\n\n\n\n\n\n\ntmap: for thematic mapping\nsf: for geospatial data handling\ntidyverse: for non-spatial data handling\nknitr:for prettifying presentation\nsfdep: for spatial analysis"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#environment-settings",
    "href": "Take-home_Ex1/Take-home_Ex1.html#environment-settings",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Environment Settings",
    "text": "Environment Settings\nWe will also set the default settings on for this document\n\ntmap_mode to plot: for plotting simple maps\ntmap_style to natural: for my preferred mapping style\nset seed for reproducibility of results"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#running-the-setup",
    "href": "Take-home_Ex1/Take-home_Ex1.html#running-the-setup",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Running the setup",
    "text": "Running the setup\nWe will label this code chunk as the setup chunk so the R runs it even after the environment restarts.\n\npacman::p_load(sf, tmap, tidyverse, knitr, sfdep)\ntmap_mode(\"plot\")\ntmap_style(\"natural\")\nset.seed(1234)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#goal-data-sets",
    "href": "Take-home_Ex1/Take-home_Ex1.html#goal-data-sets",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Goal data sets",
    "text": "Goal data sets\nTo enable the visualization and analysis in latter part of the study, we need to have the following data sets:\n\nHoneycomb geometry, a tessellation of hexagons covering the bus stops in Singapor\nHourly bus trips started from each hexagon cell\n\n1 for weekend, 1 for weekend/holidays\nRequired columns: HEX_ID, HOUR_OF_DAY, TRIPS\nMust contain geometry of the hexagon\nCan be used to generate a time series cube"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#data-management-points",
    "href": "Take-home_Ex1/Take-home_Ex1.html#data-management-points",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Data management points",
    "text": "Data management points\nAs the wrangling process is expected to have a lot of intermediate steps, Save, Load, and Data clear points are available to make our data wrangling more efficient.\n\n\n\n\n\n\nSave point\n\n\n\nThis is where data is written as rds files using write_rds() for important data sets that will be used in later analysis. Examples are:\n\nThe end goal of data wrangling: Hourly bus trips started from each hexagon cell data sets\nCritical outputs of expensive calculations\n\n\n\n\n\n\n\n\n\nLoad point\n\n\n\nThis is where data is loaded from rds files using read_rds(). They were previously generated by the save point.\nTIP: Skip to the load points to progress without running the code above it\n\n\n\n\n\n\n\n\nData clear point\n\n\n\nThis is where data that will not be used anymore are cleared. The data in RStudio environment will pile up and set #| eval: false in code chunks if you want skip the clearing. For example, the code below won’t be run.\n\nmessage &lt;- \"This code chunk executed\""
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#generating-hexagons-from-busstop-data",
    "href": "Take-home_Ex1/Take-home_Ex1.html#generating-hexagons-from-busstop-data",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Generating hexagons from BusStop data",
    "text": "Generating hexagons from BusStop data\nAs per the specifications of this study, we must use a honeycomb grid, a tesselation of hexagons to replace the mpsz data set.\n\n\n\n\n\n\nWhy hexagons?\n\n\n\nSome benefits of using a hexagons are:\n\nA hexagon is the polygon with the most number of sides that can tessellate (or tile). Hence it is the most “circular” of the polygons that can be tessellated.\nDistances of the centroid from one hexagon to the next are consistent all around the hexagon, making it easy to find neighbors.\n\nMore information about hexagons in the context of spatial analysis can be found in https://desktop.arcgis.com/en/arcmap/latest/tools/spatial-statistics-toolbox/h-whyhexagons.htm\n\n\n\nImporting the Singapore boundary data\nWe will use the Master Plan 2019 Subzone Boundary (Web) data set that has been used in class. This is a shp file, that we will import by using st_read(). We will use this to ensure that the bus stops are within Singapore.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                    layer = \"MPSZ-2019\")\n\nReading layer `MPSZ-2019' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/Take-home_Ex1/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nCorrecting the projection\n\n\n\nThis data frame using the global GPS standard projection, WGS84. We need to convert this to SVY21 that is more appropriate for Singapore 🇸🇬 context.\n\nmpsz &lt;- mpsz %&gt;% st_transform(crs=3414)\n\n\n\n\n\n\n\n\n\nSave point\n\n\n\nLet’s save this geometry with corrected projection from plotting purposes.\n\nwrite_rds(mpsz, \"data/rds/mpsz.rds\")\n\n\n\n\n\nShow the code\ntmap_style(\"natural\")\ntm_shape(mpsz) +\n  tm_fill(\"lightgreen\", title = \"Singapore Boundary\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Map of Singapore\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\nImporting the BusStop data set\nThe BusStop data set is a in shp format. We can import it by using st_read() from the sf package.\n\nbusstops &lt;- st_read(dsn = \"data/geospatial\",\n                    layer = \"BusStop\")\n\nReading layer `BusStop' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/Take-home_Ex1/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nCorrecting the projection\n\n\n\n\n\nWe want to use SVY21 as the projection for this study as it is the projection used for local Singaporean context.\nAfter the import, it shows that the Projected CRSis SVY21. However, checking the CRS with st_crs() tells a different story.\n\nst_crs(busstops)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAs we can see EPSG value is 9001, which correspond to WGS84. We have to fix the projection by transforming to EPSG value of 3414, which corresponds to SVY21.\n\nbusstops &lt;- st_transform(busstops, crs = 3414)\n\n\n\n\nNext, let’s take a look at the available columns to identify which columns we can use for analysis.\n\nkable(head(busstops))\n\n\n\n\n\n\n\n\n\n\nBUS_STOP_N\nBUS_ROOF_N\nLOC_DESC\ngeometry\n\n\n\n\n22069\nB06\nOPP CEVA LOGISTICS\nPOINT (13576.31 32883.65)\n\n\n32071\nB23\nAFT TRACK 13\nPOINT (13228.59 44206.38)\n\n\n44331\nB01\nBLK 239\nPOINT (21045.1 40242.08)\n\n\n96081\nB05\nGRACE INDEPENDENT CH\nPOINT (41603.76 35413.11)\n\n\n11561\nB05\nBLK 166\nPOINT (24568.74 30391.85)\n\n\n66191\nB03\nAFT CORFE PL\nPOINT (30951.58 38079.61)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom this initial look in the data, BUS_STOP_N and LOC_DESC can potentially be used to match records in the passenger volume data set.\n\n\n\n\n\n\n\n\nChanging columns to factor\n\n\n\n\n\nBUS_STOP_N has a finite set of values that we do not need to process sequentially so we will convert it as factor to make it easier to work with.\n\nbusstops$BUS_STOP_N &lt;- as.factor(busstops$BUS_STOP_N)\n\n\n\n\n\n\nGenerating hexagons from Singapore boundary data\nFollowing the steps from https://urbandatapalette.com/post/2021-08-tessellation-sf/, we will use st_make_grid() to generate the hexagons for analysis.\nWe need to provide a value for cellsize in the function, which is defined as “for hexagonal cells the distance between opposite edges”. We need to create hexagons whose apothem is 250m, resulting in a cell size of 500m.\n\n\n\n\n\n\nWhy is cell size 500 m?\n\n\n\n\n\nApothem is defined as the perpendicular from the center of a regular polygon to one of the sides.\nThe specification is this study requires hexagons to be 250 m from the center of the hexagon to the center of one of it’s edge.\n\n\n\n\n\nAs such, this corresponds to the length of 2 opposite apothems, which is 500 m.\nThe edge length is not the same as apothem. It is 288.675m.\n\\[\n250m/cos(30) = 288.675m\n\\]\n\n\n\nWe will use the mpsz data to ensure that the honeycomb grid perfectly covers the Singapore boundaries\n\nhoneycomb &lt;-\n  st_make_grid(mpsz,\n               cellsize = 500,\n               what = \"polygon\",\n               square = FALSE) %&gt;%\n  st_sf()\n\n\n\n\n\n\n\nWe have to use st_sf() to convert the result to a data frame that can be used for the succeeding steps.\n\n\n\nChecking the generated hexagons reveals that it covers all the bus stops.\n\n\nShow the code\ntm_shape(honeycomb) +\n  tm_fill(col = \"white\", title = \"Hexagons\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Singapore with honeycomb grid\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2, bg.color = \"white\", bg.alpha = 0.5) +\n  tm_scale_bar(bg.color = \"white\", bg.alpha = 0.5) +\n  tm_shape(mpsz) +\n  tm_fill(\"green\", title = \"Singapore Boundary\", alpha = 0.5) +\n  tm_shape(busstops) +\n  tm_dots(col = \"red\", size = 0.005, title = \"Bus Stops\")\n\n\n\n\n\n\n\n\n\n\n\nChecking the scale reveals that the generated hexagons are of the expected size, 500 m from one edge to the opposite edge as there are 10 hexagons within a 5 km distance.\n\n\n\n\n\n\n\n\n\nAbout those points outside Singapore\n\n\n\n\n\nThe map shows that there are bus stops in our data set that our outside Singapore bounds (green area). We can remove these points from our busstops data by following the filtering steps from https://urbandatapalette.com/post/2021-08-tessellation-sf/.\nWe will st_intersects() to see which points in busstops intersect with mpsz, and filter those that intersect.\n\nbusstops$n_collisions = lengths(st_intersects(busstops, mpsz))\nbusstops &lt;-\n  filter(busstops, n_collisions &gt; 0) %&gt;%\n  select(, -n_collisions) # Remove n_collisions as we do not need it anymore\n\nPlotting again shows that all bus stops are now within Singapore bounds.\n\n\nShow the code\ntm_shape(honeycomb) +\n  tm_fill(col = \"white\", title = \"Hexagons\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Honeycomb grid without bus stops outside of Singapore\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2, bg.color = \"white\", bg.alpha = 0.5) +\n  tm_scale_bar(bg.color = \"white\", bg.alpha = 0.5) +\n  tm_shape(mpsz) +\n  tm_fill(\"green\", title = \"Singapore Boundary\", alpha = 0.5) +\n  tm_shape(busstops) +\n  tm_dots(col = \"red\", size = 0.005, title = \"Bus Stops\")\n\n\n\n\n\n\n\n\n\n\nFiltering hexagons with bus stops\nThe honeycomb grid generated from [###Generating hexagons from Singapore boundary data] need to be filtered such that the hexagons remaining correspond to only those with bus stops.\nWe can do this by following the filtering steps from https://urbandatapalette.com/post/2021-08-tessellation-sf/. We will use st_intersects() to identify which hexagons intersect with bus stop locations.\n\nhoneycomb$n_collisions = lengths(st_intersects(honeycomb, busstops))\nhoneycomb &lt;- filter(honeycomb, n_collisions &gt; 0)\n\nLet’s generate the map again to check if we have the hexagons that correspond to bus stop locations.\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(\"green\", title = \"Singapore Boundary\", alpha = 0.5) +\n  tm_shape(honeycomb) +\n  tm_fill(col = \"white\", title = \"Hexagons\", alpha = 1) +\n  tm_borders(alpha = 0.2) +\n  tm_layout(main.title = \"Honeycomb grid corresponding to Singapore bus stops\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2, bg.color = \"white\", bg.alpha = 0.5) +\n  tm_scale_bar(bg.color = \"white\", bg.alpha = 0.5) +\n  tm_shape(busstops) +\n  tm_dots(col = \"red\", size = 0.001, title = \"Bus Stops\") +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\nAssigning ids to each hexagon\nHere is the structure of our honeycomb data:\n\nkable(head(honeycomb, n=3))\n\n\n\n\ngeometry\nn_collisions\n\n\n\n\nPOLYGON ((3917.538 28017.41…\n1\n\n\nPOLYGON ((4417.538 30615.49…\n1\n\n\nPOLYGON ((4667.538 28450.43…\n2\n\n\n\n\n\n\n\n\n\n\n\nRemove n-collisions\n\n\n\n\n\nWe do not need n-collisions anymore so we can remove it.\n\nhoneycomb &lt;- honeycomb %&gt;% select(, -n_collisions)\n\n\n\n\nThis data is still incomplete as we need to associate the hexagons to aspatial data, which is critical to the next steps in our data wrangling.\nFor this purpose, we will assign HEX_ID with format H0000.\n\nhoneycomb$HEX_ID &lt;- sprintf(\"H%04d\", seq_len(nrow(honeycomb))) %&gt;% as.factor()\nkable(head(honeycomb)) \n\n\n\n\ngeometry\nHEX_ID\n\n\n\n\nPOLYGON ((3917.538 28017.41…\nH0001\n\n\nPOLYGON ((4417.538 30615.49…\nH0002\n\n\nPOLYGON ((4667.538 28450.43…\nH0003\n\n\nPOLYGON ((4667.538 30182.48…\nH0004\n\n\nPOLYGON ((4667.538 31048.5,…\nH0005\n\n\nPOLYGON ((4917.538 28883.44…\nH0006\n\n\n\n\n\n\n\n\n\n\n\nSave point\n\n\n\n\n\nhoneycomb is the geometry that we will use for analysis. It will be used for tasks such as identifying neighbors and calculating spatial weights.\nIs it also one of the Goal data sets we need. Hence, we will save it.\n\nwrite_rds(honeycomb, \"data/rds/honeycomb202310.rds\")\n\n\n\n\n\n\n\n\n\n\nData clear point\n\n\n\n\n\nWe do not need mpsz anymore as we have generated hexagons already. For further analysis, we will overlay the hexagons to the Singapore map with tmap_mode(\"plot\") to use interactive maps for closer inspection.\n\nrm(mpsz)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#extracting-hourly-of-bus-trips-originating-from-hexagons",
    "href": "Take-home_Ex1/Take-home_Ex1.html#extracting-hourly-of-bus-trips-originating-from-hexagons",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Extracting Hourly # of Bus Trips Originating from Hexagons",
    "text": "Extracting Hourly # of Bus Trips Originating from Hexagons\nThe goal for this part of data wrangling is to have information on how many trips started from each hexagon for every given hour of the day.\nWe will use the Passenger Volume By Origin Destination Bus Stops from LTA DataMall via API for the months of August, September, October 2023.\n\n\n\n\n\n\nFor demonstrating the steps, we will use the October 2023 data set. The same steps will be applied to the other data sets later on.\nIf you want to run the code for August 2023 and September 2023, replace 202310, with 202308 or 202309. Our code can be used to analyze this dataset from any month.\n\n\n\n\nImporting the data set\nThe data set is an aspatial data in csv format so we will use read_csv() to import the data.\n\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202310.csv\")\nkable(head(odbus))\n\n\n\n\n\n\n\n\n\n\n\n\n\nYEAR_MONTH\nDAY_TYPE\nTIME_PER_HOUR\nPT_TYPE\nORIGIN_PT_CODE\nDESTINATION_PT_CODE\nTOTAL_TRIPS\n\n\n\n\n2023-10\nWEEKENDS/HOLIDAY\n16\nBUS\n04168\n10051\n3\n\n\n2023-10\nWEEKDAY\n16\nBUS\n04168\n10051\n5\n\n\n2023-10\nWEEKENDS/HOLIDAY\n14\nBUS\n80119\n90079\n3\n\n\n2023-10\nWEEKDAY\n14\nBUS\n80119\n90079\n5\n\n\n2023-10\nWEEKDAY\n17\nBUS\n44069\n17229\n4\n\n\n2023-10\nWEEKENDS/HOLIDAY\n17\nBUS\n20281\n20141\n1\n\n\n\n\n\n\n\n\n\n\n\nThe relevant columns for our data study are DAY_TYPE, TIME_PER_HOUR, ORIGIN_PT_CODE, TOTAL_TRIPS\nWe do not need the DESTINATION_PT_CODE as we are only interested on when passengers get on the bus.\nFurthermore, the ORIGIN_PT_CODE can be correlated to the BUS_STOP_N column of busstops data.\n\n\n\n\n\n\n\n\n\nRecap of busstops data\n\n\n\n\n\n\nkable(head(busstops))\n\n\n\n\n\n\n\n\n\n\nBUS_STOP_N\nBUS_ROOF_N\nLOC_DESC\ngeometry\n\n\n\n\n22069\nB06\nOPP CEVA LOGISTICS\nPOINT (13576.31 32883.65)\n\n\n32071\nB23\nAFT TRACK 13\nPOINT (13228.59 44206.38)\n\n\n44331\nB01\nBLK 239\nPOINT (21045.1 40242.08)\n\n\n96081\nB05\nGRACE INDEPENDENT CH\nPOINT (41603.76 35413.11)\n\n\n11561\nB05\nBLK 166\nPOINT (24568.74 30391.85)\n\n\n66191\nB03\nAFT CORFE PL\nPOINT (30951.58 38079.61)\n\n\n\n\n\n\n\n\n\n\nCleaning the data\nBefore going deep in the wrangling, we will clean up the data so that we are left with a lightweight data set that R can process more easily. We will retain and rename columns below to make them more understandable and easier to join with other data sets.\n\nDAY_TYPE\nTIME_PER_HOUR -&gt; HOUR_OF_DAY\nORIGIN_PT_CODE -&gt; BUS_STOP_N\nTOTAL_TRIPS -&gt; TRIPS\n\nWe will also rename the columns to make them more understandable and will make joining with other data sets easier.\nLastly, will also convert BUS_STOP_N to factor as it has a finite set of values so we can convert it to categorical data to make it easier to work with.\n\ntrips &lt;- odbus %&gt;%\n  select(c(ORIGIN_PT_CODE, DAY_TYPE, TIME_PER_HOUR, TOTAL_TRIPS)) %&gt;%\n  rename(BUS_STOP_N = ORIGIN_PT_CODE) %&gt;%\n  rename(HOUR_OF_DAY = TIME_PER_HOUR) %&gt;%\n  rename(TRIPS = TOTAL_TRIPS)\ntrips$BUS_STOP_N &lt;- as.factor(trips$BUS_STOP_N)\nkable(head(trips))\n\n\n\n\nBUS_STOP_N\nDAY_TYPE\nHOUR_OF_DAY\nTRIPS\n\n\n\n\n04168\nWEEKENDS/HOLIDAY\n16\n3\n\n\n04168\nWEEKDAY\n16\n5\n\n\n80119\nWEEKENDS/HOLIDAY\n14\n3\n\n\n80119\nWEEKDAY\n14\n5\n\n\n44069\nWEEKDAY\n17\n4\n\n\n20281\nWEEKENDS/HOLIDAY\n17\n1\n\n\n\n\n\n\n\n\n\n\n\nselect() is used to select the columns we need.\nrename() is used to rename the columns.\n\n\n\n\n\n\n\n\n\nData clear point\n\n\n\n\n\nWe do not need odbus anymore as we will be working with the more lightweight trips from this point on.\n\nrm(odbus)\n\n\n\n\n\n\nMapping the bus stops to hexagon\nIn [###Filtering hexagons with bus stops] we were able to overlay the bus locations to our generated hexagon. While this is enough for visualization, it is not enough for the rest of the data processing we need.\nFrom [###Cleaning the data], we have the BUS_STOP_N in the that we can use to associate with busstops.\nWe need to create an aspatial table that contain BUS_STOP_N and HEX_ID of the hexagon containing them. We will use st_intersection().\n\n\n\n\n\n\nWhy aspatial?\n\n\n\n\n\nWe want to use generate a simple mapping here as this table will serve as a “glue” between the other aspatial data sets and our geospatial data, honeycomb.\n\n\n\n\nbs_hex &lt;- st_intersection(busstops, honeycomb) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(c(BUS_STOP_N, HEX_ID))\nkable(head(bs_hex))\n\n\n\n\n\nBUS_STOP_N\nHEX_ID\n\n\n\n\n3265\n25059\nH0001\n\n\n254\n26379\nH0002\n\n\n2566\n25751\nH0003\n\n\n2893\n25761\nH0003\n\n\n4199\n26389\nH0004\n\n\n2399\n26369\nH0005\n\n\n\n\n\n\n\n\n\n\n\nst_intersection() - find which hexagon contains the bus stop\nst_drop_geometry() - to make data aspatial\nselect() - to retain only the relevant columns: BUS_STOP_N and HEX_ID\n\n\n\n\n\nAdding HEX_ID information to bus trips data\nTo achieve our goal of having the hourly # of bus trips per location, we need to add HEX_ID to trips data. This is so we can answer, how many bus trip originate from a certain hexagon?\nTo do this, we will do an inner_join() to join the trips data with bs_hex.\n\n\n\n\n\n\nWhy `inner_join()` instead of `left_join()`?\n\n\n\n\n\nWe will use inner_join as there are BUS_STOP_N values in trips data that are not in bs_hex.\n\ntrips$BUS_STOP_N[!(trips$BUS_STOP_N %in% bs_hex$BUS_STOP_N)] %&gt;%\n  unique() %&gt;% length()\n\n[1] 63\n\n\nThere are 63 bus stops in trips that are not in bs_hex. 5 of this can be attributed the bus stops we removed in [####About those points outside Singapore]. Others may be due to the BusStops data set not having complete information.\nNonetheless, we have to remove these bus stops from our analysis as we do not have geospatial data to associate to the hexagons.\nTherefore, we will use inner_join to keep only the observations in trips with the matching bus stops in bs_hex.\n\n\n\n\ntrips &lt;- inner_join(trips, bs_hex)\nkable(head(trips))\n\n\n\n\nBUS_STOP_N\nDAY_TYPE\nHOUR_OF_DAY\nTRIPS\nHEX_ID\n\n\n\n\n04168\nWEEKENDS/HOLIDAY\n16\n3\nH0955\n\n\n04168\nWEEKDAY\n16\n5\nH0955\n\n\n80119\nWEEKENDS/HOLIDAY\n14\n3\nH1102\n\n\n80119\nWEEKDAY\n14\n5\nH1102\n\n\n44069\nWEEKDAY\n17\n4\nH0410\n\n\n20281\nWEEKENDS/HOLIDAY\n17\n1\nH0388\n\n\n\n\n\n\n\nAggregating TRIPS based on HEX_ID\nNext, we will add the TRIPS for all the bus stops within a hexagon. We will group via HEX_ID, DAY_TYPE, and HOUR_OF_DAY.\n\ntrips &lt;- trips %&gt;%\n  group_by(\n    HEX_ID,\n    DAY_TYPE,\n    HOUR_OF_DAY) %&gt;%\n  summarise(TRIPS = sum(TRIPS))\nkable(head(trips))\n\n\n\n\nHEX_ID\nDAY_TYPE\nHOUR_OF_DAY\nTRIPS\n\n\n\n\nH0001\nWEEKDAY\n7\n74\n\n\nH0001\nWEEKDAY\n8\n19\n\n\nH0001\nWEEKDAY\n9\n10\n\n\nH0001\nWEEKDAY\n10\n7\n\n\nH0001\nWEEKDAY\n16\n26\n\n\nH0001\nWEEKDAY\n17\n122\n\n\n\n\n\n\n\n\n\n\n\nSave point\n\n\n\n\n\ntrips was processed from Passenger Volume By Origin Destination Bus Stops, which has almost 6 million observations.\nWe now have a more lightweight dataset with almost 60,000 observations, which is about 100x smaller.\nLet’s save this data as an rds file so we don’t need to reprocess again later on.\n\nwrite_rds(trips, \"data/rds/trips202310.rds\")"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#generating-time-series-cube-friendly-data",
    "href": "Take-home_Ex1/Take-home_Ex1.html#generating-time-series-cube-friendly-data",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Generating time series cube-friendly data",
    "text": "Generating time series cube-friendly data\n\n\n\n\n\n\nLoad point\n\n\n\n\n\nWe can run the rest of the document from this point by loading this data.\n\ntrips &lt;- read_rds(\"data/rds/trips202310.rds\")\nhoneycomb &lt;- read_rds(\"data/rds/honeycomb202310.rds\")\n\n\n\n\nWhen doing Emerging Hotspot Analysis (EHSA), we need to create a time series cube. To do that we must pass the following criteria:\n\nIt must have a row for each combination of HEX_ID (location) and HOUR_OF_DAY (time)\nThere are no missing values in TRIPS column\n\n\n\n\n\n\n\nIs our trips data time series cube-friendly? The answer is NO.\n\n\n\n\n\n\nspacetime(trips, honeycomb,\n          .loc_col = \"HEX_ID\",\n          .time_col = \"HOUR_OF_DAY\") %&gt;%\n  is_spacetime_cube()\n\n[1] FALSE\n\n\nWe do not pass the first requirement for generating a time series cube\n\nIt must have a row for each combination of HEX_ID (location) and HOUR_OF_DAY (time)\n\n\n\n\n\nGenerating the combinations\n\n\n\n\n\n\nHow many combinations are there? The answer is 36,456.\n\n\n\n\n\nTo satisfy the requirement of:\n\nIt must have a row for each combination of HEX_ID (location) and HOUR_OF_DAY (time)\n\nWe need to find out how many such combinations exist.\n\nThere are 1519 hexagons in our honeycomb\nThere are 24 hours in a day\n\nTherefore, there are \\(1519 \\times 24 = 36,456\\) combinations. We will use this value to verify if we have the correct space time cube.\n\n\n\nTo generate the combinations, we will use expand.grid() and for us to provide the list possible values for HEX_ID and HOUR_OF_DAY.\n\ncombos &lt;- expand_grid(\n  HEX_ID = honeycomb$HEX_ID,\n  HOUR_OF_DAY = 0:23\n)\nkable(combos[20:29,])\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\n\n\n\n\nH0001\n19\n\n\nH0001\n20\n\n\nH0001\n21\n\n\nH0001\n22\n\n\nH0001\n23\n\n\nH0002\n0\n\n\nH0002\n1\n\n\nH0002\n2\n\n\nH0002\n3\n\n\nH0002\n4\n\n\n\n\n\ncombos also has 36,456 rows, aligned with our expectations.\n\nnrow(combos)\n\n[1] 36456\n\n\nWith this generated, we can use this as a glue to generate our time series cube.\n\n\nSplitting the data\nAs we want to do separate analysis for weekdays and weekends, we will split the data. We will also remove the DAY_TYPE column as we do not need it anymore. To do this, we have to ungroup() before removing as we use DAY_TYPE as filter.\n\nWeekdayWeekend/Holidays\n\n\n\ntrips_wkdy &lt;- trips %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  ungroup() %&gt;%\n  select(, -DAY_TYPE)\nkable(trips_wkdy[20:29,])\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\n\n\n\n\nH0002\n16\n257\n\n\nH0002\n17\n159\n\n\nH0002\n18\n62\n\n\nH0002\n19\n42\n\n\nH0002\n20\n28\n\n\nH0002\n21\n2\n\n\nH0002\n22\n2\n\n\nH0003\n6\n44\n\n\nH0003\n7\n69\n\n\nH0003\n8\n72\n\n\n\n\n\n\n\n\ntrips_wknd &lt;- trips %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  ungroup() %&gt;%\n  select(, -DAY_TYPE)\nkable(trips_wknd[20:29,])\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\n\n\n\n\nH0002\n16\n42\n\n\nH0002\n17\n15\n\n\nH0002\n18\n32\n\n\nH0002\n19\n11\n\n\nH0002\n20\n13\n\n\nH0002\n21\n5\n\n\nH0002\n22\n2\n\n\nH0003\n6\n37\n\n\nH0003\n7\n40\n\n\nH0003\n8\n35\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChecking if split covers the full data\n\n\n\n\n\nLet’s check the total rows in trips_wkdy and trips_wknd add up to the number of rows in trips.\n\nnrow(trips_wkdy) + nrow(trips_wknd) == nrow(trips)\n\n[1] TRUE\n\n\nThere are no lost data so we can proceed to the next step.\n\n\n\n\n\n\n\n\n\nData clear point\n\n\n\n\n\nWe do not need trips anymore as we will be using trips_wkdy and trips_wknd from this point.\n\nrm(trips)\n\n\n\n\n\n\nFilling in the all the combos\nNow that we have separate data frames for weekday and weekend/holiday, we need to make sure that our data frame as all the combination in combos. We can do that by joining trips_wkxx with combos.\n\nWeekdayWeekend/Holiday\n\n\n\ntrips_cube_wkdy &lt;- left_join(combos, trips_wkdy)\nkable(head(trips_cube_wkdy, n = 24))\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\n\n\n\n\nH0001\n0\nNA\n\n\nH0001\n1\nNA\n\n\nH0001\n2\nNA\n\n\nH0001\n3\nNA\n\n\nH0001\n4\nNA\n\n\nH0001\n5\nNA\n\n\nH0001\n6\nNA\n\n\nH0001\n7\n74\n\n\nH0001\n8\n19\n\n\nH0001\n9\n10\n\n\nH0001\n10\n7\n\n\nH0001\n11\nNA\n\n\nH0001\n12\nNA\n\n\nH0001\n13\nNA\n\n\nH0001\n14\nNA\n\n\nH0001\n15\nNA\n\n\nH0001\n16\n26\n\n\nH0001\n17\n122\n\n\nH0001\n18\n224\n\n\nH0001\n19\n38\n\n\nH0001\n20\n6\n\n\nH0001\n21\nNA\n\n\nH0001\n22\nNA\n\n\nH0001\n23\nNA\n\n\n\n\n\nCheck if the output has the same rows as combos.\n\nnrow(trips_cube_wkdy) == nrow(combos)\n\n[1] TRUE\n\n\n\n\n\ntrips_cube_wknd &lt;- left_join(combos, trips_wknd)\nkable(head(trips_cube_wknd, n = 24))\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\n\n\n\n\nH0001\n0\nNA\n\n\nH0001\n1\nNA\n\n\nH0001\n2\nNA\n\n\nH0001\n3\nNA\n\n\nH0001\n4\nNA\n\n\nH0001\n5\nNA\n\n\nH0001\n6\nNA\n\n\nH0001\n7\n28\n\n\nH0001\n8\n9\n\n\nH0001\n9\n9\n\n\nH0001\n10\n2\n\n\nH0001\n11\nNA\n\n\nH0001\n12\nNA\n\n\nH0001\n13\nNA\n\n\nH0001\n14\nNA\n\n\nH0001\n15\nNA\n\n\nH0001\n16\n3\n\n\nH0001\n17\n21\n\n\nH0001\n18\n18\n\n\nH0001\n19\n14\n\n\nH0001\n20\n1\n\n\nH0001\n21\nNA\n\n\nH0001\n22\nNA\n\n\nH0001\n23\nNA\n\n\n\n\n\nCheck if the output has the same rows as combos.\n\nnrow(trips_cube_wknd) == nrow(combos)\n\n[1] TRUE\n\n\n\n\n\n\n\n\n\n\n\nThe data frames generated now passes\n\nIt must have a row for each combination of HEX_ID (location) and HOUR_OF_DAY (time)\n\n\n\n\n\n\n\n\n\n\nThe data frames generated violate\n\nThere are no missing values in TRIPS column\n\nThis is because there are some HOUR_OF_DAY where the value of TRIPS is NA. We need to fill in these missing values.\n\n\n\n\n\n\n\n\n\nData clear point\n\n\n\n\n\nWe do not need trips_wkxx anymore as we will be using trips_cube_wkxx from this point on.\n\nrm(trips_wkdy)\nrm(trips_wknd)\n\n\n\n\n\n\nFilling in missing values\nLastly, we need to fill in the missing values in TRIPS. This can be done by filtering the rows with NA and setting those to 0.\n\nWeekdayWeekend/Holiday\n\n\n\ntrips_cube_wkdy$TRIPS[is.na(trips_cube_wkdy$TRIPS)] &lt;- 0\nkable(head(trips_cube_wkdy, n = 24))\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\n\n\n\n\nH0001\n0\n0\n\n\nH0001\n1\n0\n\n\nH0001\n2\n0\n\n\nH0001\n3\n0\n\n\nH0001\n4\n0\n\n\nH0001\n5\n0\n\n\nH0001\n6\n0\n\n\nH0001\n7\n74\n\n\nH0001\n8\n19\n\n\nH0001\n9\n10\n\n\nH0001\n10\n7\n\n\nH0001\n11\n0\n\n\nH0001\n12\n0\n\n\nH0001\n13\n0\n\n\nH0001\n14\n0\n\n\nH0001\n15\n0\n\n\nH0001\n16\n26\n\n\nH0001\n17\n122\n\n\nH0001\n18\n224\n\n\nH0001\n19\n38\n\n\nH0001\n20\n6\n\n\nH0001\n21\n0\n\n\nH0001\n22\n0\n\n\nH0001\n23\n0\n\n\n\n\n\n\n\n\ntrips_cube_wknd$TRIPS[is.na(trips_cube_wknd$TRIPS)] &lt;- 0\nkable(head(trips_cube_wknd, n = 24))\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\n\n\n\n\nH0001\n0\n0\n\n\nH0001\n1\n0\n\n\nH0001\n2\n0\n\n\nH0001\n3\n0\n\n\nH0001\n4\n0\n\n\nH0001\n5\n0\n\n\nH0001\n6\n0\n\n\nH0001\n7\n28\n\n\nH0001\n8\n9\n\n\nH0001\n9\n9\n\n\nH0001\n10\n2\n\n\nH0001\n11\n0\n\n\nH0001\n12\n0\n\n\nH0001\n13\n0\n\n\nH0001\n14\n0\n\n\nH0001\n15\n0\n\n\nH0001\n16\n3\n\n\nH0001\n17\n21\n\n\nH0001\n18\n18\n\n\nH0001\n19\n14\n\n\nH0001\n20\n1\n\n\nH0001\n21\n0\n\n\nH0001\n22\n0\n\n\nH0001\n23\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs our data frame time series cube-friendly? The answer is YES.\n\n\n\n\n\nLet us check of our data frame can be used to create spacetime cubes.\n\nWeekend\n\nspacetime(trips_cube_wkdy, honeycomb,\n          .loc_col = \"HEX_ID\",\n          .time_col = \"HOUR_OF_DAY\") %&gt;%\n  is_spacetime_cube()\n\n[1] TRUE\n\n\n\n\nWeekend/Holiday\n\nspacetime(trips_cube_wknd, honeycomb,\n          .loc_col = \"HEX_ID\",\n          .time_col = \"HOUR_OF_DAY\") %&gt;%\n  is_spacetime_cube()\n\n[1] TRUE\n\n\n\n\n\n\n\n\n\n\n\n\nSave point\n\n\n\n\n\ntrips_cube_wkxx data is part the Goal data sets we need. Hence, we will save them.\n\nwrite_rds(trips_cube_wkdy, \"data/rds/trips_cube_wkdy202310.rds\")\nwrite_rds(trips_cube_wknd, \"data/rds/trips_cube_wknd202310.rds\")"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#creating-spatial-data-for-peak-period-traffic",
    "href": "Take-home_Ex1/Take-home_Ex1.html#creating-spatial-data-for-peak-period-traffic",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Creating spatial data for peak period traffic",
    "text": "Creating spatial data for peak period traffic\nAs we want to study the data for peak times, we have to extract and aggregate the data for those times.\nWe will create a geospatial object with columns for peak_period trips and join them with the aggregation data.\n\npeak_trips_sf &lt;- honeycomb\n\nWe will need filter the data from relevant hours using filter(). We also need to aggregate the data to get the total number of trips per hexagon, using group_by(HEX_ID) and summarise().\n\n\n\n\n\n\nHow to filter data by HOUR_OF_DAY\n\n\n\n\n\nThe HOUR_OF_DAY in data set covers the data from the start to the end of the hour in 24-hour format, i.e. when HOUR_OF_DAY = 16, this means bus taps from 4:00 PM ton4:59:59PM.\nHence, if we want to get 6 to 9am data, we will filter by:\nHOUR_DAY &gt;= 6 & HOUR_OF_DAY &lt; 9\n\n\n\n\nWeekday AM (6 - 9 AM)Weekday PM (5 - 8 PM)Weekend AM (11 AM - 2 PM)Weekend PM (4 - 7 PM)\n\n\n\npeak_trips_sf &lt;- trips_cube_wkdy %&gt;%\n  filter(\n    HOUR_OF_DAY &gt;= 6 &\n      HOUR_OF_DAY &lt; 9\n  ) %&gt;%\n  group_by(HEX_ID) %&gt;%\n  summarise(WEEKDAY_AM_TRIPS = sum(TRIPS)) %&gt;%\n  right_join(peak_trips_sf)\nkable(head(peak_trips_sf))\n\n\n\n\nHEX_ID\nWEEKDAY_AM_TRIPS\ngeometry\n\n\n\n\nH0001\n93\nPOLYGON ((3917.538 28017.41…\n\n\nH0002\n73\nPOLYGON ((4417.538 30615.49…\n\n\nH0003\n185\nPOLYGON ((4667.538 28450.43…\n\n\nH0004\n265\nPOLYGON ((4667.538 30182.48…\n\n\nH0005\n43\nPOLYGON ((4667.538 31048.5,…\n\n\nH0006\n54\nPOLYGON ((4917.538 28883.44…\n\n\n\n\n\n\n\n\npeak_trips_sf &lt;- trips_cube_wkdy %&gt;%\n  filter(\n    HOUR_OF_DAY &gt;= 17 &\n      HOUR_OF_DAY &lt; 20\n  ) %&gt;%\n  group_by(HEX_ID) %&gt;%\n  summarise(WEEKDAY_PM_TRIPS = sum(TRIPS)) %&gt;%\n  right_join(peak_trips_sf)\nkable(head(peak_trips_sf))\n\n\n\n\n\n\n\n\n\n\nHEX_ID\nWEEKDAY_PM_TRIPS\nWEEKDAY_AM_TRIPS\ngeometry\n\n\n\n\nH0001\n384\n93\nPOLYGON ((3917.538 28017.41…\n\n\nH0002\n263\n73\nPOLYGON ((4417.538 30615.49…\n\n\nH0003\n1812\n185\nPOLYGON ((4667.538 28450.43…\n\n\nH0004\n259\n265\nPOLYGON ((4667.538 30182.48…\n\n\nH0005\n211\n43\nPOLYGON ((4667.538 31048.5,…\n\n\nH0006\n259\n54\nPOLYGON ((4917.538 28883.44…\n\n\n\n\n\n\n\n\npeak_trips_sf &lt;- trips_cube_wknd %&gt;%\n  filter(\n    HOUR_OF_DAY &gt;= 11 &\n      HOUR_OF_DAY &lt; 14\n  ) %&gt;%\n  group_by(HEX_ID) %&gt;%\n  summarise(WEEKEND_AM_TRIPS = sum(TRIPS)) %&gt;%\n  right_join(peak_trips_sf)\nkable(head(peak_trips_sf))\n\n\n\n\n\n\n\n\n\n\n\nHEX_ID\nWEEKEND_AM_TRIPS\nWEEKDAY_PM_TRIPS\nWEEKDAY_AM_TRIPS\ngeometry\n\n\n\n\nH0001\n0\n384\n93\nPOLYGON ((3917.538 28017.41…\n\n\nH0002\n41\n263\n73\nPOLYGON ((4417.538 30615.49…\n\n\nH0003\n154\n1812\n185\nPOLYGON ((4667.538 28450.43…\n\n\nH0004\n75\n259\n265\nPOLYGON ((4667.538 30182.48…\n\n\nH0005\n56\n211\n43\nPOLYGON ((4667.538 31048.5,…\n\n\nH0006\n18\n259\n54\nPOLYGON ((4917.538 28883.44…\n\n\n\n\n\n\n\n\npeak_trips_sf &lt;- trips_cube_wknd %&gt;%\n  filter(\n    HOUR_OF_DAY &gt;= 16 &\n      HOUR_OF_DAY &lt; 19\n  ) %&gt;%\n  group_by(HEX_ID) %&gt;%\n  summarise(WEEKEND_PM_TRIPS = sum(TRIPS)) %&gt;%\n  right_join(peak_trips_sf)\nkable(head(peak_trips_sf))\n\n\n\n\n\n\n\n\n\n\n\n\nHEX_ID\nWEEKEND_PM_TRIPS\nWEEKEND_AM_TRIPS\nWEEKDAY_PM_TRIPS\nWEEKDAY_AM_TRIPS\ngeometry\n\n\n\n\nH0001\n42\n0\n384\n93\nPOLYGON ((3917.538 28017.41…\n\n\nH0002\n89\n41\n263\n73\nPOLYGON ((4417.538 30615.49…\n\n\nH0003\n275\n154\n1812\n185\nPOLYGON ((4667.538 28450.43…\n\n\nH0004\n94\n75\n259\n265\nPOLYGON ((4667.538 30182.48…\n\n\nH0005\n42\n56\n211\n43\nPOLYGON ((4667.538 31048.5,…\n\n\nH0006\n41\n18\n259\n54\nPOLYGON ((4917.538 28883.44…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConverting back to sf data type\n\n\n\n\n\nSince we used right_join(), peak_trips_sf became a tbl_df data type.\nWe need convert the peak_trips_sf back to sf data type so it can be processed as a spatial data.\n\npeak_trips_sf &lt;- peak_trips_sf %&gt;% st_sf()"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#visualizing-the-data",
    "href": "Take-home_Ex1/Take-home_Ex1.html#visualizing-the-data",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Visualizing the data",
    "text": "Visualizing the data\n\nInitial look at the data\nLet us plot maps for each peak period. I’m using tabsets for this so we can see the differences in data when switching from 1 tab to the other\n\nWeekday AM (6 - 9 AM)Weekday PM (5 - 8 PM)Weekend AM (11 AM - 2 PM)Weekend PM (4 - 7 PM)\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKDAY_AM_TRIPS\",\n    style = \"quantile\",\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekdays 6 - 9 AM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKDAY_PM_TRIPS\",\n    style = \"quantile\",\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekdays 5 - 8 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_AM_TRIPS\",\n    style = \"quantile\",\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 11 AM - 2 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_PM_TRIPS\",\n    style = \"quantile\",\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 4 - 7 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nThe visualizations above are satisfactory if we look at them individually as we can see which areas are busier than the others.\nHowever, comparing the commuter patterns from a peak period to another can be misleading.\nTo illustrate, consider the values of the darkest red on weekends. Hexagons with 6000 trips already are dark red on weekend maps. However, these values fall under the middle category, visualized as orange, on weekdays.\nDue to this, one might misinterpret that an area is busier on certain peak periods due to the difference in colors when in fact, they are actually as busy or even less busy. Colors are powerful and easier to interpret than looking at the categories in the legend.\nAs it is, we are comparing apples to oranges. We need a way to compare these maps apples to apples.\n\n\n\n\nDeriving the break points\nTo compare the maps apples to apples, we need to calculate the break points that our maps can use. When tmap uses style = quantile, it calculates the styles depending on the number of categories.\nWe can replicate this by using quantile() and using the full range of data from all the peak periods. To see the differences in more detail, we will use 8 categories instead of the default 5 categories.\n\n\n\n\n\n\nHow to calculate probs?\n\n\n\n\n\nSince we want 8 categories, we will divide 100 by 8.\n\\[\n\\frac{100}{8} = 12.5\n\\]\nHence, we will supply multiples of 0.125 as probs, e.g. 0, 0.125, 0.25, … 1\n\n\n\n\nquantile(\n  c(\n    peak_trips_sf$WEEKDAY_AM_TRIPS,\n    peak_trips_sf$WEEKDAY_PM_TRIPS,\n    peak_trips_sf$WEEKEND_AM_TRIPS,\n    peak_trips_sf$WEEKEND_PM_TRIPS\n  ),\n  probs = c(0, 0.125, 0.25, 0.375, 0.50, 0.625, 0.75, 0.875, 1)\n)\n\n        0%      12.5%        25%      37.5%        50%      62.5%        75% \n     0.000    165.000    631.750   1544.375   2985.000   5275.875   9179.750 \n     87.5%       100% \n 17850.750 462160.000 \n\n\nWith this result, instead of using style = quantile in our maps, we can specify these values to breaks.\n\n\n\n\n\n\nWhy not just use summary()?\n\n\n\n\n\nsummary() uses quartiles or every 25th quantile. This results in only 4 categories. This is not enough for the level detail we want to present so it’s better to use quantile() to generate more categories ourselves.\n\n\n\n\n\nRemapping with the breaks\nWe will supply the values generated (rounded to the nearest integer) to breaks, instead of style = \"quantile\".\nbreaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160)\n\nWeekday AM (6 - 9 AM)Weekday PM (5 - 8 PM)Weekend AM (11 AM - 2 PM)Weekend PM (4 - 7 PM)\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKDAY_AM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekdays 6 - 9 AM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nThere is a stark difference is colors in this map, showing a concentration of places with the darkest reds, along with areas of lighter colors.\nWe may infer that this concentration could be from residential areas due to people leaving their homes for their daily activities like work or school.\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKDAY_PM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekdays 5 - 8 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nThis peak period looks busiest because it looks reddest. However we must take some caution on this interpretation as the reds are more scattered so our brains may interpret this as red all throughout, compared to clusters of red and white.\nThis peak period is when workers and student most likely go home so it may look more scattered as workplaces or schools may be more scattered compared to residential areas.\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_AM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 11 AM - 2 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nThis map looks a lot paler compared to the weekday maps. This could be because it there is no work or school, people tend to stay home.\nHowever, while paler, people going out should still come from their homes so this should just look like the lighter weekday morning map. We will explore this later when we compare these 2 peak periods.\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_PM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 4 - 7 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nThis map also looks lighter due to it being a weekend and people staying home. However, another possible explanation is that people have more freedom to schedule their activities on weekends or holidays so the traffic may be more scattered throughout the day.\nA spatio-temporal analysis may reveal more information that can validate this inference.\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nDoing the adjustments made it much easier to compare peak hours compared to the others.\nFor example, we are able to see that weekdays are indeed generally busier than weekends due to the weekends due to the maps looking darker.\nWe can also observe that the West and Northwest parts of Singapore has less bus trips compared to the rest of Singapore. The cause could be fewer residents in that area, sparse distribution of the population, or people preferring to use other modes of transportation (e.g., MRT, cars).\nHowever, some details were lost with this adjustment. For example, we are not able to see what is the highest and lowest number of trips for each peak period. If this is the intention or the narrative we want, the previous unadjusted visualizations are more effective for these purposes.\nFor our analysis, we want to compare the number of bus trips from one peak period to the other so making the adjustment in the break points is very helpful for this purpose.\n\n\n\n\nComparing peak periods\nNow that our maps can be compared apples to apples, we will compare 2 peak periods side by side.\n\n\n\n\n\n\nTo compare each pair of maps more easily, we will use tabs so we can just switch between them to see how the patterns change from one map to the other.\n\n\n\n\nWeekday peak periods\n\nWeekday AM (6 - 9 AM)Weekday PM (5 - 8 PM)\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKDAY_AM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekdays 6 - 9 AM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKDAY_PM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekdays 5 - 8 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nAn interesting observation is that when switch from weekend AM to weekend PM map, there is perceived outward scattering from the darker areas to the neighboring areas.\nSwitching to weekend PM to weekend AM shows the reds converging to the darker points.\nThis is consistent with real world pattern of people going out of their homes for their daily activities in the morning and going home in the evening.\nHowever, what cannot be verified from this comparison is whether people really going to work or school in places close to their homes, as suggested by outside scattering. This may not be necessarily true as people from different areas of Singapore may travel long distances or so to their places of work and school, not the neighboring areas. A flow analysis will give us better insights about this.\n\n\n\n\nWeekend peak periods\n\n\n\n\n\n\nAdjusting the break points\n\n\n\n\n\nFor this comparison, we adjusted the break points using the same method as in Deriving the break points. This is because the traffic is much less over the weekend so we are not able to use the full range of categories we previously derived.\nSee the maps below.\n\nWeekend AM (11 AM - 2 PM)Weekend PM (4 - 7 PM)\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_AM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 11 AM - 2 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_PM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 4 - 7 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\nHence, we couldn’t get insights to the same level of details as the other comparisons. We will use the following breaks for this comparison:\n\nquantile(\n  c(\n    peak_trips_sf$WEEKEND_AM_TRIPS,\n    peak_trips_sf$WEEKEND_PM_TRIPS\n  ),\n  probs = c(0, 0.125, 0.25, 0.375, 0.50, 0.625, 0.75, 0.875, 1)\n)\n\n        0%      12.5%        25%      37.5%        50%      62.5%        75% \n     0.000    113.000    382.000    854.750   1686.000   2861.625   4616.250 \n     87.5%       100% \n  7785.375 111171.000 \n\n\nWith this change, we have shifted 7786 from the 6th to 8th category, providing us with the better level of detail on values between 0 to 7786.\nThis just shows that the breaks we calculated before is not appropriate for all comparisons. We still need to apply the scale appropriate to the data we have,\n\n\n\n\nWeekend AM (11 AM - 2 PM)Weekend PM (4 - 7 PM)\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_AM_TRIPS\",\n    breaks = c(0, 113, 382, 855, 1686, 2862, 4616, 7785, 111171),\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 11 AM - 2 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_PM_TRIPS\",\n    breaks = c(0, 113, 382, 855, 1686, 2862, 4616, 7785, 111171),\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 4 - 7 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nLike the weekday comparison, there is a slight scattering observed, although not as stark. The difference is not as big despite Adjusting the break points.\nThis very small shift could be because people are coming out of their homes throughout the day as they are free to schedule their activities throughout the day on weekends or holidays.\nThis is consistent with the insights in Weekend PM (4 - 7 PM). Again, a spatio-temporal analysis may provide us more information to verify this inference.\n\n\n\n\nMorning peaks\n\nWeekday AM (6 - 9 AM)Weekend AM (11 AM - 2 PM)\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKDAY_AM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekdays 6 - 9 AM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_AM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 11 AM - 2 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nThe weekend map generally looks like a lighter version of the weekday map. This may mean that less people go out of their homes on weekends.\nHowever, there are some notable things that can be observed.\n\nSome busy areas on the West during weekdays have low bus trips on weekends\n\nThis area corresponds to what seems to be an industrial area near the Tuas checkpoint.\nThis looks strange to me as it contradicts our hypothesis that morning bus rides are mostly due to people commuting from their home.\n\nLocations that coincide with bus interchanges are constantly busy\n\nThere are multiple bus lines originating from these interchanges so they are expected to have constant flow of commuters.\n\nThe bus stops for international travel are constantly busy\n\nExamples are those in Woodlands Checkpoint, Kranji Station, Changi Airport bus stops\nThese are popular for tourists and locals alike as Sinagapore is a business and tourism hub in Asia. Workers also commute regularly between Johor Bahru and Singapore, while locals visit Johor Bahru for weekend recreation.\n\nThe bus stops to areas popular to tourists are constantly busy\n\nSome examples are the Vivo City bus stops and the bus stop near Singapore Zoo (North of Central Water Catchment). Tourists come to this places throughout the week, and locals also go too these places for recreation.\n\n\n\n\n\n\nEvening peaks\n\nWeekday PM (5 - 8 PM)Weekend PM (4 - 7 PM)\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKDAY_PM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekdays 5 - 8 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_PM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"## of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 4 - 7 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nThere is not much difference in insights compared to Morning peaks aside from the active bus stop near Tuas Checkpoint on weekday mornings is not active in the evenings.\nThis is an interesting observation that is difficult to explain. One possible explanations is because of its relative proximity to Nanyang Technological University, students commuting to NTU transfer via these bus stops going to school. However, we may not see the same pattern during weekday evening peak period because university students have different schedules so their time leaving the university may be more scattered throughout the day.\nA flow analysis with information on the destination bus stop may reveal more information about this.\n\n\n\n\n\n\n\n\nWhy do we seem notice the busiest areas first?\n\n\n\n\n\nAnother interesting insight about these visualizations is that we seem notice the busy areas in the map more easily than the areas that have low traffic.\nInterestingly, our biology may play into this as our eyes have more cones that are sensitive to red light than any other type. As we use red to visualized the busiest areas in our map, they are the ones that catches our eye first.\nIf we had reversed our color scheme, perhaps we would have noticed the least busy areas first.\nHowever, we have to look into more reliable scientific and psychology sources to verify if this phenomenon is true.\n\n\n\n\n\n\n\n\n\nSave point\n\n\n\n\n\nWe will use peak_trips_sf for Local Indicators of Spatial Association (LISA) analysis.\n\nwrite_rds(peak_trips_sf, \"data/rds/peak_trips_sf202310.rds\")"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#computing-adaptive-distance-weights-matrix",
    "href": "Take-home_Ex1/Take-home_Ex1.html#computing-adaptive-distance-weights-matrix",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Computing adaptive distance weights matrix",
    "text": "Computing adaptive distance weights matrix\nBefore getting Moran’s I, we need to get the spatial weights first. We will use adaptive distance-based weights for this, which uses the k-nearest neighbors.\n\n\n\n\n\n\nWhy not contiguity weights?\n\n\n\n\n\nVisualizing the honeycomb grid reveals that there are hexagon islands, or hexagons without contiguity neighbors.\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(\"green\", title = \"Singapore Boundary\", alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_fill(col = \"white\", title = \"Hexagons\", alpha = 1) +\n  tm_borders(alpha = 0.2) +\n  tm_layout(main.title = \"Honeycomb grid corresponding to Singapore bus stops\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2, bg.color = \"white\", bg.alpha = 0.5) +\n  tm_scale_bar(bg.color = \"white\", bg.alpha = 0.5) +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nSome of these are at the East side of Singapore, around Changi Airport.\nWe cannot associate them to another hexagon if we use contiguity neighbors so it will be difficult to do the analysis. With that, we will use distance-based weights to ensure that all hexagons have neighbors.\n\n\n\nAs we are working with hexagons, which has 6 sides, it is logical to get the 6 nearest neighbors and use it for our analysis.\n\nknn6_nb &lt;- peak_trips_sf %&gt;% st_centroid() %&gt;% st_knn(k=6)\nhead(knn6_nb, n = 3)\n\n[[1]]\n[1]  3  6  7 11 21 22\n\n[[2]]\n[1]  4  5  8  9 12 16\n\n[[3]]\n[1]  1  6  7 11 21 22\n\n\n\n\n\n\n\n\nst_centroid() - used to get the centroids of each hexagon, which is required for st_knn()\nst_knn() - used to get the k-nearest neighbors\n\n\n\n\n\n\n\n\n\nSave point\n\n\n\n\n\nWe will save knn6_nb to be used as neighbor list for Emerging Hot Spot Analysis (EHSA).\n\nwrite_rds(knn6_nb, \"data/rds/knn6_nb202310.rds\")\n\n\n\n\n\nwm_knn6 &lt;- peak_trips_sf %&gt;%\n  mutate(\n    nb = knn6_nb,\n    #wt = st_weights(nb, style = \"W\")\n    wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1)\n    )"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#calculating-global-morans-i",
    "href": "Take-home_Ex1/Take-home_Ex1.html#calculating-global-morans-i",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Calculating Global Moran’s I",
    "text": "Calculating Global Moran’s I\nTo check for signs of clustering, we will first calculate the global Moran’s I value.\n\nWhen \\(I &gt; 0\\), observations are similar (sign of clustering)\nWhen \\(I &lt; 0\\), observations are dissimilar (low indication of clustering)\nIf \\(p &lt; \\alpha\\) (0.05), the result is significant and did not happen by chance\n\nWe will do this for all peak periods.\nTo perform the permutation test, we will use global_moran_perm() to perform Monte-Carlo simulations.\n\nWeekday morningWeekday eveningWeekend morningWeekend evening\n\n\n\nglobal_moran_perm(\n  wm_knn6$WEEKDAY_AM_TRIPS,\n  wm_knn6$nb,\n  wm_knn6$wt,\n  nsim = 99\n)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.22173, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\\(0.21487 &gt; 0\\) means there are signs of clustering\n\\(p-value &lt; 2.2\\times10^{-16} &lt; 0.05\\) means the result is significant\n\n\n\n\nglobal_moran_perm(\n  wm_knn6$WEEKDAY_PM_TRIPS,\n  wm_knn6$nb,\n  wm_knn6$wt,\n  nsim = 99\n)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.05726, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\\(0.057327 &gt; 0\\) means there are signs of clustering\n\\(p-value &lt; 2.2\\times10^{-16} &lt; 0.05\\) means the result is significant\n\n\n\n\nglobal_moran_perm(\n  wm_knn6$WEEKEND_AM_TRIPS,\n  wm_knn6$nb,\n  wm_knn6$wt,\n  nsim = 99\n)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.16267, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\\(0.15955 &gt; 0\\) means there are signs of clustering\n\\(p-value &lt; 2.2\\times10^{-16} &lt; 0.05\\) means the result is significant\n\n\n\n\nglobal_moran_perm(\n  wm_knn6$WEEKEND_PM_TRIPS,\n  wm_knn6$nb,\n  wm_knn6$wt,\n  nsim = 99\n)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.1057, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\\(0.10265 &gt; 0\\) means there are signs of clustering\n\\(p-value &lt; 2.2\\times10^{-16} &lt; 0.05\\) means the result is significant\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nIn all peak periods, \\(I &gt; 0\\) and \\(p &lt; \\alpha\\) (0.05) so there are signs of geospatial clustering in the data."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#calculating-local-morans-i",
    "href": "Take-home_Ex1/Take-home_Ex1.html#calculating-local-morans-i",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Calculating Local Moran’s I",
    "text": "Calculating Local Moran’s I\nAs we have verified the existence of clusters, we can now proceed to looking for those clusters.\n\nWeekday morningWeekday eveningWeekend morningWeekend evening\n\n\n\nlmi_wkdy_am &lt;- wm_knn6 %&gt;% \n  mutate(local_moran = local_moran(\n    WEEKDAY_AM_TRIPS, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\nlmi_wkdy_pm &lt;- wm_knn6 %&gt;% \n  mutate(local_moran = local_moran(\n    WEEKDAY_PM_TRIPS, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\nlmi_wknd_am &lt;- wm_knn6 %&gt;% \n  mutate(local_moran = local_moran(\n    WEEKDAY_AM_TRIPS, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\nlmi_wknd_pm &lt;- wm_knn6 %&gt;% \n  mutate(local_moran = local_moran(\n    WEEKDAY_AM_TRIPS, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#plotting-local-morans-i-and-p-value",
    "href": "Take-home_Ex1/Take-home_Ex1.html#plotting-local-morans-i-and-p-value",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Plotting Local Moran’s I and p-value",
    "text": "Plotting Local Moran’s I and p-value\n\n\n\n\n\n\nFunction for plotting ii and p_ii_sim maps side by side\n\n\n\n\n\nWe will create a function to plot the local Moran’s I and p-values side by side. This will allow for easy comparison of the 2 maps.\n\nplot_lmi_pv &lt;- function(lmi) {\n  tmap_arrange(\n    tm_shape(mpsz) +\n      tm_fill(col=\"green\") +\n      tm_borders(alpha = 0.5) +\n      tm_shape(lmi) +\n      tm_polygons(\"ii\"),\n    tm_shape(mpsz) +\n      tm_fill(col=\"green\") +\n      tm_borders(alpha = 0.5) +\n      tm_shape(lmi) +\n      tm_polygons(\"p_ii_sim\",\n              breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\"),\n              palette = \"YlOrRd\"\n              ),\n    ncol = 2\n  )\n}\n\n\n\n\n\nWeekday morningWeekday eveningWeekend morningWeekend evening\n\n\n\n\nShow the code\nplot_lmi_pv(lmi_wkdy_am)\n\n\n\n\n\n\n\n\n\nShow the code\nplot_lmi_pv(lmi_wkdy_pm)\n\n\n\n\n\n\n\n\n\nShow the code\nplot_lmi_pv(lmi_wkdy_pm)\n\n\n\n\n\n\n\n\n\nShow the code\nplot_lmi_pv(lmi_wkdy_pm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nBig part of the maps show that most of the hexagons’ p-values resulted into insignificant values. This means that those areas in red do not have clusters.\nVisually, they all look similar. However, we can see the clusters and outliers more clearly once we plot the LISA maps."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#plotting-lisa-maps",
    "href": "Take-home_Ex1/Take-home_Ex1.html#plotting-lisa-maps",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Plotting LISA Maps",
    "text": "Plotting LISA Maps\nWe will plot the Local Indicator of Spatial Association (LISA) maps to pinpoint where the clusters and outliers are.\nTo do that, we will only include those hexagons for which the results are significant, or p_ii_sim is less than our \\(\\alpha\\) of 0.05.\n\nWeekday morningWeekday eveningWeekend morningWeekend evening\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col=\"white\")+\n  tm_borders(alpha = 0.5) +\n  tm_shape(lmi_wkdy_am %&gt;% filter(p_ii_sim &lt; 0.05)) +\n  tm_polygons(\"mean\") + \n  tm_layout(\n    main.title = \"LISA for Weekday 6 - 9 AM\",\n    main.title.position = \"center\",\n    main.title.size = 1\n  )\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nHigh-high areas can be observed at the Causeway bus stops. This can be attributed to workers living in Johor Bahru commuting to Singapore for work causing this bus stops to be busy in the mornings. Larger high-high clusters during this peak period. Most of them seem to correspond to bus interchanges.\nSome high-low bus stops are can also be observed at the West. This could be key transfer points in these areas for people going to work or school.\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col=\"white\")+\n  tm_borders(alpha = 0.5) +\n  tm_shape(lmi_wkdy_pm %&gt;% filter(p_ii_sim &lt; 0.05)) +\n  tm_polygons(\"mean\") + \n  tm_layout(\n    main.title = \"LISA for Weekday 5 - 8 PM\",\n    main.title.position = \"center\",\n    main.title.size = 1\n  )\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nCompared to weekday mornings, there are smaller clusters in the Central part of Singapore. This could indicate that traffic is more distributed in this area and people riding buses are more spread out.\nOne surprising observation is that the Causeway bus stops are not in a cluster during this peak period. It could indicate either lower traffic or more spread out traffic. One reason could be there are less people coming into the country during this time period.\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col=\"white\")+\n  tm_borders(alpha = 0.5) +\n  tm_shape(lmi_wknd_am %&gt;% filter(p_ii_sim &lt; 0.05)) +\n  tm_polygons(\"mean\") + \n  tm_layout(\n    main.title = \"LISA for Weekend/Holiday 11 AM - 2 PM\",\n    main.title.position = \"center\",\n    main.title.size = 1\n  )\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nThe clusters here coincide the most with weekday morning clusters. This could indicate that people are riding the bus from the same place, like their home.\nSome notable differences are there is no cluster on the Causeway bus stops (less workers coming in to Singapore), and a high-low outlier in the Southern part. It is in close proximity to NUS Hospital and Kent Ridge Park. so perhaps people go for their morning run or visit the hospital.\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col=\"white\")+\n  tm_borders(alpha = 0.5) +\n  tm_shape(lmi_wknd_pm %&gt;% filter(p_ii_sim &lt; 0.05)) +\n  tm_polygons(\"mean\") + \n  tm_layout(\n    main.title = \"LISA for Weekend/Holiday 4- 7 PM\",\n    main.title.position = \"center\",\n    main.title.size = 1\n  )\n\n\n\n\n\n\n\n\n\n\n\nThe high-low outlier in the South part of Singapore that is present in weekend morning is also present here. This is a potential recreation area or a destination for weekend errands/\nAnother key feature is the triangle of high-low outliers in the West (around Tuas and Jurong area). This pattern is also present in weekday morning, and weekend morning peak hours. These strongly supports the idea that these bus stops could be key transfer bus stops in this area.\nIn addition, high-high clusters of 3 or more hexagons are present in the same areas but are not present in the weekday evening peak hours. This can indicate similar behavior of people commuting from their homes.\nLastly, the Causeway bus stops are in a high-high cluster again, same as in weekday morning. This can indicate people coming back to Singapore after a short trip to Malaysia over the weekend.\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nThere are low-low clusters in the West and Northwest parts of the country. This is consistent with the previous observations that these areas have less traffic. In addition, other low-low clusters were also observed at the edges of Singapore like in the East and South. This could suggest that people in these areas use alternative modes of transportation like the MRT.\nAnother key observation is that the low-high clusters are in close proximity to the high-high clusters. On the other hand, high-low clusters are usually on their own. This could indicate that the bus stops in the low-high cluster are under-utilized because people prefer to use the bus stops in the nearby high-high areas there are less bus routes passing through these stops."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#selecting-time-period-to-study",
    "href": "Take-home_Ex1/Take-home_Ex1.html#selecting-time-period-to-study",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Selecting time period to study",
    "text": "Selecting time period to study\nTo perform the Emerging Hot Spot Analysis (EHSA), we will select 6-hour periods that covers each of the peak periods.\n\n\n\n\n\n\nWhy we need to select a 6 hour period\n\n\n\n\n\nAs much as we want to do the test for the whole 24 hours, doing emerging_hotspot_analysis() is an expensive operation.\nMann-Kendall numbers are calculated based on the number of elements in the spacetime cube.\nThe cube has \\(n_{location}\\times t\\) items, which is in this case 36456. Furthermore, comparing items to other items has a complexity \\(O(n^2)\\) of. Hence, emerging_hotspot_analysis() is an \\(O(n_{location}^2t^2)\\) operation.\nIf we compare a 24-hr period to processing a 6-hr period:\n\\[\nn_{location}^2t_{24hr}^2 = n_{location}^2(4t_{6hr})^2 = 16n_{location}^2t_{6hr}^2\n\\]\nCalculating the full 24-hr period can take 16x longer that doing the same operation for a 6-hour time period.\nIs such, we will select 2 6-hr periods each for weekday and weekend data sets, covering the peak hours.\n\n\n\n\nInspecting the hourly trip data\nTo help us select the time periods to study, let us take a look a the hourly distribution of the data.\nFrom the results below, the median trips for 1 &lt;= HOUR_OF_DAY &lt; 5 is 0. This means that most bus routes are not in service. So we will exclude them from the study.\n\nWeekdayWeekend\n\n\n\nkable(head(\n  trips_cube_wkdy %&gt;%\n    group_by(HOUR_OF_DAY) %&gt;%\n    summarise(\n      min = min(TRIPS),\n      median = median(TRIPS),\n      max = max(TRIPS)\n  ), n = 24))\n\n\n\n\nHOUR_OF_DAY\nmin\nmedian\nmax\n\n\n\n\n0\n0\n27\n12150\n\n\n1\n0\n0\n1450\n\n\n2\n0\n0\n14\n\n\n3\n0\n0\n0\n\n\n4\n0\n0\n85\n\n\n5\n0\n99\n31239\n\n\n6\n0\n1126\n77075\n\n\n7\n0\n2343\n134511\n\n\n8\n0\n2157\n124501\n\n\n9\n0\n1371\n61747\n\n\n10\n0\n1091\n42929\n\n\n11\n0\n1162\n45570\n\n\n12\n0\n1322\n60276\n\n\n13\n0\n1214\n70819\n\n\n14\n0\n1010\n70324\n\n\n15\n0\n1170\n72720\n\n\n16\n0\n1591\n90961\n\n\n17\n0\n2488\n129719\n\n\n18\n0\n2359\n184941\n\n\n19\n0\n1420\n147500\n\n\n20\n0\n847\n107809\n\n\n21\n0\n563\n95025\n\n\n22\n0\n372\n74793\n\n\n23\n0\n164\n41258\n\n\n\n\n\nMorningpeak: 5 - 11AM, covers the peak period of 5 - 9AM\nEvening peak: 3 - 9PM, covers the peak period of 5 - 8PM\n\n\n\nkable(head(\n  trips_cube_wknd %&gt;%\n    group_by(HOUR_OF_DAY) %&gt;%\n    summarise(\n      min = min(TRIPS),\n      median = median(TRIPS),\n      max = max(TRIPS)\n  ), n = 24))\n\n\n\n\nHOUR_OF_DAY\nmin\nmedian\nmax\n\n\n\n\n0\n0\n14\n6101\n\n\n1\n0\n0\n727\n\n\n2\n0\n0\n0\n\n\n3\n0\n0\n0\n\n\n4\n0\n0\n22\n\n\n5\n0\n20\n4151\n\n\n6\n0\n178\n12103\n\n\n7\n0\n344\n20180\n\n\n8\n0\n475\n22296\n\n\n9\n0\n514\n21268\n\n\n10\n0\n566\n21567\n\n\n11\n0\n584\n23487\n\n\n12\n0\n579\n28469\n\n\n13\n0\n526\n32932\n\n\n14\n0\n453\n33007\n\n\n15\n0\n454\n32787\n\n\n16\n0\n522\n33743\n\n\n17\n0\n594\n37178\n\n\n18\n0\n549\n40663\n\n\n19\n0\n437\n40543\n\n\n20\n0\n294\n41284\n\n\n21\n0\n215\n38345\n\n\n22\n0\n156\n31849\n\n\n23\n0\n79\n19388"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#creating-spacetime-cube-for-the-peak-periods",
    "href": "Take-home_Ex1/Take-home_Ex1.html#creating-spacetime-cube-for-the-peak-periods",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Creating spacetime cube for the peak periods",
    "text": "Creating spacetime cube for the peak periods\n\nExtracting data for the peak periods\nNow that we have selected the periods we are interested in, we will extract data needed for those by using filter(). We will then use these for creating the spacetime cube.\n\nWeekdayWeekend\n\n\n\ntrips_cube_wkdy_am &lt;- trips_cube_wkdy %&gt;%\n  filter(HOUR_OF_DAY &gt;= 5 &\n           HOUR_OF_DAY &lt; 11)\n\ntrips_cube_wkdy_pm &lt;- trips_cube_wkdy %&gt;%\n  filter(HOUR_OF_DAY &gt;= 15 &\n           HOUR_OF_DAY &lt; 21)\n\n\n\n\ntrips_cube_wknd_am &lt;- trips_cube_wknd %&gt;%\n  filter(HOUR_OF_DAY &gt;= 9 &\n           HOUR_OF_DAY &lt; 15)\n\ntrips_cube_wknd_pm &lt;- trips_cube_wknd %&gt;%\n  filter(HOUR_OF_DAY &gt;= 15 &\n           HOUR_OF_DAY &lt; 21)\n\n\n\n\n\n\n\n\n\n\nData clear point\n\n\n\n\n\nWe do not need trips_cube_wkdy and trips_cube_wknd anymore so we can remove them from the environment.\n\nrm(trips_cube_wkdy)\nrm(trips_cube_wknd)\n\n\n\n\n\n\nBuilding the spacetime cubes\nFrom the extracted peak period data, we will create the spacetime cubes using spacetime. We will use HEX_ID as the time parameter, and HOUR_OF_DAY as the location parameter.\nNext, the function needs a geometry to be passed along the aspatial data. We will use honeycomb for this.\nLastly, we will use is_spacetime_cube() to check the validity of the generated spacetime cube.\n\nWeekday morningWeekday eveningWeekend morningWeekend evening\n\n\n\nwkdy_am_st &lt;- spacetime(trips_cube_wkdy_am, honeycomb,\n                        .loc_col = \"HEX_ID\",\n                        .time_col = \"HOUR_OF_DAY\")\nis_spacetime_cube(wkdy_am_st)\n\n[1] TRUE\n\n\n\n\n\nwkdy_pm_st &lt;- spacetime(trips_cube_wkdy_pm, honeycomb,\n                        .loc_col = \"HEX_ID\",\n                        .time_col = \"HOUR_OF_DAY\")\nis_spacetime_cube(wkdy_pm_st)\n\n[1] TRUE\n\n\n\n\n\nwknd_am_st &lt;- spacetime(trips_cube_wknd_am, honeycomb,\n                        .loc_col = \"HEX_ID\",\n                        .time_col = \"HOUR_OF_DAY\")\nis_spacetime_cube(wknd_am_st)\n\n[1] TRUE\n\n\n\n\n\nwknd_pm_st &lt;- spacetime(trips_cube_wknd_pm, honeycomb,\n                        .loc_col = \"HEX_ID\",\n                        .time_col = \"HOUR_OF_DAY\")\nis_spacetime_cube(wknd_pm_st)\n\n[1] TRUE"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#computing-local-gi",
    "href": "Take-home_Ex1/Take-home_Ex1.html#computing-local-gi",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Computing local Gi*",
    "text": "Computing local Gi*\nBefore performing the EHSA, we will need to calculate the local Gi* values first. This will help us associating hexagons to their respective neighbors, which is critical for the analysis.\n\nCalculating inverse distance weights\nIn performing any geospatial analysis, we need to calculate spatial weights. We will use the neighbor list we generated from LISA analysis, which used k-near neighbors, with k=6. Inverse distance weights will also be used so that the association between regions is stronger the closer they are.\n\n\n\n\n\n\nWe will use include_self() in neighbor list as we are calculating Gi* values.\nWe will calculate this individually for each spacetime cube as in Running the simulations, the neighbors and weights must be in the geometry context of the spacetime cube.\n\n\n\n\nWeekday morningWeekday eveningWeekend morningWeekend evening\n\n\n\nwkdy_am_st &lt;- wkdy_am_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(knn6_nb),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\nkable(tail(wkdy_am_st, n = 3))\n\n\n\n\n\n\n\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\nnb\nwt\n\n\n\n\nH1517\n10\n675\n1511, 1513, 1514, 1515, 1516, 1517, 1518\n0.0011547005, 0.0010000000, 0.0020000000, 0.0020000000, 0.0011547005, 0.0000000000, 0.0007559289\n\n\nH1518\n10\n258\n1510, 1513, 1514, 1515, 1516, 1517, 1518\n0.0010000000, 0.0011547005, 0.0010000000, 0.0005547002, 0.0020000000, 0.0007559289, 0.0000000000\n\n\nH1519\n10\n192\n1498, 1500, 1501, 1505, 1506, 1512, 1519\n0.0002096570, 0.0002309401, 0.0002073903, 0.0003049971, 0.0001970659, 0.0004364358, 0.0000000000\n\n\n\n\n\n\n\n\nwkdy_pm_st &lt;- wkdy_pm_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(knn6_nb),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\nkable(tail(wkdy_pm_st, n = 3))\n\n\n\n\n\n\n\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\nnb\nwt\n\n\n\n\nH1517\n20\n2377\n1511, 1513, 1514, 1515, 1516, 1517, 1518\n0.0011547005, 0.0010000000, 0.0020000000, 0.0020000000, 0.0011547005, 0.0000000000, 0.0007559289\n\n\nH1518\n20\n173\n1510, 1513, 1514, 1515, 1516, 1517, 1518\n0.0010000000, 0.0011547005, 0.0010000000, 0.0005547002, 0.0020000000, 0.0007559289, 0.0000000000\n\n\nH1519\n20\n167\n1498, 1500, 1501, 1505, 1506, 1512, 1519\n0.0002096570, 0.0002309401, 0.0002073903, 0.0003049971, 0.0001970659, 0.0004364358, 0.0000000000\n\n\n\n\n\n\n\n\nwknd_am_st &lt;- wknd_am_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(knn6_nb),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\nkable(tail(wknd_am_st, n = 3))\n\n\n\n\n\n\n\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\nnb\nwt\n\n\n\n\nH1517\n14\n293\n1511, 1513, 1514, 1515, 1516, 1517, 1518\n0.0011547005, 0.0010000000, 0.0020000000, 0.0020000000, 0.0011547005, 0.0000000000, 0.0007559289\n\n\nH1518\n14\n60\n1510, 1513, 1514, 1515, 1516, 1517, 1518\n0.0010000000, 0.0011547005, 0.0010000000, 0.0005547002, 0.0020000000, 0.0007559289, 0.0000000000\n\n\nH1519\n14\n72\n1498, 1500, 1501, 1505, 1506, 1512, 1519\n0.0002096570, 0.0002309401, 0.0002073903, 0.0003049971, 0.0001970659, 0.0004364358, 0.0000000000\n\n\n\n\n\n\n\n\nwknd_pm_st &lt;- wknd_pm_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(knn6_nb),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\nkable(tail(wknd_pm_st, n = 3))\n\n\n\n\n\n\n\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\nnb\nwt\n\n\n\n\nH1517\n20\n519\n1511, 1513, 1514, 1515, 1516, 1517, 1518\n0.0011547005, 0.0010000000, 0.0020000000, 0.0020000000, 0.0011547005, 0.0000000000, 0.0007559289\n\n\nH1518\n20\n0\n1510, 1513, 1514, 1515, 1516, 1517, 1518\n0.0010000000, 0.0011547005, 0.0010000000, 0.0005547002, 0.0020000000, 0.0007559289, 0.0000000000\n\n\nH1519\n20\n0\n1498, 1500, 1501, 1505, 1506, 1512, 1519\n0.0002096570, 0.0002309401, 0.0002073903, 0.0003049971, 0.0001970659, 0.0004364358, 0.0000000000\n\n\n\n\n\n\n\n\n\n\nCalculating local Gi* using local_gstar_perm\nAs we included the hexagons themselves in their own neighbor list in Calculating inverse distance weights, we can proceed with calculating the local GI*.\n\nWeekday morningWeekday eveningWeekend morningWeekend evening\n\n\n\ngi_st_wkdy_am &lt;- wkdy_am_st %&gt;% \n  group_by(HOUR_OF_DAY) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    TRIPS, nb, wt)) %&gt;% \n  unnest(gi_star)\n\n\n\n\ngi_st_wkdy_pm &lt;- wkdy_pm_st %&gt;% \n  group_by(HOUR_OF_DAY) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    TRIPS, nb, wt)) %&gt;% \n  unnest(gi_star)\n\n\n\n\ngi_st_wknd_am &lt;- wknd_am_st %&gt;% \n  group_by(HOUR_OF_DAY) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    TRIPS, nb, wt)) %&gt;% \n  unnest(gi_star)\n\n\n\n\ngi_st_wknd_pm &lt;- wknd_pm_st %&gt;% \n  group_by(HOUR_OF_DAY) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    TRIPS, nb, wt)) %&gt;% \n  unnest(gi_star)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#mann-kendall-test",
    "href": "Take-home_Ex1/Take-home_Ex1.html#mann-kendall-test",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Mann-Kendall Test",
    "text": "Mann-Kendall Test\nBy using the Gi* we just calculated, let us look for notable patterns in the data.\nWe will plot the Gi* for 5 hexagons with emerging patterns.\n\nWeekday morningWeekday eveningWeekend morningWeekend evening\n\n\n\ntop5_wkdy_am &lt;- gi_st_wkdy_am %&gt;%\n  group_by(HEX_ID) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk) %&gt;%\n  arrange(sl, abs(tau)) %&gt;%\n  head()\n\n\n\nShow the code\nggplot(\n  data = gi_st_wkdy_am %&gt;%\n    filter(HEX_ID %in% top5_wkdy_am$HEX_ID),\n  aes(x = HOUR_OF_DAY, \n      y = gi_star,\n      group = HEX_ID)) +\n  geom_line(\n    aes(color=HEX_ID),\n    size = 1) +\n  theme_light() +\n  labs(title = \"Gi* for Weekdays (5 - 11 AM)\")\n\n\n\n\n\nThe line shows increasing trend, with different slopes. This indicates presence of hot spots. It is most likely that these hexagons are not neighbors as they are not closely associated to each other.\n\n\n\ntop5_wkdy_pm &lt;- gi_st_wkdy_pm %&gt;%\n  group_by(HEX_ID) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk) %&gt;%\n  arrange(sl, abs(tau)) %&gt;%\n  head()\n\n\n\nShow the code\nggplot(\n  data = gi_st_wkdy_pm %&gt;%\n    filter(HEX_ID %in% top5_wkdy_pm$HEX_ID),\n  aes(x = HOUR_OF_DAY, \n      y = gi_star,\n      group = HEX_ID)) +\n  geom_line(\n    aes(color=HEX_ID),\n    size = 1) +\n  theme_light() +\n  labs(title = \"Gi* for Weekdays (3 - 9 PM)\")\n\n\n\n\n\nUnlike the weekday morning trend line, these lines look very similar to each other. They may be neighbors as they show strong association with it other. It can indicate that we have a cluster of emerging hot spots for weekday evening.\n\n\n\ntop5_wknd_am &lt;- gi_st_wknd_am %&gt;%\n  group_by(HEX_ID) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk) %&gt;%\n  arrange(sl, abs(tau)) %&gt;%\n  head()\n\n\n\nShow the code\nggplot(\n  data = gi_st_wknd_am %&gt;%\n    filter(HEX_ID %in% top5_wknd_am$HEX_ID),\n  aes(x = HOUR_OF_DAY, \n      y = gi_star,\n      group = HEX_ID)) +\n  geom_line(\n    aes(color=HEX_ID),\n    size = 1) +\n  theme_light() +\n  labs(title = \"Gi* for Weekends/Holidays (9 AM - 3 PM)\")\n\n\n\n\n\nThese lines converged to a very close bunch of lines. The HEX_ID are also very close to each other, making it likely that they are neighbors. We should expect a hot spot made up of at least 5 hexagons when we plot our map later.\n\n\n\ntop5_wknd_pm &lt;- gi_st_wknd_pm %&gt;%\n  group_by(HEX_ID) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk) %&gt;%\n  arrange(sl, abs(tau)) %&gt;%\n  head()\n\n\n\nShow the code\nggplot(\n  data = gi_st_wknd_pm %&gt;%\n    filter(HEX_ID %in% top5_wknd_pm$HEX_ID),\n  aes(x = HOUR_OF_DAY, \n      y = gi_star,\n      group = HEX_ID)) +\n  geom_line(\n    aes(color=HEX_ID),\n    size = 1) +\n  theme_light() +\n  labs(title = \"Gi* for Weekends/Holidays (9 AM - 3 PM)\")\n\n\n\n\n\nUnlike the weekend morning graph, we do not see the same clustering of the lines for weekend evening. The hot spot clusters may be smaller than in weekend morning, which can indicate that traffic is less concentrated."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#emerging-hot-spot-analysis",
    "href": "Take-home_Ex1/Take-home_Ex1.html#emerging-hot-spot-analysis",
    "title": "Take Home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Emerging Hot Spot Analysis",
    "text": "Emerging Hot Spot Analysis\n\nRunning the simulations\nWe will now perform the Emerging Hot Spot Analysis with Monte-Carlo simulations using emerging_hotspot_analysis().\n\n\n\n\n\n\nWe will specify the nb_col and wt_col to the ones we used when calculating the spatial weights.\nThis is because emerging_hotspot_analysis() uses contiguity neighbors and standard weights. We used k-nearest neighbors with k = 6 to generate the neighbor list so we will retain this for consistency.\n\n\n\n\n\n\n\n\n\nWarning: This is a relatively expensive calculation\n\n\n\n\n\nThis code chunk takes a relatively long time to run so it is set not to run with eval: false by default. We will also save it to an rds file so we do not need to recalculate the result all the time.\nThe file size is small, 87.9KB, so saving this file is a great way to cache the result for this expensive calculation.\nPlease set code chunks to run with eval: true, or manually trigger the run on Rstudio if running for the first time.\n\n\n\n\nWeekday morningWeekday eveningWeekend morningWeekend evening\n\n\n\nehsa_wkdy_am &lt;- emerging_hotspot_analysis(\n  x = wkdy_am_st, \n  .var = \"TRIPS\", \n  k = 1, \n  nsim = 99,\n  nb_col = \"nb\",\n  wt_col = \"wt\"\n)\nwrite_rds(ehsa_wkdy_am, \"data/rds/ehsa_wkdy_am202310.rds\")\n\n\n\n\nehsa_wkdy_pm &lt;- emerging_hotspot_analysis(\n  x = wkdy_pm_st, \n  .var = \"TRIPS\", \n  k = 1, \n  nsim = 99,\n  nb_col = \"nb\",\n  wt_col = \"wt\"\n)\nwrite_rds(ehsa_wkdy_pm, \"data/rds/ehsa_wkdy_pm202310.rds\")\n\n\n\n\nehsa_wknd_am &lt;- emerging_hotspot_analysis(\n  x = wknd_am_st, \n  .var = \"TRIPS\", \n  k = 1, \n  nsim = 99,\n  nb_col = \"nb\",\n  wt_col = \"wt\"\n)\nwrite_rds(ehsa_wknd_am, \"data/rds/ehsa_wknd_am202310.rds\")\n\n\n\n\nehsa_wknd_pm &lt;- emerging_hotspot_analysis(\n  x = wknd_pm_st, \n  .var = \"TRIPS\", \n  k = 1, \n  nsim = 99,\n  nb_col = \"nb\",\n  wt_col = \"wt\"\n)\nwrite_rds(ehsa_wknd_pm, \"data/rds/ehsa_wknd_pm202310.rds\")\n\n\n\n\n\n\n\n\n\n\nLoad point\n\n\n\nWe will reload the previously generated ehsa_wk*_* data.\n\nehsa_wkdy_am &lt;- read_rds(\"data/rds/ehsa_wkdy_am202310.rds\")\nehsa_wkdy_pm &lt;- read_rds(\"data/rds/ehsa_wkdy_pm202310.rds\")\nehsa_wknd_am &lt;- read_rds(\"data/rds/ehsa_wknd_am202310.rds\")\nehsa_wknd_pm &lt;- read_rds(\"data/rds/ehsa_wknd_pm202310.rds\")\n\n\n\n\n\nVisualizing EHSA\nWe will now visualize the results by plotting the categories for the areas where significant trends are detected.\nAs our \\(\\alpha\\) is 0.05, we will be filtering the values for which the p-value &lt; 0.05 as it is for these areas where the trends observed where significant and did not happen by chance.\nNext, we will join the filtered values with honeycomb and transforming it to a sf object for it to be processed as a geospatial data.\n\n\n\n\n\n\nAddressing the palette\n\n\n\n\n\nTo show the EHSA classes as reds for hot spots and blues for cold spots, we will assign numbers to each category. This way the palette will identify the number values instead of sorting the categories alphabetically.\nReference for the categories: https://www.azavea.com/blog/2017/08/15/emerging-hot-spot-spatial-statistics/\n\nehsa_colors &lt;- data.frame(\n  CLASS = c(\n      \"persistent coldspot\", \"consecutive coldspot\", \"intensifying coldspot\",\n      \"sporadic coldspot\", \"new coldspot\", \"oscillating coldspot\",\n      \"historical coldspot\", \"diminishing coldspot\",\n      \"no pattern detected\",\n      \"diminishing hotspot\", \"historical hotspot\",\n      \"oscillating hotspot\", \"new hotspot\", \"sporadic hotspot\",\n      \"intensifying hotspot\", \"consecutive hotspot\", \"persistent hotspot\"\n    ),\n  LEVEL = -8:8\n)\n\nWe will use this table to join with the EHSA plot to visualize the hot spots and cold spots appropriately.\n\n\n\n\nWeekday morningWeekday eveningWeekend morningWeekend evening\n\n\n\n\nShow the code\nehsa_sig_wkdy_am &lt;- ehsa_wkdy_am %&gt;%\n    filter(p_value &lt; 0.05) %&gt;%\n    left_join(honeycomb,\n              by = join_by(location == HEX_ID)) %&gt;%\n    left_join(ehsa_colors,\n              by = join_by(classification == CLASS)) %&gt;%\n    st_sf()\n\ntm_shape(mpsz) +\n  tm_polygons(col = \"white\") +\ntm_shape(ehsa_sig_wkdy_am) +\n  tm_polygons(\n    \"LEVEL\",\n    palette = \"-Spectral\",\n    breaks = -8:9,\n    labels = ehsa_colors$CLASS,\n    title = \"Legend\"\n  ) +\n  tm_layout(main.title = \"Hot Spots and Cold Spots (Weekdays 5AM - 11PM)\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.4, \n            legend.width = 0.4,\n            legend.bg.color = \"white\",\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(bg.color = \"white\") +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nOn weekday mornings, hot spots can be observed in residential areas like Clementi (with the biggest cluster of hot spots), Queenstown, Bukit Merah, and Toa Payoh. This aligns with our other observations as people are expected to come out of their homes to go to work or school in the morning.\nOn the other hand, cold spots are observed in other residential areas like Punggol, Jurong, and Choa Chu Kang areas. A possible explanation is that alternative modes of transportation like the MRT or cars is preferred by people living in these areas.\n\n\n\n\nShow the code\nehsa_sig_wkdy_pm &lt;- ehsa_wkdy_pm %&gt;%\n    filter(p_value &lt; 0.05) %&gt;%\n    left_join(honeycomb,\n              by = join_by(location == HEX_ID)) %&gt;%\n    left_join(ehsa_colors,\n              by = join_by(classification == CLASS)) %&gt;%\n    st_sf()\n  \ntm_shape(mpsz) +\n  tm_polygons(col = \"white\") +\ntm_shape(ehsa_sig_wkdy_pm) +\n  tm_polygons(\n    \"LEVEL\",\n    palette = \"-Spectral\",\n    breaks = -8:9,\n    labels = ehsa_colors$CLASS,\n    title = \"Legend\"\n  ) +\n  tm_layout(main.title = \"Hot Spots and Cold Spots (Weekdays 3 - 9PM)\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.4, \n            legend.width = 0.4,\n            legend.bg.color = \"white\",\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(bg.color = \"white\") +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nInterestingly, the trend is reversed on weekday evenings. There are hot spots in the residential areas that were cold spots in the morning — Punggol, Jurong, and Choa Chu Kang areas. Other notable additions to these hot spots are Woodlands and Yishun, other residential areas.\nThis may mean that people in the area take the MRT in the morning going to office or work, but take the bus home in the evening.\n\n\n\n\nShow the code\nehsa_sig_wknd_am &lt;- ehsa_wknd_am %&gt;%\n    filter(p_value &lt; 0.05) %&gt;%\n    left_join(honeycomb,\n              by = join_by(location == HEX_ID)) %&gt;%\n    left_join(ehsa_colors,\n              by = join_by(classification == CLASS)) %&gt;%\n    st_sf()\n  \ntm_shape(mpsz) +\n  tm_polygons(col = \"white\") +\ntm_shape(ehsa_sig_wknd_am) +\n  tm_polygons(\n    \"LEVEL\",\n    palette = \"-Spectral\",\n    breaks = -8:9,\n    labels = ehsa_colors$CLASS,\n    title = \"Legend\"\n  ) +\n  tm_layout(main.title = \"Hot Spots and Cold Spots (Weekends/Holidays 9AM - 3PM)\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.4, \n            legend.width = 0.4,\n            legend.bg.color = \"white\",\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(bg.color = \"white\") +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nOn weekend mornings, there are notable hot spots on many residential hubs like Jurong, Tampines, Woodlands, Bedok, etc.\nThis can indicate consistent activity of people going out of their houses on weekend mornings. Having a lot of persistent hot spots indicate that this high activity is consistent throughout the morning.\nAnother thing of note is the big hot spot in the Central area. This coincides with the Orchard area, which is a very popular are for recreation.\n\n\n\n\nShow the code\nehsa_sig_wknd_pm &lt;- ehsa_wknd_pm %&gt;%\n    filter(p_value &lt; 0.05) %&gt;%\n    left_join(honeycomb,\n              by = join_by(location == HEX_ID)) %&gt;%\n    left_join(ehsa_colors,\n              by = join_by(classification == CLASS)) %&gt;%\n    st_sf()\n  \ntm_shape(mpsz) +\n  tm_polygons(col = \"white\") +\ntm_shape(ehsa_sig_wknd_pm) +\n  tm_polygons(\n    \"LEVEL\",\n    palette = \"-Spectral\",\n    breaks = -8:9,\n    labels = ehsa_colors$CLASS,\n    title = \"Legend\"\n  ) +\n  tm_layout(main.title = \"Hot Spots and Cold Spots (Weekends 3PM - 9PM)\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.4, \n            legend.width = 0.4,\n            legend.bg.color = \"white\",\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(bg.color = \"white\") +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nThere are less hot spots compared to weekend mornings. Some residential areas are busier than others, notably Woodlands and Jurong areas.\nAnother key observation is the Causeway area in Woodlands are also hot spots, which indicate an influx of people coming into Singapore after spending the weekend in Malaysia."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1D.html",
    "href": "Take-home_Ex1/Take-home_Ex1D.html",
    "title": "Take-home Exercise 1D: Spatio-temporal Analysis with EHSA",
    "section": "",
    "text": "The aim of this study is to uncover spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.\nThe main modes of analysis to be used here are Local Indicators of Spatial Association (LISA) and Emerging Hot Spot Analysis (EHSA).\nIn doing these study, we will be looking at bus trips started during the hours below.\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday evening peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nMore details about the study can be found here.\nIn this part of the study, we will do spatio-temporal analysis with EHSA using bus commuter traffic data generated from Data Wrangling. We will also attempt the answer the Open Questions from Geovisualization and Analysis:\n\nWhat are the commuting patterns of people during weekdays? weekends?\nAre bus trips really more spread out throughout the day during weekend?"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1D.html#setting-up-the-r-environment",
    "href": "Take-home_Ex1/Take-home_Ex1D.html#setting-up-the-r-environment",
    "title": "Take-home Exercise 1D: Spatio-temporal Analysis with EHSA",
    "section": "Setting Up the R Environment",
    "text": "Setting Up the R Environment\nWe will load the following R packages needed for this study.\n\ntmap: for thematic mapping\nsf: for geospatial data handling\ntidyverse: for non-spatial data handling\nsfdep: for spatial analysis\nknitr:for prettifying presentation\nplotly: for interactive plots\n\n\npacman::p_load(sf, sfdep, tmap, tidyverse, plotly, knitr)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1D.html#environment-settings",
    "href": "Take-home_Ex1/Take-home_Ex1D.html#environment-settings",
    "title": "Take-home Exercise 1D: Spatio-temporal Analysis with EHSA",
    "section": "Environment settings",
    "text": "Environment settings\nWe will also set the default settings on for this document\n\ntmap_style to natural: for displaying the maps with preferred style\nset seed for reproducibility of results\n\n\ntmap_mode(\"plot\")\ntmap_style(\"natural\")\nset.seed(1234)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1D.html#loading-the-data",
    "href": "Take-home_Ex1/Take-home_Ex1D.html#loading-the-data",
    "title": "Take-home Exercise 1D: Spatio-temporal Analysis with EHSA",
    "section": "Loading the data",
    "text": "Loading the data\n\n\n\n\n\n\nImportant\n\n\n\nBefore running this part, please run all the code chunks in Data Wrangling as it generates the data needed for this document.\n\n\nUse read_rds() to load the rds data needed for geovisualization and analysis.\n\nmpsz &lt;- read_rds(\"data/rds/mpsz.rds\")\nhoneycomb &lt;- read_rds(\"data/rds/honeycomb202310.rds\")\nknn6_nb &lt;- read_rds(\"data/rds/knn6_nb202310.rds\")\ntrips_cube_wkdy &lt;- read_rds(\"data/rds/trips_cube_wkdy202310.rds\")\ntrips_cube_wknd &lt;- read_rds(\"data/rds/trips_cube_wknd202310.rds\")\n\n\nmpsz - Singapore boundary map for visualization\nhoneycomb - honeycomb grid containing bus stops in Singapore\ntrips_cube_wkdy - hourly bus commuter trips data for weekdays\ntrips_cube_wknd - hourly bus commuter trips data for weekend\nknn6_nb - Nearest 6 neighbors of each hexagon. Will be used as neighbor list for EHSA"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1D.html#inspecting-the-hourly-trip-data",
    "href": "Take-home_Ex1/Take-home_Ex1D.html#inspecting-the-hourly-trip-data",
    "title": "Take-home Exercise 1D: Spatio-temporal Analysis with EHSA",
    "section": "Inspecting the hourly trip data",
    "text": "Inspecting the hourly trip data\nTo help us select the time periods to study, let us take a look a the hourly distribution of the data.\nFrom the results below, the median trips for 1 &lt;= HOUR_OF_DAY &lt; 5 is 0. This means that most bus routes are not in service. So we will exclude them from the study.\n\nWeekdayWeekend\n\n\n\nkable(head(\n  trips_cube_wkdy %&gt;%\n    group_by(HOUR_OF_DAY) %&gt;%\n    summarise(\n      min = min(TRIPS),\n      median = median(TRIPS),\n      max = max(TRIPS)\n  ), n = 24))\n\n\n\n\nHOUR_OF_DAY\nmin\nmedian\nmax\n\n\n\n\n0\n0\n27\n12150\n\n\n1\n0\n0\n1450\n\n\n2\n0\n0\n14\n\n\n3\n0\n0\n0\n\n\n4\n0\n0\n85\n\n\n5\n0\n99\n31239\n\n\n6\n0\n1126\n77075\n\n\n7\n0\n2343\n134511\n\n\n8\n0\n2157\n124501\n\n\n9\n0\n1371\n61747\n\n\n10\n0\n1091\n42929\n\n\n11\n0\n1162\n45570\n\n\n12\n0\n1322\n60276\n\n\n13\n0\n1214\n70819\n\n\n14\n0\n1010\n70324\n\n\n15\n0\n1170\n72720\n\n\n16\n0\n1591\n90961\n\n\n17\n0\n2488\n129719\n\n\n18\n0\n2359\n184941\n\n\n19\n0\n1420\n147500\n\n\n20\n0\n847\n107809\n\n\n21\n0\n563\n95025\n\n\n22\n0\n372\n74793\n\n\n23\n0\n164\n41258\n\n\n\n\n\nMorningpeak: 5 - 11AM, covers the peak period of 5 - 9AM\nEvening peak: 3 - 9PM, covers the peak period of 5 - 8PM\n\n\n\nkable(head(\n  trips_cube_wknd %&gt;%\n    group_by(HOUR_OF_DAY) %&gt;%\n    summarise(\n      min = min(TRIPS),\n      median = median(TRIPS),\n      max = max(TRIPS)\n  ), n = 24))\n\n\n\n\nHOUR_OF_DAY\nmin\nmedian\nmax\n\n\n\n\n0\n0\n14\n6101\n\n\n1\n0\n0\n727\n\n\n2\n0\n0\n0\n\n\n3\n0\n0\n0\n\n\n4\n0\n0\n22\n\n\n5\n0\n20\n4151\n\n\n6\n0\n178\n12103\n\n\n7\n0\n344\n20180\n\n\n8\n0\n475\n22296\n\n\n9\n0\n514\n21268\n\n\n10\n0\n566\n21567\n\n\n11\n0\n584\n23487\n\n\n12\n0\n579\n28469\n\n\n13\n0\n526\n32932\n\n\n14\n0\n453\n33007\n\n\n15\n0\n454\n32787\n\n\n16\n0\n522\n33743\n\n\n17\n0\n594\n37178\n\n\n18\n0\n549\n40663\n\n\n19\n0\n437\n40543\n\n\n20\n0\n294\n41284\n\n\n21\n0\n215\n38345\n\n\n22\n0\n156\n31849\n\n\n23\n0\n79\n19388"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1D.html#extracting-data-for-the-peak-periods",
    "href": "Take-home_Ex1/Take-home_Ex1D.html#extracting-data-for-the-peak-periods",
    "title": "Take-home Exercise 1D: Spatio-temporal Analysis with EHSA",
    "section": "Extracting data for the peak periods",
    "text": "Extracting data for the peak periods\nNow that we have selected the periods we are interested in, we will extract data needed for those by using filter(). We will then use these for creating the spacetime cube.\n\nWeekdayWeekend\n\n\n\ntrips_cube_wkdy_am &lt;- trips_cube_wkdy %&gt;%\n  filter(HOUR_OF_DAY &gt;= 5 &\n           HOUR_OF_DAY &lt; 11)\n\ntrips_cube_wkdy_pm &lt;- trips_cube_wkdy %&gt;%\n  filter(HOUR_OF_DAY &gt;= 15 &\n           HOUR_OF_DAY &lt; 21)\n\n\n\n\ntrips_cube_wknd_am &lt;- trips_cube_wknd %&gt;%\n  filter(HOUR_OF_DAY &gt;= 9 &\n           HOUR_OF_DAY &lt; 15)\n\ntrips_cube_wknd_pm &lt;- trips_cube_wknd %&gt;%\n  filter(HOUR_OF_DAY &gt;= 15 &\n           HOUR_OF_DAY &lt; 21)\n\n\n\n\n\n\n\n\n\n\nData clear point\n\n\n\n\n\nWe do not need trips_cube_wkdy and trips_cube_wknd anymore so we can remove them from the environment.\n\nrm(trips_cube_wkdy)\nrm(trips_cube_wknd)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1D.html#building-the-spacetime-cubes",
    "href": "Take-home_Ex1/Take-home_Ex1D.html#building-the-spacetime-cubes",
    "title": "Take-home Exercise 1D: Spatio-temporal Analysis with EHSA",
    "section": "Building the spacetime cubes",
    "text": "Building the spacetime cubes\nFrom the extracted peak period data, we will create the spacetime cubes using spacetime. We will use HEX_ID as the time parameter, and HOUR_OF_DAY as the location parameter.\nNext, the function needs a geometry to be passed along the aspatial data. We will use honeycomb for this.\nLastly, we will use is_spacetime_cube() to check the validity of the generated spacetime cube.\n\nWeekday morningWeekday eveningWeekend morningWeekend evening\n\n\n\nwkdy_am_st &lt;- spacetime(trips_cube_wkdy_am, honeycomb,\n                        .loc_col = \"HEX_ID\",\n                        .time_col = \"HOUR_OF_DAY\")\nis_spacetime_cube(wkdy_am_st)\n\n[1] TRUE\n\n\n\n\n\nwkdy_pm_st &lt;- spacetime(trips_cube_wkdy_pm, honeycomb,\n                        .loc_col = \"HEX_ID\",\n                        .time_col = \"HOUR_OF_DAY\")\nis_spacetime_cube(wkdy_pm_st)\n\n[1] TRUE\n\n\n\n\n\nwknd_am_st &lt;- spacetime(trips_cube_wknd_am, honeycomb,\n                        .loc_col = \"HEX_ID\",\n                        .time_col = \"HOUR_OF_DAY\")\nis_spacetime_cube(wknd_am_st)\n\n[1] TRUE\n\n\n\n\n\nwknd_pm_st &lt;- spacetime(trips_cube_wknd_pm, honeycomb,\n                        .loc_col = \"HEX_ID\",\n                        .time_col = \"HOUR_OF_DAY\")\nis_spacetime_cube(wknd_pm_st)\n\n[1] TRUE"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1D.html#calculating-inverse-distance-weights",
    "href": "Take-home_Ex1/Take-home_Ex1D.html#calculating-inverse-distance-weights",
    "title": "Take-home Exercise 1D: Spatio-temporal Analysis with EHSA",
    "section": "Calculating inverse distance weights",
    "text": "Calculating inverse distance weights\nIn performing any geospatial analysis, we need to calculate spatial weights. We will use the neighbor list we generated from LISA analysis, which used k-near neighbors, with k=6. Inverse distance weights will also be used so that the association between regions is stronger the closer they are.\n\n\n\n\n\n\nWe will use include_self() in neighbor list as we are calculating Gi* values.\nWe will calculate this individually for each spacetime cube as in Running the simulations, the neighbors and weights must be in the geometry context of the spacetime cube.\n\n\n\n\nWeekday morningWeekday eveningWeekend morningWeekend evening\n\n\n\nwkdy_am_st &lt;- wkdy_am_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(knn6_nb),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\nkable(tail(wkdy_am_st, n = 3))\n\n\n\n\n\n\n\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\nnb\nwt\n\n\n\n\nH1517\n10\n675\n1511, 1513, 1514, 1515, 1516, 1517, 1518\n0.0011547005, 0.0010000000, 0.0020000000, 0.0020000000, 0.0011547005, 0.0000000000, 0.0007559289\n\n\nH1518\n10\n258\n1510, 1513, 1514, 1515, 1516, 1517, 1518\n0.0010000000, 0.0011547005, 0.0010000000, 0.0005547002, 0.0020000000, 0.0007559289, 0.0000000000\n\n\nH1519\n10\n192\n1498, 1500, 1501, 1505, 1506, 1512, 1519\n0.0002096570, 0.0002309401, 0.0002073903, 0.0003049971, 0.0001970659, 0.0004364358, 0.0000000000\n\n\n\n\n\n\n\n\nwkdy_pm_st &lt;- wkdy_pm_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(knn6_nb),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\nkable(tail(wkdy_pm_st, n = 3))\n\n\n\n\n\n\n\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\nnb\nwt\n\n\n\n\nH1517\n20\n2377\n1511, 1513, 1514, 1515, 1516, 1517, 1518\n0.0011547005, 0.0010000000, 0.0020000000, 0.0020000000, 0.0011547005, 0.0000000000, 0.0007559289\n\n\nH1518\n20\n173\n1510, 1513, 1514, 1515, 1516, 1517, 1518\n0.0010000000, 0.0011547005, 0.0010000000, 0.0005547002, 0.0020000000, 0.0007559289, 0.0000000000\n\n\nH1519\n20\n167\n1498, 1500, 1501, 1505, 1506, 1512, 1519\n0.0002096570, 0.0002309401, 0.0002073903, 0.0003049971, 0.0001970659, 0.0004364358, 0.0000000000\n\n\n\n\n\n\n\n\nwknd_am_st &lt;- wknd_am_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(knn6_nb),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\nkable(tail(wknd_am_st, n = 3))\n\n\n\n\n\n\n\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\nnb\nwt\n\n\n\n\nH1517\n14\n293\n1511, 1513, 1514, 1515, 1516, 1517, 1518\n0.0011547005, 0.0010000000, 0.0020000000, 0.0020000000, 0.0011547005, 0.0000000000, 0.0007559289\n\n\nH1518\n14\n60\n1510, 1513, 1514, 1515, 1516, 1517, 1518\n0.0010000000, 0.0011547005, 0.0010000000, 0.0005547002, 0.0020000000, 0.0007559289, 0.0000000000\n\n\nH1519\n14\n72\n1498, 1500, 1501, 1505, 1506, 1512, 1519\n0.0002096570, 0.0002309401, 0.0002073903, 0.0003049971, 0.0001970659, 0.0004364358, 0.0000000000\n\n\n\n\n\n\n\n\nwknd_pm_st &lt;- wknd_pm_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(knn6_nb),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\nkable(tail(wknd_pm_st, n = 3))\n\n\n\n\n\n\n\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\nnb\nwt\n\n\n\n\nH1517\n20\n519\n1511, 1513, 1514, 1515, 1516, 1517, 1518\n0.0011547005, 0.0010000000, 0.0020000000, 0.0020000000, 0.0011547005, 0.0000000000, 0.0007559289\n\n\nH1518\n20\n0\n1510, 1513, 1514, 1515, 1516, 1517, 1518\n0.0010000000, 0.0011547005, 0.0010000000, 0.0005547002, 0.0020000000, 0.0007559289, 0.0000000000\n\n\nH1519\n20\n0\n1498, 1500, 1501, 1505, 1506, 1512, 1519\n0.0002096570, 0.0002309401, 0.0002073903, 0.0003049971, 0.0001970659, 0.0004364358, 0.0000000000"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1D.html#calculating-local-gi-using-local_gstar_perm",
    "href": "Take-home_Ex1/Take-home_Ex1D.html#calculating-local-gi-using-local_gstar_perm",
    "title": "Take-home Exercise 1D: Spatio-temporal Analysis with EHSA",
    "section": "Calculating local Gi* using local_gstar_perm",
    "text": "Calculating local Gi* using local_gstar_perm\nAs we included the hexagons themselves in their own neighbor list in Calculating inverse distance weights, we can proceed with calculating the local GI*.\n\nWeekday morningWeekday eveningWeekend morningWeekend evening\n\n\n\ngi_st_wkdy_am &lt;- wkdy_am_st %&gt;% \n  group_by(HOUR_OF_DAY) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    TRIPS, nb, wt)) %&gt;% \n  unnest(gi_star)\n\n\n\n\ngi_st_wkdy_pm &lt;- wkdy_pm_st %&gt;% \n  group_by(HOUR_OF_DAY) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    TRIPS, nb, wt)) %&gt;% \n  unnest(gi_star)\n\n\n\n\ngi_st_wknd_am &lt;- wknd_am_st %&gt;% \n  group_by(HOUR_OF_DAY) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    TRIPS, nb, wt)) %&gt;% \n  unnest(gi_star)\n\n\n\n\ngi_st_wknd_pm &lt;- wknd_pm_st %&gt;% \n  group_by(HOUR_OF_DAY) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    TRIPS, nb, wt)) %&gt;% \n  unnest(gi_star)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1D.html#running-the-simulations",
    "href": "Take-home_Ex1/Take-home_Ex1D.html#running-the-simulations",
    "title": "Take-home Exercise 1D: Spatio-temporal Analysis with EHSA",
    "section": "Running the simulations",
    "text": "Running the simulations\nWe will now perform the Emerging Hot Spot Analysis with Monte-Carlo simulations using emerging_hotspot_analysis().\n\n\n\n\n\n\nWe will specify the nb_col and wt_col to the ones we used when calculating the spatial weights.\nThis is because emerging_hotspot_analysis() uses contiguity neighbors and standard weights. We used k-nearest neighbors with k = 6 to generate the neighbor list so we will retain this for consistency.\n\n\n\n\n\n\n\n\n\nWarning: This is a relatively expensive calculation\n\n\n\n\n\nThis code chunk takes a relatively long time to run so it is set not to run with eval: false by default. We will also save it to an rds file so we do not need to recalculate the result all the time.\nThe file size is small, 87.9KB, so saving this file is a great way to cache the result for this expensive calculation.\nPlease set code chunks to run with eval: true, or manually trigger the run on Rstudio if running for the first time.\n\n\n\n\nWeekday morningWeekday eveningWeekend morningWeekend evening\n\n\n\nehsa_wkdy_am &lt;- emerging_hotspot_analysis(\n  x = wkdy_am_st, \n  .var = \"TRIPS\", \n  k = 1, \n  nsim = 99,\n  nb_col = \"nb\",\n  wt_col = \"wt\"\n)\nwrite_rds(ehsa_wkdy_am, \"data/rds/ehsa_wkdy_am202310.rds\")\n\n\n\n\nehsa_wkdy_pm &lt;- emerging_hotspot_analysis(\n  x = wkdy_pm_st, \n  .var = \"TRIPS\", \n  k = 1, \n  nsim = 99,\n  nb_col = \"nb\",\n  wt_col = \"wt\"\n)\nwrite_rds(ehsa_wkdy_pm, \"data/rds/ehsa_wkdy_pm202310.rds\")\n\n\n\n\nehsa_wknd_am &lt;- emerging_hotspot_analysis(\n  x = wknd_am_st, \n  .var = \"TRIPS\", \n  k = 1, \n  nsim = 99,\n  nb_col = \"nb\",\n  wt_col = \"wt\"\n)\nwrite_rds(ehsa_wknd_am, \"data/rds/ehsa_wknd_am202310.rds\")\n\n\n\n\nehsa_wknd_pm &lt;- emerging_hotspot_analysis(\n  x = wknd_pm_st, \n  .var = \"TRIPS\", \n  k = 1, \n  nsim = 99,\n  nb_col = \"nb\",\n  wt_col = \"wt\"\n)\nwrite_rds(ehsa_wknd_pm, \"data/rds/ehsa_wknd_pm202310.rds\")\n\n\n\n\n\n\n\n\n\n\nLoad point\n\n\n\nWe will reload the previously generated ehsa_wk*_* data.\n\nehsa_wkdy_am &lt;- read_rds(\"data/rds/ehsa_wkdy_am202310.rds\")\nehsa_wkdy_pm &lt;- read_rds(\"data/rds/ehsa_wkdy_pm202310.rds\")\nehsa_wknd_am &lt;- read_rds(\"data/rds/ehsa_wknd_am202310.rds\")\nehsa_wknd_pm &lt;- read_rds(\"data/rds/ehsa_wknd_pm202310.rds\")"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1D.html#visualizing-ehsa",
    "href": "Take-home_Ex1/Take-home_Ex1D.html#visualizing-ehsa",
    "title": "Take-home Exercise 1D: Spatio-temporal Analysis with EHSA",
    "section": "Visualizing EHSA",
    "text": "Visualizing EHSA\nWe will now visualize the results by plotting the categories for the areas where significant trends are detected.\nAs our \\(\\alpha\\) is 0.05, we will be filtering the values for which the p-value &lt; 0.05 as it is for these areas where the trends observed where significant and did not happen by chance.\nNext, we will join the filtered values with honeycomb and transforming it to a sf object for it to be processed as a geospatial data.\n\n\n\n\n\n\nAddressing the palette\n\n\n\n\n\nTo show the EHSA classes as reds for hot spots and blues for cold spots, we will assign numbers to each category. This way the palette will identify the number values instead of sorting the categories alphabetically.\nReference for the categories: https://www.azavea.com/blog/2017/08/15/emerging-hot-spot-spatial-statistics/\n\nehsa_colors &lt;- data.frame(\n  CLASS = c(\n      \"persistent coldspot\", \"consecutive coldspot\", \"intensifying coldspot\",\n      \"sporadic coldspot\", \"new coldspot\", \"oscillating coldspot\",\n      \"historical coldspot\", \"diminishing coldspot\",\n      \"no pattern detected\",\n      \"diminishing hotspot\", \"historical hotspot\",\n      \"oscillating hotspot\", \"new hotspot\", \"sporadic hotspot\",\n      \"intensifying hotspot\", \"consecutive hotspot\", \"persistent hotspot\"\n    ),\n  LEVEL = -8:8\n)\n\nWe will use this table to join with the EHSA plot to visualize the hot spots and cold spots appropriately.\n\n\n\n\nWeekday morningWeekday eveningWeekend morningWeekend evening\n\n\n\n\nShow the code\nehsa_sig_wkdy_am &lt;- ehsa_wkdy_am %&gt;%\n    filter(p_value &lt; 0.05) %&gt;%\n    left_join(honeycomb,\n              by = join_by(location == HEX_ID)) %&gt;%\n    left_join(ehsa_colors,\n              by = join_by(classification == CLASS)) %&gt;%\n    st_sf()\n\ntm_shape(mpsz) +\n  tm_polygons(col = \"white\") +\ntm_shape(ehsa_sig_wkdy_am) +\n  tm_polygons(\n    \"LEVEL\",\n    palette = \"-Spectral\",\n    breaks = -8:9,\n    labels = ehsa_colors$CLASS,\n    title = \"Legend\"\n  ) +\n  tm_layout(main.title = \"Hot Spots and Cold Spots (Weekdays 5AM - 11PM)\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.4, \n            legend.width = 0.4,\n            legend.bg.color = \"white\",\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(bg.color = \"white\") +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nOn weekday mornings, hot spots can be observed in residential areas like Clementi (with the biggest cluster of hot spots), Queenstown, Bukit Merah, and Toa Payoh. This aligns with our other observations as people are expected to come out of their homes to go to work or school in the morning.\nOn the other hand, cold spots are observed in other residential areas like Punggol, Jurong, and Choa Chu Kang areas. A possible explanation is that alternative modes of transportation like the MRT or cars is preferred by people living in these areas.\n\n\n\n\nShow the code\nehsa_sig_wkdy_pm &lt;- ehsa_wkdy_pm %&gt;%\n    filter(p_value &lt; 0.05) %&gt;%\n    left_join(honeycomb,\n              by = join_by(location == HEX_ID)) %&gt;%\n    left_join(ehsa_colors,\n              by = join_by(classification == CLASS)) %&gt;%\n    st_sf()\n  \ntm_shape(mpsz) +\n  tm_polygons(col = \"white\") +\ntm_shape(ehsa_sig_wkdy_pm) +\n  tm_polygons(\n    \"LEVEL\",\n    palette = \"-Spectral\",\n    breaks = -8:9,\n    labels = ehsa_colors$CLASS,\n    title = \"Legend\"\n  ) +\n  tm_layout(main.title = \"Hot Spots and Cold Spots (Weekdays 3 - 9PM)\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.4, \n            legend.width = 0.4,\n            legend.bg.color = \"white\",\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(bg.color = \"white\") +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nInterestingly, the trend is reversed on weekday evenings. There are hot spots in the residential areas that were cold spots in the morning — Punggol, Jurong, and Choa Chu Kang areas. Other notable additions to these hot spots are Woodlands and Yishun, other residential areas.\nThis may mean that people in the area take the MRT in the morning going to office or work, but take the bus home in the evening.\n\n\n\n\nShow the code\nehsa_sig_wknd_am &lt;- ehsa_wknd_am %&gt;%\n    filter(p_value &lt; 0.05) %&gt;%\n    left_join(honeycomb,\n              by = join_by(location == HEX_ID)) %&gt;%\n    left_join(ehsa_colors,\n              by = join_by(classification == CLASS)) %&gt;%\n    st_sf()\n  \ntm_shape(mpsz) +\n  tm_polygons(col = \"white\") +\ntm_shape(ehsa_sig_wknd_am) +\n  tm_polygons(\n    \"LEVEL\",\n    palette = \"-Spectral\",\n    breaks = -8:9,\n    labels = ehsa_colors$CLASS,\n    title = \"Legend\"\n  ) +\n  tm_layout(main.title = \"Hot Spots and Cold Spots (Weekends/Holidays 9AM - 3PM)\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.4, \n            legend.width = 0.4,\n            legend.bg.color = \"white\",\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(bg.color = \"white\") +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nOn weekend mornings, there are notable hot spots on many residential hubs like Jurong, Tampines, Woodlands, Bedok, etc.\nThis can indicate consistent activity of people going out of their houses on weekend mornings. Having a lot of persistent hot spots indicate that this high activity is consistent throughout the morning.\nAnother thing of note is the big hot spot in the Central area. This coincides with the Orchard area, which is a very popular are for recreation.\n\n\n\n\nShow the code\nehsa_sig_wknd_pm &lt;- ehsa_wknd_pm %&gt;%\n    filter(p_value &lt; 0.05) %&gt;%\n    left_join(honeycomb,\n              by = join_by(location == HEX_ID)) %&gt;%\n    left_join(ehsa_colors,\n              by = join_by(classification == CLASS)) %&gt;%\n    st_sf()\n  \ntm_shape(mpsz) +\n  tm_polygons(col = \"white\") +\ntm_shape(ehsa_sig_wknd_pm) +\n  tm_polygons(\n    \"LEVEL\",\n    palette = \"-Spectral\",\n    breaks = -8:9,\n    labels = ehsa_colors$CLASS,\n    title = \"Legend\"\n  ) +\n  tm_layout(main.title = \"Hot Spots and Cold Spots (Weekends 3PM - 9PM)\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.4, \n            legend.width = 0.4,\n            legend.bg.color = \"white\",\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(bg.color = \"white\") +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nThere are less hot spots compared to weekend mornings. Some residential areas are busier than others, notably Woodlands and Jurong areas.\nAnother key observation is the Causeway area in Woodlands are also hot spots, which indicate an influx of people coming into Singapore after spending the weekend in Malaysia."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1C.html",
    "href": "Take-home_Ex1/Take-home_Ex1C.html",
    "title": "Take Home Exercise 1C: Spatial Analysis with LISA",
    "section": "",
    "text": "The aim of this study is to uncover spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.\nThe main modes of analysis to be used here are Local Indicators of Spatial Association (GLISA) and Emerging Hot Spot Analysis (EHSA).\nIn doing these study, we will be looking at bus trips started during the hours below.\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday evening peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nMore details about the study can be found here.\nIn this part of the study, we will do spatial analysis with LISA using bus commuter traffic data generated from Data Wrangling. We will also attempt the answer the Open Questions from Geovisualization and Analysis:\n\nWhich areas are busier on weekends? on weekdays?\nWhat are the commuting patterns of people during weekdays? weekends?"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1C.html#setting-up-the-r-environment",
    "href": "Take-home_Ex1/Take-home_Ex1C.html#setting-up-the-r-environment",
    "title": "Take Home Exercise 1C: Spatial Analysis with LISA",
    "section": "Setting Up the R Environment",
    "text": "Setting Up the R Environment\nWe will load the following R packages needed for this study.\n\ntmap: for thematic mapping\nsf: for geospatial data handling\ntidyverse: for non-spatial data handling\nsfdep: for spatial analysis\nknitr:for prettifying presentation\n\n\npacman::p_load(sf, sfdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1C.html#environment-settings",
    "href": "Take-home_Ex1/Take-home_Ex1C.html#environment-settings",
    "title": "Take Home Exercise 1C: Spatial Analysis with LISA",
    "section": "Environment settings",
    "text": "Environment settings\nWe will also set the default settings on for this document\n\ntmap_style to natural: for displaying the maps with preferred style\nset seed for reproducibility of results\n\n\ntmap_mode(\"plot\")\ntmap_style(\"natural\")\nset.seed(1234)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1C.html#loading-the-data",
    "href": "Take-home_Ex1/Take-home_Ex1C.html#loading-the-data",
    "title": "Take Home Exercise 1C: Spatial Analysis with LISA",
    "section": "Loading the data",
    "text": "Loading the data\n\n\n\n\n\n\nImportant\n\n\n\nBefore running this part, please run all the code chunks in Data Wrangling as it generates the data needed for this document.\n\n\nUse read_rds() to load the rds data needed for Local Indicator of Spatial Association (LISA) Analysis\n\nmpsz &lt;- read_rds(\"data/rds/mpsz.rds\")\nhoneycomb &lt;- read_rds(\"data/rds/honeycomb202310.rds\")\npeak_trips_sf &lt;- read_rds(\"data/rds/peak_trips_sf202310.rds\")\n\n\nhoneycomb - contains the geometry of honeycomb grid containing Singapore bus stops\npeak_trips_sf - contains number of bus trips originating from each hexagon for each peak period"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2A.html",
    "href": "In-class_Ex2/In-class_Ex2A.html",
    "title": "In-class Exercise 2: Spatial Weights with sfdep",
    "section": "",
    "text": "This introduces sfdep functions for getting spatial weights."
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2A.html#importing-the-data",
    "href": "In-class_Ex2/In-class_Ex2A.html#importing-the-data",
    "title": "In-class Exercise 2: Spatial Weights with sfdep",
    "section": "Importing the data",
    "text": "Importing the data\nFirst, we will import the geospatial data in shp format.\n\nhunan = st_read(dsn = \"data/geospatial\",\n                layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/In-class_Ex2/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nSecond, we import the aspatial data Hunan_2012, which contains the GDP Per Capita (GDPPC) of Chinese counties in 2012.\n\nhunan2012 = read_csv(\"data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2A.html#combining-them-all-together",
    "href": "In-class_Ex2/In-class_Ex2A.html#combining-them-all-together",
    "title": "In-class Exercise 2: Spatial Weights with sfdep",
    "section": "Combining them all together",
    "text": "Combining them all together\nAs seen from the import above, each of the dataframes have 88 rows each. Each row corresponds to a record per county.\nHowever, we are already interested in the following columns:\n\nCounty\nGDPPC\n\n\nhunan &lt;- left_join(hunan, hunan2012)%&gt;%\n  select(7, 15)\n\n\n\n\n\n\n\nNote\n\n\n\nWe do not need to specify the columns to join as both dataframes have the County column so left_join() is able to detect that this is the column to join by."
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2A.html#plotting-a-choropleth-map",
    "href": "In-class_Ex2/In-class_Ex2A.html#plotting-a-choropleth-map",
    "title": "In-class Exercise 2: Spatial Weights with sfdep",
    "section": "Plotting a choropleth map",
    "text": "Plotting a choropleth map\nNext is to plot the map of GDP per capita values.\n\ntmap_mode(\"plot\")\ntm_shape(hunan) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDP per capita by county in China (2012)\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2A.html#identifying-neighbors-via-queens-method",
    "href": "In-class_Ex2/In-class_Ex2A.html#identifying-neighbors-via-queens-method",
    "title": "In-class Exercise 2: Spatial Weights with sfdep",
    "section": "Identifying neighbors via Queen’s method",
    "text": "Identifying neighbors via Queen’s method\n\n\n\n\n\n\nImportant\n\n\n\nst_continguity() used queen = TRUE as default. If not specified, it will use the Queen’s method.\n\n\n\nnb_queen &lt;- hunan %&gt;% \n  mutate(nb = st_contiguity(geometry),\n         .before = 1)\nsummary(nb_queen)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n    nb          County              GDPPC                geometry \n NULL:NULL   Length:88          Min.   : 8497   POLYGON      :88  \n             Class :character   1st Qu.:14566   epsg:4326    : 0  \n             Mode  :character   Median :20433   +proj=long...: 0  \n                                Mean   :24405                     \n                                3rd Qu.:27224                     \n                                Max.   :88656                     \n\n\nTo prettify the output of head(), we can use kable.\n\nkable(head(nb_queen,\n           n=10))\n\n\n\n\n\n\n\n\n\n\nnb\nCounty\nGDPPC\ngeometry\n\n\n\n\n2, 3, 4, 57, 85\nAnxiang\n23667\nPOLYGON ((112.0625 29.75523…\n\n\n1, 57, 58, 78, 85\nHanshou\n20981\nPOLYGON ((112.2288 29.11684…\n\n\n1, 4, 5, 85\nJinshi\n34592\nPOLYGON ((111.8927 29.6013,…\n\n\n1, 3, 5, 6\nLi\n24473\nPOLYGON ((111.3731 29.94649…\n\n\n3, 4, 6, 85\nLinli\n25554\nPOLYGON ((111.6324 29.76288…\n\n\n4, 5, 69, 75, 85\nShimen\n27137\nPOLYGON ((110.8825 30.11675…\n\n\n67, 71, 74, 84\nLiuyang\n63118\nPOLYGON ((113.9905 28.5682,…\n\n\n9, 46, 47, 56, 78, 80, 86\nNingxiang\n62202\nPOLYGON ((112.7181 28.38299…\n\n\n8, 66, 68, 78, 84, 86\nWangcheng\n70666\nPOLYGON ((112.7914 28.52688…\n\n\n16, 17, 19, 20, 22, 70, 72, 73\nAnren\n12761\nPOLYGON ((113.1757 26.82734…"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2A.html#identifying-neighbors-via-rooks-method",
    "href": "In-class_Ex2/In-class_Ex2A.html#identifying-neighbors-via-rooks-method",
    "title": "In-class Exercise 2: Spatial Weights with sfdep",
    "section": "Identifying neighbors via Rook’s method",
    "text": "Identifying neighbors via Rook’s method\nWe do the same for the Rook’s method. This time, we need to supply queen = FALSE to st_contiguity().\n\nnb_rook &lt;- hunan %&gt;% \n  mutate(nb = st_contiguity(geometry, queen = FALSE),\n         .before = 1)\nsummary(nb_rook)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\n    nb          County              GDPPC                geometry \n NULL:NULL   Length:88          Min.   : 8497   POLYGON      :88  \n             Class :character   1st Qu.:14566   epsg:4326    : 0  \n             Mode  :character   Median :20433   +proj=long...: 0  \n                                Mean   :24405                     \n                                3rd Qu.:27224                     \n                                Max.   :88656                     \n\n\n\nkable(head(nb_rook,\n           n=10))\n\n\n\n\n\n\n\n\n\n\nnb\nCounty\nGDPPC\ngeometry\n\n\n\n\n3, 4, 57, 85\nAnxiang\n23667\nPOLYGON ((112.0625 29.75523…\n\n\n57, 58, 78, 85\nHanshou\n20981\nPOLYGON ((112.2288 29.11684…\n\n\n1, 4, 5, 85\nJinshi\n34592\nPOLYGON ((111.8927 29.6013,…\n\n\n1, 3, 5, 6\nLi\n24473\nPOLYGON ((111.3731 29.94649…\n\n\n3, 4, 6, 85\nLinli\n25554\nPOLYGON ((111.6324 29.76288…\n\n\n4, 5, 69, 75, 85\nShimen\n27137\nPOLYGON ((110.8825 30.11675…\n\n\n67, 71, 74, 84\nLiuyang\n63118\nPOLYGON ((113.9905 28.5682,…\n\n\n9, 46, 47, 56, 78, 80, 86\nNingxiang\n62202\nPOLYGON ((112.7181 28.38299…\n\n\n8, 66, 68, 78, 84, 86\nWangcheng\n70666\nPOLYGON ((112.7914 28.52688…\n\n\n16, 19, 20, 22, 70, 72, 73\nAnren\n12761\nPOLYGON ((113.1757 26.82734…"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2A.html#identifying-higher-order-contiguity-neighbors",
    "href": "In-class_Ex2/In-class_Ex2A.html#identifying-higher-order-contiguity-neighbors",
    "title": "In-class Exercise 2: Spatial Weights with sfdep",
    "section": "Identifying higher-order contiguity neighbors",
    "text": "Identifying higher-order contiguity neighbors\nThis simply means neighbors of neighbors.\n\nnb2_queen &lt;-  hunan %&gt;% \n  mutate(nb = st_contiguity(geometry),\n         nb2 = st_nb_lag_cumul(nb, 2),\n         .before = 1)\nsummary(nb2_queen)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 1324 \nPercentage nonzero weights: 17.09711 \nAverage number of links: 15.04545 \nLink number distribution:\n\n 5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26 28 33 \n 2  1  6  4  5  4  8  5 10  4  4  8  4  8  5  2  2  1  2  1  1  1 \n2 least connected regions:\n30 88 with 5 links\n1 most connected region:\n56 with 33 links\n\n\n    nb         nb2          County              GDPPC                geometry \n NULL:NULL   NULL:NULL   Length:88          Min.   : 8497   POLYGON      :88  \n                         Class :character   1st Qu.:14566   epsg:4326    : 0  \n                         Mode  :character   Median :20433   +proj=long...: 0  \n                                            Mean   :24405                     \n                                            3rd Qu.:27224                     \n                                            Max.   :88656                     \n\n\n\nkable(head(nb2_queen))\n\n\n\n\n\n\n\n\n\n\n\nnb\nnb2\nCounty\nGDPPC\ngeometry\n\n\n\n\n2, 3, 4, 57, 85\n2, 3, 4, 5, 6, 32, 56, 57, 58, 64, 69, 75, 76, 78, 85\nAnxiang\n23667\nPOLYGON ((112.0625 29.75523…\n\n\n1, 57, 58, 78, 85\n1, 3, 4, 5, 6, 8, 9, 32, 56, 57, 58, 64, 68, 69, 75, 76, 78, 85\nHanshou\n20981\nPOLYGON ((112.2288 29.11684…\n\n\n1, 4, 5, 85\n1, 2, 4, 5, 6, 32, 56, 57, 69, 75, 78, 85\nJinshi\n34592\nPOLYGON ((111.8927 29.6013,…\n\n\n1, 3, 5, 6\n1, 2, 3, 5, 6, 57, 69, 75, 85\nLi\n24473\nPOLYGON ((111.3731 29.94649…\n\n\n3, 4, 6, 85\n1, 2, 3, 4, 6, 32, 56, 57, 69, 75, 78, 85\nLinli\n25554\nPOLYGON ((111.6324 29.76288…\n\n\n4, 5, 69, 75, 85\n1, 2, 3, 4, 5, 32, 53, 55, 56, 57, 69, 75, 78, 85\nShimen\n27137\nPOLYGON ((110.8825 30.11675…"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2A.html#deriving-fixed-distance-weights",
    "href": "In-class_Ex2/In-class_Ex2A.html#deriving-fixed-distance-weights",
    "title": "In-class Exercise 2: Spatial Weights with sfdep",
    "section": "Deriving fixed-distance weights",
    "text": "Deriving fixed-distance weights\nFirst is to determine the upper limit for the distance for the nearest neigbor.\n\ngeo &lt;- sf::st_geometry(hunan)\nnb &lt;- st_knn(geo, longlat = TRUE)\ndists &lt;- unlist(st_nb_dists(geo, nb))\n\n\nsummary(dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.56   29.11   36.89   37.34   43.21   65.80 \n\n\nWe use the max value of the result, 65.80. For simplicity, let’s set it to 66.\nNow we can calculate the fixed-distance weights.\n\nwm_fd &lt;- hunan %&gt;%\n  mutate(nb = st_dist_band(geometry,\n                           upper = 66),\n               wt = st_weights(nb),\n               .before = 1)\nwm_fd\n\nSimple feature collection with 88 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                       nb\n1      2, 3, 4, 5, 57, 64\n2       1, 57, 58, 78, 85\n3             1, 4, 5, 57\n4              1, 3, 5, 6\n5          1, 3, 4, 6, 69\n6                4, 5, 69\n7              67, 71, 84\n8       9, 46, 47, 78, 80\n9   8, 46, 66, 68, 84, 86\n10 16, 20, 22, 70, 72, 73\n                                                                 wt    County\n1  0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667   Anxiang\n2                                           0.2, 0.2, 0.2, 0.2, 0.2   Hanshou\n3                                            0.25, 0.25, 0.25, 0.25    Jinshi\n4                                            0.25, 0.25, 0.25, 0.25        Li\n5                                           0.2, 0.2, 0.2, 0.2, 0.2     Linli\n6                                   0.3333333, 0.3333333, 0.3333333    Shimen\n7                                   0.3333333, 0.3333333, 0.3333333   Liuyang\n8                                           0.2, 0.2, 0.2, 0.2, 0.2 Ningxiang\n9  0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667 Wangcheng\n10 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667     Anren\n   GDPPC                       geometry\n1  23667 POLYGON ((112.0625 29.75523...\n2  20981 POLYGON ((112.2288 29.11684...\n3  34592 POLYGON ((111.8927 29.6013,...\n4  24473 POLYGON ((111.3731 29.94649...\n5  25554 POLYGON ((111.6324 29.76288...\n6  27137 POLYGON ((110.8825 30.11675...\n7  63118 POLYGON ((113.9905 28.5682,...\n8  62202 POLYGON ((112.7181 28.38299...\n9  70666 POLYGON ((112.7914 28.52688...\n10 12761 POLYGON ((113.1757 26.82734..."
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2A.html#calculate-inverse-distance-weights",
    "href": "In-class_Ex2/In-class_Ex2A.html#calculate-inverse-distance-weights",
    "title": "In-class Exercise 2: Spatial Weights with sfdep",
    "section": "Calculate inverse distance weights",
    "text": "Calculate inverse distance weights\n\nwm_idw &lt;- hunan %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\nwm_idw\n\nSimple feature collection with 88 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                                              wts\n1                                      0.01526149, 0.03515537, 0.02176677, 0.02836978, 0.01029857\n2                                      0.01526149, 0.01601100, 0.01911052, 0.02327058, 0.01591694\n3                                                  0.03515537, 0.04581089, 0.04116397, 0.01208437\n4                                                  0.02176677, 0.04581089, 0.04637578, 0.01585302\n5                                                  0.04116397, 0.04637578, 0.01896212, 0.01351099\n6                                      0.01585302, 0.01896212, 0.02710909, 0.01140718, 0.01080890\n7                                                  0.01621067, 0.01536702, 0.01133628, 0.01836488\n8              0.01930410, 0.02675555, 0.02151751, 0.01076895, 0.02608065, 0.01519804, 0.01337412\n9                          0.01930410, 0.01651371, 0.01798519, 0.01473155, 0.03015561, 0.01612293\n10 0.02737233, 0.01390810, 0.01458881, 0.02156771, 0.02419268, 0.02350470, 0.01784174, 0.01621545\n      County GDPPC                       geometry\n1    Anxiang 23667 POLYGON ((112.0625 29.75523...\n2    Hanshou 20981 POLYGON ((112.2288 29.11684...\n3     Jinshi 34592 POLYGON ((111.8927 29.6013,...\n4         Li 24473 POLYGON ((111.3731 29.94649...\n5      Linli 25554 POLYGON ((111.6324 29.76288...\n6     Shimen 27137 POLYGON ((110.8825 30.11675...\n7    Liuyang 63118 POLYGON ((113.9905 28.5682,...\n8  Ningxiang 62202 POLYGON ((112.7181 28.38299...\n9  Wangcheng 70666 POLYGON ((112.7914 28.52688...\n10     Anren 12761 POLYGON ((113.1757 26.82734..."
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html",
    "href": "In-class_Ex2/In-class_Ex2B.html",
    "title": "In-class Exercise 2: GLSA",
    "section": "",
    "text": "This introduces sfdep functions for analysis related to Global and Local Measures of Association."
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html#importing-the-data",
    "href": "In-class_Ex2/In-class_Ex2B.html#importing-the-data",
    "title": "In-class Exercise 2: GLSA",
    "section": "Importing the data",
    "text": "Importing the data\nFirst, we will import the geospatial data in shp format.\n\nhunan = st_read(dsn = \"data/geospatial\",\n                layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/In-class_Ex2/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nSecond, we import the aspatial data Hunan_2012, which contains the GDP Per Capita (GDPPC) of Chinese counties in 2012.\n\nhunan2012 = read_csv(\"data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html#combining-them-all-together",
    "href": "In-class_Ex2/In-class_Ex2B.html#combining-them-all-together",
    "title": "In-class Exercise 2: GLSA",
    "section": "Combining them all together",
    "text": "Combining them all together\nAs seen from the import above, each of the dataframes have 88 rows each. Each row corresponds to a record per county.\nHowever, we are already interested in the following columns:\n\nCounty\nGDPPC\n\n\nhunan &lt;- left_join(hunan, hunan2012)%&gt;%\n  select(7, 15)\n\n\n\n\n\n\n\nNote\n\n\n\nWe do not need to specify the columns to join as both dataframes have the County column so left_join() is able to detect that this is the column to join by."
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html#plotting-a-choropleth-map",
    "href": "In-class_Ex2/In-class_Ex2B.html#plotting-a-choropleth-map",
    "title": "In-class Exercise 2: GLSA",
    "section": "Plotting a choropleth map",
    "text": "Plotting a choropleth map\nNext is to plot the map of GDP per capita values.\n\ntmap_mode(\"plot\")\ntm_shape(hunan) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDP per capita by county in China (2012)\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html#deriving-contiguity-weights-queens-method",
    "href": "In-class_Ex2/In-class_Ex2B.html#deriving-contiguity-weights-queens-method",
    "title": "In-class Exercise 2: GLSA",
    "section": "Deriving Contiguity Weights: Queen’s Method",
    "text": "Deriving Contiguity Weights: Queen’s Method\n\nwm_q &lt;- hunan %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \nwm_q\n\nSimple feature collection with 88 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n      County GDPPC                       geometry\n1    Anxiang 23667 POLYGON ((112.0625 29.75523...\n2    Hanshou 20981 POLYGON ((112.2288 29.11684...\n3     Jinshi 34592 POLYGON ((111.8927 29.6013,...\n4         Li 24473 POLYGON ((111.3731 29.94649...\n5      Linli 25554 POLYGON ((111.6324 29.76288...\n6     Shimen 27137 POLYGON ((110.8825 30.11675...\n7    Liuyang 63118 POLYGON ((113.9905 28.5682,...\n8  Ningxiang 62202 POLYGON ((112.7181 28.38299...\n9  Wangcheng 70666 POLYGON ((112.7914 28.52688...\n10     Anren 12761 POLYGON ((113.1757 26.82734..."
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html#computing-global-morans-i",
    "href": "In-class_Ex2/In-class_Ex2B.html#computing-global-morans-i",
    "title": "In-class Exercise 2: GLSA",
    "section": "Computing Global Moran’s I",
    "text": "Computing Global Moran’s I\n\nmoranI &lt;- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\nglimpse(moranI)\n\nList of 2\n $ I: num 0.301\n $ K: num 7.64"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html#performing-global-morans-i-test",
    "href": "In-class_Ex2/In-class_Ex2B.html#performing-global-morans-i-test",
    "title": "In-class Exercise 2: GLSA",
    "section": "Performing Global Moran’s I test",
    "text": "Performing Global Moran’s I test\n\n\n\n\n\n\nTip\n\n\n\nThis is preferred over just calculating the statistic.\n\n\n\nglobal_moran_test(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html#performing-global-morans-i-test-1",
    "href": "In-class_Ex2/In-class_Ex2B.html#performing-global-morans-i-test-1",
    "title": "In-class Exercise 2: GLSA",
    "section": "Performing Global Moran’s I test",
    "text": "Performing Global Moran’s I test\n\n\n\n\n\n\nTip\n\n\n\nThis is the ideal method in practice.\n\n\n\nset.seed(1234)\nglobal_moran_perm(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html#visualizing-morans-i",
    "href": "In-class_Ex2/In-class_Ex2B.html#visualizing-morans-i",
    "title": "In-class Exercise 2: GLSA",
    "section": "Visualizing Moran’s I",
    "text": "Visualizing Moran’s I\nIn visualizing the Moran’s I values, plot using the ii column.\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html#visualizing-p-value-of-morans-i",
    "href": "In-class_Ex2/In-class_Ex2B.html#visualizing-p-value-of-morans-i",
    "title": "In-class Exercise 2: GLSA",
    "section": "Visualizing p-value of Moran’s I",
    "text": "Visualizing p-value of Moran’s I\nTo visualize the p-value, plot using p_ii_sim.\n\n\n\n\n\n\nWarning\n\n\n\nThese are from simulation results\n\n\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\") + \n  tm_borders(alpha = 0.5) +\n   tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html#visualizing-local-morans-i-and-p-value",
    "href": "In-class_Ex2/In-class_Ex2B.html#visualizing-local-morans-i-and-p-value",
    "title": "In-class Exercise 2: GLSA",
    "section": "Visualizing local Moran’s I and p-value",
    "text": "Visualizing local Moran’s I and p-value\n\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html#visualizing-lisa-map",
    "href": "In-class_Ex2/In-class_Ex2B.html#visualizing-lisa-map",
    "title": "In-class Exercise 2: GLSA",
    "section": "Visualizing LISA map",
    "text": "Visualizing LISA map\n\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html#computing-local-gi-statistics",
    "href": "In-class_Ex2/In-class_Ex2B.html#computing-local-gi-statistics",
    "title": "In-class Exercise 2: GLSA",
    "section": "Computing local Gi* statistics",
    "text": "Computing local Gi* statistics\nWe need to compute the inverse distance weights first.\n\nwm_idw &lt;- hunan %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\nwm_idw\n\nSimple feature collection with 88 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                                              wts\n1                                      0.01526149, 0.03515537, 0.02176677, 0.02836978, 0.01029857\n2                                      0.01526149, 0.01601100, 0.01911052, 0.02327058, 0.01591694\n3                                                  0.03515537, 0.04581089, 0.04116397, 0.01208437\n4                                                  0.02176677, 0.04581089, 0.04637578, 0.01585302\n5                                                  0.04116397, 0.04637578, 0.01896212, 0.01351099\n6                                      0.01585302, 0.01896212, 0.02710909, 0.01140718, 0.01080890\n7                                                  0.01621067, 0.01536702, 0.01133628, 0.01836488\n8              0.01930410, 0.02675555, 0.02151751, 0.01076895, 0.02608065, 0.01519804, 0.01337412\n9                          0.01930410, 0.01651371, 0.01798519, 0.01473155, 0.03015561, 0.01612293\n10 0.02737233, 0.01390810, 0.01458881, 0.02156771, 0.02419268, 0.02350470, 0.01784174, 0.01621545\n      County GDPPC                       geometry\n1    Anxiang 23667 POLYGON ((112.0625 29.75523...\n2    Hanshou 20981 POLYGON ((112.2288 29.11684...\n3     Jinshi 34592 POLYGON ((111.8927 29.6013,...\n4         Li 24473 POLYGON ((111.3731 29.94649...\n5      Linli 25554 POLYGON ((111.6324 29.76288...\n6     Shimen 27137 POLYGON ((110.8825 30.11675...\n7    Liuyang 63118 POLYGON ((113.9905 28.5682,...\n8  Ningxiang 62202 POLYGON ((112.7181 28.38299...\n9  Wangcheng 70666 POLYGON ((112.7914 28.52688...\n10     Anren 12761 POLYGON ((113.1757 26.82734...\n\n\n\nHCSA &lt;- wm_idw %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\nHCSA\n\nSimple feature collection with 88 features and 12 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 13\n   gi_star   e_gi    var_gi p_value   p_sim p_folded_sim skewness kurtosis nb   \n     &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;nb&gt; \n 1  0.0416 0.0114   6.41e-6  0.0493 9.61e-1         0.7      0.35    0.875 &lt;int&gt;\n 2 -0.333  0.0106   3.84e-6 -0.0941 9.25e-1         1        0.5     0.661 &lt;int&gt;\n 3  0.281  0.0126   7.51e-6 -0.151  8.80e-1         0.9      0.45    0.640 &lt;int&gt;\n 4  0.411  0.0118   9.22e-6  0.264  7.92e-1         0.6      0.3     0.853 &lt;int&gt;\n 5  0.387  0.0115   9.56e-6  0.339  7.34e-1         0.62     0.31    1.07  &lt;int&gt;\n 6 -0.368  0.0118   5.91e-6 -0.583  5.60e-1         0.72     0.36    0.594 &lt;int&gt;\n 7  3.56   0.0151   7.31e-6  2.61   9.01e-3         0.06     0.03    1.09  &lt;int&gt;\n 8  2.52   0.0136   6.14e-6  1.49   1.35e-1         0.2      0.1     1.12  &lt;int&gt;\n 9  4.56   0.0144   5.84e-6  3.53   4.17e-4         0.04     0.02    1.23  &lt;int&gt;\n10  1.16   0.0104   3.70e-6  1.82   6.86e-2         0.12     0.06    0.416 &lt;int&gt;\n# ℹ 78 more rows\n# ℹ 4 more variables: wts &lt;list&gt;, County &lt;chr&gt;, GDPPC &lt;dbl&gt;,\n#   geometry &lt;POLYGON [°]&gt;"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html#visualizing-gi",
    "href": "In-class_Ex2/In-class_Ex2B.html#visualizing-gi",
    "title": "In-class Exercise 2: GLSA",
    "section": "Visualizing Gi*",
    "text": "Visualizing Gi*\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html#visualizing-p-value-of-hcsa",
    "href": "In-class_Ex2/In-class_Ex2B.html#visualizing-p-value-of-hcsa",
    "title": "In-class Exercise 2: GLSA",
    "section": "Visualizing p-value of HCSA",
    "text": "Visualizing p-value of HCSA\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"p_sim\") + \n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html#visualizing-local-hcsa",
    "href": "In-class_Ex2/In-class_Ex2B.html#visualizing-local-hcsa",
    "title": "In-class Exercise 2: GLSA",
    "section": "Visualizing local HCSA",
    "text": "Visualizing local HCSA\n\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2B.html#visualizing-hot-spot-and-cold-spot-areas",
    "href": "In-class_Ex2/In-class_Ex2B.html#visualizing-hot-spot-and-cold-spot-areas",
    "title": "In-class Exercise 2: GLSA",
    "section": "Visualizing hot spot and cold spot areas",
    "text": "Visualizing hot spot and cold spot areas\n\nHCSA_sig &lt;- HCSA  %&gt;%\n  filter(p_sim &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#converting-from-sf-data.table-to-spatialpolygonsdataframe",
    "href": "In-class_Ex3/In-class_Ex3.html#converting-from-sf-data.table-to-spatialpolygonsdataframe",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Converting from sf data.table to SpatialPolygonsDataFrame",
    "text": "Converting from sf data.table to SpatialPolygonsDataFrame\n\nmpsz_sp &lt;- as(mpsz, \"Spatial\")\nmpsz_sp\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 332 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 6\nnames       : SUBZONE_N, SUBZONE_C, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C \nmin values  : ADMIRALTY,    AMSZ01, ANG MO KIO,         AM, CENTRAL REGION,       CR \nmax values  :    YUNNAN,    YSSZ09,     YISHUN,         YS,    WEST REGION,       WR \n\n\nSample operation in mpsz_sp on how to read the data table. (Check after class how this is done because it didn’t work. I might have copied wrongly)\n\nmpsz_sp_selected &lt;- mpsz_sp %&gt;%\n  selected(mpsz@data$SUBZONE)"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#computing-the-distance-matrix",
    "href": "In-class_Ex3/In-class_Ex3.html#computing-the-distance-matrix",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Computing the distance matrix",
    "text": "Computing the distance matrix\n\ndist &lt;- spDists(mpsz_sp, \n                longlat = FALSE)\nhead(dist, n=c(10, 10))\n\n           [,1]       [,2]      [,3]      [,4]       [,5]      [,6]      [,7]\n [1,]     0.000  3926.0025  3939.108 20252.964  2989.9839  1431.330 19211.836\n [2,]  3926.003     0.0000   305.737 16513.865   951.8314  5254.066 16242.523\n [3,]  3939.108   305.7370     0.000 16412.062  1045.9088  5299.849 16026.146\n [4,] 20252.964 16513.8648 16412.062     0.000 17450.3044 21665.795  7229.017\n [5,]  2989.984   951.8314  1045.909 17450.304     0.0000  4303.232 17020.916\n [6,]  1431.330  5254.0664  5299.849 21665.795  4303.2323     0.000 20617.082\n [7,] 19211.836 16242.5230 16026.146  7229.017 17020.9161 20617.082     0.000\n [8,] 14960.942 12749.4101 12477.871 11284.279 13336.0421 16281.453  5606.082\n [9,]  7515.256  7934.8082  7649.776 18427.503  7801.6163  8403.896 14810.930\n[10,]  6391.342  4975.0021  4669.295 15469.566  5226.8731  7707.091 13111.391\n           [,8]      [,9]     [,10]\n [1,] 14960.942  7515.256  6391.342\n [2,] 12749.410  7934.808  4975.002\n [3,] 12477.871  7649.776  4669.295\n [4,] 11284.279 18427.503 15469.566\n [5,] 13336.042  7801.616  5226.873\n [6,] 16281.453  8403.896  7707.091\n [7,]  5606.082 14810.930 13111.391\n [8,]     0.000  9472.024  8575.490\n [9,]  9472.024     0.000  3780.800\n[10,]  8575.490  3780.800     0.000"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#labelling-column-and-row-heanders-of-a-distance-matrix",
    "href": "In-class_Ex3/In-class_Ex3.html#labelling-column-and-row-heanders-of-a-distance-matrix",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Labelling column and row heanders of a distance matrix",
    "text": "Labelling column and row heanders of a distance matrix\n\nsz_names &lt;- mpsz$SUBZONE_C\n\n\ncolnames(dist) &lt;- paste0(sz_names)\nrownames(dist) &lt;- paste0(sz_names)"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#pivoting-distance-value-by-subzone_c",
    "href": "In-class_Ex3/In-class_Ex3.html#pivoting-distance-value-by-subzone_c",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Pivoting distance value by SUBZONE_C",
    "text": "Pivoting distance value by SUBZONE_C\nThis will generate a list of pair of location1 and location2.\nNumber of rows should be:\n\\[\nn_{loc}^2 = 332^2 = 110224\n\\]\n\ndistPair &lt;- melt(dist) %&gt;%\n  rename(dist = value)\nhead(distPair, 10)\n\n     Var1   Var2      dist\n1  MESZ01 MESZ01     0.000\n2  RVSZ05 MESZ01  3926.003\n3  SRSZ01 MESZ01  3939.108\n4  WISZ01 MESZ01 20252.964\n5  MUSZ02 MESZ01  2989.984\n6  MPSZ05 MESZ01  1431.330\n7  WISZ03 MESZ01 19211.836\n8  WISZ02 MESZ01 14960.942\n9  SISZ02 MESZ01  7515.256\n10 SISZ01 MESZ01  6391.342\n\n\n\nnrow(distPair)\n\n[1] 110224"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#separating-intra-flow-from-passenger-volume-df",
    "href": "In-class_Ex3/In-class_Ex3.html#separating-intra-flow-from-passenger-volume-df",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Separating intra-flow from passenger volume df",
    "text": "Separating intra-flow from passenger volume df\n\nflow_data$FlowNoIntra &lt;- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, \n  0, flow_data$TRIPS)\nflow_data$offset &lt;- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, \n  0.000001, 1)"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#combining-passenger-volume-data-with-distance-value",
    "href": "In-class_Ex3/In-class_Ex3.html#combining-passenger-volume-data-with-distance-value",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Combining passenger volume data with distance value",
    "text": "Combining passenger volume data with distance value\n\nflow_data$ORIGIN_SZ &lt;- as.factor(flow_data$ORIGIN_SZ)\nflow_data$DESTIN_SZ &lt;- as.factor(flow_data$DESTIN_SZ)\n\n\nflow_data1 &lt;- flow_data %&gt;%\n  left_join (distPair,\n             by = c(\"ORIGIN_SZ\" = \"orig\",\n                    \"DESTIN_SZ\" = \"dest\"))"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#importing-population-data",
    "href": "In-class_Ex3/In-class_Ex3.html#importing-population-data",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Importing population data",
    "text": "Importing population data\n\npop &lt;- read_csv(\"data/aspatial/pop.csv\")"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#geospatial-data-wrangling",
    "href": "In-class_Ex3/In-class_Ex3.html#geospatial-data-wrangling",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Geospatial data wrangling",
    "text": "Geospatial data wrangling\n\npop &lt;- pop %&gt;%\n  left_join(mpsz,\n            by = c(\"PA\" = \"PLN_AREA_N\",\n                   \"SZ\" = \"SUBZONE_N\")) %&gt;%\n  select(1:6) %&gt;%\n  rename(SZ_NAME = SZ,\n         SZ = SUBZONE_C)"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#preparing-origin-attribute",
    "href": "In-class_Ex3/In-class_Ex3.html#preparing-origin-attribute",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Preparing origin attribute",
    "text": "Preparing origin attribute\n\nflow_data1 &lt;- flow_data1 %&gt;%\n  left_join(pop,\n            by = c(ORIGIN_SZ = \"SZ\")) %&gt;%\n  rename(ORIGIN_AGE7_12 = AGE7_12,\n         ORIGIN_AGE13_24 = AGE13_24,\n         ORIGIN_AGE25_64 = AGE25_64) %&gt;%\n  select(-c(PA, SZ_NAME))"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#preparing-destination-attribute",
    "href": "In-class_Ex3/In-class_Ex3.html#preparing-destination-attribute",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Preparing destination attribute",
    "text": "Preparing destination attribute\n\nflow_data1 &lt;- flow_data1 %&gt;%\n  left_join(pop,\n            by = c(DESTIN_SZ = \"SZ\")) %&gt;%\n  rename(DESTIN_AGE7_12 = AGE7_12,\n         DESTIN_AGE13_24 = AGE13_24,\n         DESTIN_AGE25_64 = AGE25_64) %&gt;%\n  select(-c(PA, SZ_NAME))\n\n\nwrite_rds(flow_data1, \"data/rds/SIM_data.rds\")"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#importing-the-modelling-data",
    "href": "In-class_Ex3/In-class_Ex3.html#importing-the-modelling-data",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Importing the modelling data",
    "text": "Importing the modelling data\n\nSIM_data &lt;- read_rds(\"data/rds/SIM_data.rds\")"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#visualising-the-dependent-variable",
    "href": "In-class_Ex3/In-class_Ex3.html#visualising-the-dependent-variable",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Visualising the dependent variable",
    "text": "Visualising the dependent variable\nDependent variable = TRIPS\n\nggplot(data = SIM_data,\n       aes(x = TRIPS)) +\n  geom_histogram()\n\n\n\n\nThis is highly skewed, not resembling “bell curve” distribution.\n\nggplot(data = SIM_data,\n       aes(x = dist,\n           y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\nPlotting log axis resembles linear relationship more\n\nggplot(data = SIM_data,\n       aes(x = log(dist),\n           y = log(TRIPS))) +\n  geom_point() +\n  geom_smooth(method = lm)"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#checking-for-variables-with-zero-values",
    "href": "In-class_Ex3/In-class_Ex3.html#checking-for-variables-with-zero-values",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Checking for variables with zero values",
    "text": "Checking for variables with zero values\nPoisson regression uses log of values and since log(0) is undefined, we must ensure that there are no zeroes in SIM_data\n\nsummary(SIM_data)\n\n  ORIGIN_SZ          DESTIN_SZ             TRIPS           FlowNoIntra    \n Length:21079       Length:21079       Min.   :     1.0   Min.   :     0  \n Class :character   Class :character   1st Qu.:    18.0   1st Qu.:    16  \n Mode  :character   Mode  :character   Median :    91.0   Median :    84  \n                                       Mean   :  1199.6   Mean   :  1001  \n                                       3rd Qu.:   467.5   3rd Qu.:   431  \n                                       Max.   :341902.0   Max.   :230782  \n     offset              dist       ORIGIN_AGE7_12   ORIGIN_AGE13_24\n Min.   :0.000001   Min.   :   50   Min.   :   0.0   Min.   :    0  \n 1st Qu.:1.000000   1st Qu.: 3390   1st Qu.:  30.0   1st Qu.:  100  \n Median :1.000000   Median : 6186   Median : 510.0   Median : 1120  \n Mean   :0.986147   Mean   : 7019   Mean   : 883.4   Mean   : 1942  \n 3rd Qu.:1.000000   3rd Qu.: 9978   3rd Qu.:1360.0   3rd Qu.: 2950  \n Max.   :1.000000   Max.   :26136   Max.   :6340.0   Max.   :16380  \n ORIGIN_AGE25_64 DESTIN_AGE7_12   DESTIN_AGE13_24 DESTIN_AGE25_64\n Min.   :    0   Min.   :   0.0   Min.   :    0   Min.   :    0  \n 1st Qu.:  730   1st Qu.:  10.0   1st Qu.:   60   1st Qu.:  630  \n Median : 5640   Median : 510.0   Median : 1100   Median : 5710  \n Mean   : 9034   Mean   : 852.3   Mean   : 1891   Mean   : 8803  \n 3rd Qu.:14180   3rd Qu.:1350.0   3rd Qu.: 2920   3rd Qu.:13830  \n Max.   :74610   Max.   :6340.0   Max.   :16380   Max.   :74610  \n\n\nThe print report above reveals that variables ORIGIN_AGE7_12, ORIGIN_AGE13_24, ORIGIN_AGE25_64,DESTIN_AGE7_12, DESTIN_AGE13_24, DESTIN_AGE25_64 consist of 0 values.\nIn view of this, code chunk below will be used to replace zero values to 0.99.\n\nSIM_data$ORIGIN_AGE7_12[SIM_data$ORIGIN_AGE7_12 == 0] &lt;- 0.99\nSIM_data$ORIGIN_AGE13_24[SIM_data$ORIGIN_AGE13_24 == 0] &lt;- 0.99\nSIM_data$ORIGIN_AGE25_64[SIM_data$ORIGIN_AGE25_64 == 0] &lt;- 0.99\nSIM_data$DESTIN_AGE7_12[SIM_data$DESTIN_AGE7_12 == 0] &lt;- 0.99\nSIM_data$DESTIN_AGE13_23[SIM_data$DESTIN_AGE13_24 == 0] &lt;- 0.99\nSIM_data$DESTIN_AGE25_64[SIM_data$DESTIN_AGE25_64 == 0] &lt;- 0.99\n\nVerify the values\n\nsummary(SIM_data)\n\n  ORIGIN_SZ          DESTIN_SZ             TRIPS           FlowNoIntra    \n Length:21079       Length:21079       Min.   :     1.0   Min.   :     0  \n Class :character   Class :character   1st Qu.:    18.0   1st Qu.:    16  \n Mode  :character   Mode  :character   Median :    91.0   Median :    84  \n                                       Mean   :  1199.6   Mean   :  1001  \n                                       3rd Qu.:   467.5   3rd Qu.:   431  \n                                       Max.   :341902.0   Max.   :230782  \n                                                                          \n     offset              dist       ORIGIN_AGE7_12    ORIGIN_AGE13_24   \n Min.   :0.000001   Min.   :   50   Min.   :   0.99   Min.   :    0.99  \n 1st Qu.:1.000000   1st Qu.: 3390   1st Qu.:  30.00   1st Qu.:  100.00  \n Median :1.000000   Median : 6186   Median : 510.00   Median : 1120.00  \n Mean   :0.986147   Mean   : 7019   Mean   : 883.65   Mean   : 1942.45  \n 3rd Qu.:1.000000   3rd Qu.: 9978   3rd Qu.:1360.00   3rd Qu.: 2950.00  \n Max.   :1.000000   Max.   :26136   Max.   :6340.00   Max.   :16380.00  \n                                                                        \n ORIGIN_AGE25_64    DESTIN_AGE7_12    DESTIN_AGE13_24 DESTIN_AGE25_64   \n Min.   :    0.99   Min.   :   0.99   Min.   :    0   Min.   :    0.99  \n 1st Qu.:  730.00   1st Qu.:  10.00   1st Qu.:   60   1st Qu.:  630.00  \n Median : 5640.00   Median : 510.00   Median : 1100   Median : 5710.00  \n Mean   : 9034.46   Mean   : 852.50   Mean   : 1891   Mean   : 8803.25  \n 3rd Qu.:14180.00   3rd Qu.:1350.00   3rd Qu.: 2920   3rd Qu.:13830.00  \n Max.   :74610.00   Max.   :6340.00   Max.   :16380   Max.   :74610.00  \n                                                                        \n DESTIN_AGE13_23\n Min.   :0.99   \n 1st Qu.:0.99   \n Median :0.99   \n Mean   :0.99   \n 3rd Qu.:0.99   \n Max.   :0.99   \n NA's   :16861"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#unconstrained-spatial-interaction-model",
    "href": "In-class_Ex3/In-class_Ex3.html#unconstrained-spatial-interaction-model",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Unconstrained Spatial Interaction Model",
    "text": "Unconstrained Spatial Interaction Model\nThe general formula of Unconstrained Spatial Interaction Model\n\\[\n\\lambda_{ij} = exp(k + \\mu lnV_i + \\alpha lnW_j + \\beta ln d_{ij})\n\\]\nCalibrate model\n\nuncSIM &lt;- glm(formula = TRIPS ~ \n                log(ORIGIN_AGE25_64) + \n                log(DESTIN_AGE25_64) +\n                log(dist),\n              family = poisson(link = \"log\"),\n              data = SIM_data,\n              na.action = na.exclude)\nuncSIM\n\n\nCall:  glm(formula = TRIPS ~ log(ORIGIN_AGE25_64) + log(DESTIN_AGE25_64) + \n    log(dist), family = poisson(link = \"log\"), data = SIM_data, \n    na.action = na.exclude)\n\nCoefficients:\n         (Intercept)  log(ORIGIN_AGE25_64)  log(DESTIN_AGE25_64)  \n             10.5581                0.2651                0.0266  \n           log(dist)  \n             -0.7261  \n\nDegrees of Freedom: 21078 Total (i.e. Null);  21075 Residual\nNull Deviance:      107900000 \nResidual Deviance: 63930000     AIC: 64070000"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#r-squared-function",
    "href": "In-class_Ex3/In-class_Ex3.html#r-squared-function",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "R-squared function",
    "text": "R-squared function\nRead on this since I do not fully understand what this is for. i know we uses function since this is a model\n\nCalcRSquared &lt;- function(observed,estimated){\n  r &lt;- cor(observed,estimated)\n  R2 &lt;- r^2\n  R2\n}\n\n\nCalcRSquared(uncSIM$data$TRIPS, uncSIM$fitted.values)\n\n[1] 0.1922701\n\n\n\nr2_mcfadden(uncSIM)\n\n# R2 for Generalized Linear Regression\n       R2: 0.407\n  adj. R2: 0.407"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#origin-production-constrained-sim",
    "href": "In-class_Ex3/In-class_Ex3.html#origin-production-constrained-sim",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Origin (Production) constrained SIM",
    "text": "Origin (Production) constrained SIM\nThe general formula of Origin Constrained Spatial Interaction Model\n\\[\n\\lambda_{ij} = exp(k + \\mu_i + \\alpha lnW_j + \\beta ln d_{ij})\n\\]\nCode below takes a while to run so be patient\n\n# Set eval = true when need to recompute\norcSIM &lt;- glm(formula = TRIPS ~ \n                 ORIGIN_SZ +\n                 log(DESTIN_AGE25_64) +\n                 log(dist),\n              family = poisson(link = \"log\"),\n              data = SIM_data,\n              na.action = na.exclude)\nwrite_rds(orcSIM, \"data/rds/orcSIM.rds\")\n\n\norcSIM &lt;- read_rds(\"data/rds/orcSIM.rds\")\nsummary(orcSIM)\n\n\nCall:\nglm(formula = TRIPS ~ ORIGIN_SZ + log(DESTIN_AGE25_64) + log(dist), \n    family = poisson(link = \"log\"), data = SIM_data, na.action = na.exclude)\n\nCoefficients:\n                       Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept)           1.248e+01  3.120e-03  3999.413  &lt; 2e-16 ***\nORIGIN_SZAMSZ02       1.124e+00  3.634e-03   309.321  &lt; 2e-16 ***\nORIGIN_SZAMSZ03       6.726e-01  3.722e-03   180.682  &lt; 2e-16 ***\nORIGIN_SZAMSZ04       3.533e-02  4.204e-03     8.405  &lt; 2e-16 ***\nORIGIN_SZAMSZ05      -1.606e-01  4.821e-03   -33.312  &lt; 2e-16 ***\nORIGIN_SZAMSZ06       3.908e-01  4.340e-03    90.039  &lt; 2e-16 ***\nORIGIN_SZAMSZ07      -1.133e+00  7.309e-03  -154.971  &lt; 2e-16 ***\nORIGIN_SZAMSZ08      -7.993e-01  6.741e-03  -118.562  &lt; 2e-16 ***\nORIGIN_SZAMSZ09       2.485e-01  4.508e-03    55.135  &lt; 2e-16 ***\nORIGIN_SZAMSZ10       5.484e-01  3.921e-03   139.854  &lt; 2e-16 ***\nORIGIN_SZAMSZ11      -1.453e+00  8.925e-03  -162.850  &lt; 2e-16 ***\nORIGIN_SZAMSZ12      -1.469e+00  8.845e-03  -166.116  &lt; 2e-16 ***\nORIGIN_SZBDSZ01       1.085e+00  3.598e-03   301.519  &lt; 2e-16 ***\nORIGIN_SZBDSZ02       5.465e-01  4.160e-03   131.355  &lt; 2e-16 ***\nORIGIN_SZBDSZ03       1.103e+00  3.679e-03   299.797  &lt; 2e-16 ***\nORIGIN_SZBDSZ04       1.776e+00  3.232e-03   549.540  &lt; 2e-16 ***\nORIGIN_SZBDSZ05       8.316e-01  3.640e-03   228.493  &lt; 2e-16 ***\nORIGIN_SZBDSZ06       9.994e-01  3.694e-03   270.585  &lt; 2e-16 ***\nORIGIN_SZBDSZ07      -7.095e-01  6.207e-03  -114.312  &lt; 2e-16 ***\nORIGIN_SZBDSZ08      -5.959e-01  5.951e-03  -100.134  &lt; 2e-16 ***\nORIGIN_SZBKSZ01      -2.838e-01  5.258e-03   -53.977  &lt; 2e-16 ***\nORIGIN_SZBKSZ02       4.914e-01  4.198e-03   117.071  &lt; 2e-16 ***\nORIGIN_SZBKSZ03       7.302e-01  3.950e-03   184.855  &lt; 2e-16 ***\nORIGIN_SZBKSZ04      -2.259e-02  4.844e-03    -4.663 3.12e-06 ***\nORIGIN_SZBKSZ05       4.191e-02  4.545e-03     9.219  &lt; 2e-16 ***\nORIGIN_SZBKSZ06       1.389e-01  4.806e-03    28.906  &lt; 2e-16 ***\nORIGIN_SZBKSZ07       8.048e-01  3.640e-03   221.102  &lt; 2e-16 ***\nORIGIN_SZBKSZ08       1.480e-01  4.298e-03    34.442  &lt; 2e-16 ***\nORIGIN_SZBKSZ09       1.008e-01  4.534e-03    22.228  &lt; 2e-16 ***\nORIGIN_SZBLSZ01      -1.258e+00  1.028e-02  -122.405  &lt; 2e-16 ***\nORIGIN_SZBLSZ02      -1.919e+00  1.449e-02  -132.426  &lt; 2e-16 ***\nORIGIN_SZBLSZ03      -3.157e+00  3.250e-02   -97.132  &lt; 2e-16 ***\nORIGIN_SZBLSZ04      -2.050e+00  1.853e-02  -110.667  &lt; 2e-16 ***\nORIGIN_SZBMSZ01       2.155e-01  3.927e-03    54.869  &lt; 2e-16 ***\nORIGIN_SZBMSZ02      -1.128e+00  5.638e-03  -200.114  &lt; 2e-16 ***\nORIGIN_SZBMSZ03      -3.022e-01  4.492e-03   -67.283  &lt; 2e-16 ***\nORIGIN_SZBMSZ04       8.771e-02  3.988e-03    21.995  &lt; 2e-16 ***\nORIGIN_SZBMSZ05      -1.210e+00  5.879e-03  -205.758  &lt; 2e-16 ***\nORIGIN_SZBMSZ06      -1.754e+00  9.449e-03  -185.637  &lt; 2e-16 ***\nORIGIN_SZBMSZ07      -3.479e-01  4.435e-03   -78.443  &lt; 2e-16 ***\nORIGIN_SZBMSZ08      -4.323e-01  4.449e-03   -97.181  &lt; 2e-16 ***\nORIGIN_SZBMSZ09      -1.095e+00  5.780e-03  -189.470  &lt; 2e-16 ***\nORIGIN_SZBMSZ10      -1.294e+00  6.359e-03  -203.489  &lt; 2e-16 ***\nORIGIN_SZBMSZ11      -7.901e-01  5.307e-03  -148.894  &lt; 2e-16 ***\nORIGIN_SZBMSZ12      -1.033e+00  6.920e-03  -149.342  &lt; 2e-16 ***\nORIGIN_SZBMSZ13      -1.941e-02  4.417e-03    -4.393 1.12e-05 ***\nORIGIN_SZBMSZ14      -5.485e-01  5.214e-03  -105.187  &lt; 2e-16 ***\nORIGIN_SZBMSZ15      -2.578e-01  4.742e-03   -54.364  &lt; 2e-16 ***\nORIGIN_SZBMSZ16      -1.266e+00  6.089e-03  -207.847  &lt; 2e-16 ***\nORIGIN_SZBMSZ17      -1.631e+00  9.219e-03  -176.921  &lt; 2e-16 ***\nORIGIN_SZBPSZ01       2.640e-01  4.434e-03    59.536  &lt; 2e-16 ***\nORIGIN_SZBPSZ02       2.693e-01  4.896e-03    55.003  &lt; 2e-16 ***\nORIGIN_SZBPSZ03       5.129e-01  4.548e-03   112.761  &lt; 2e-16 ***\nORIGIN_SZBPSZ04       5.876e-01  4.019e-03   146.223  &lt; 2e-16 ***\nORIGIN_SZBPSZ05       6.137e-01  3.748e-03   163.747  &lt; 2e-16 ***\nORIGIN_SZBPSZ06      -1.082e+00  6.867e-03  -157.584  &lt; 2e-16 ***\nORIGIN_SZBPSZ07      -7.816e-01  6.418e-03  -121.778  &lt; 2e-16 ***\nORIGIN_SZBSSZ01       3.383e-02  4.305e-03     7.858 3.89e-15 ***\nORIGIN_SZBSSZ02       4.096e-01  3.892e-03   105.227  &lt; 2e-16 ***\nORIGIN_SZBSSZ03       3.323e-01  3.854e-03    86.223  &lt; 2e-16 ***\nORIGIN_SZBTSZ01       1.239e-01  4.223e-03    29.341  &lt; 2e-16 ***\nORIGIN_SZBTSZ02      -9.493e-01  6.018e-03  -157.744  &lt; 2e-16 ***\nORIGIN_SZBTSZ03       3.488e-03  4.402e-03     0.792    0.428    \nORIGIN_SZBTSZ04      -7.666e-01  7.692e-03   -99.671  &lt; 2e-16 ***\nORIGIN_SZBTSZ05      -1.484e+00  8.412e-03  -176.430  &lt; 2e-16 ***\nORIGIN_SZBTSZ06      -6.652e-01  5.639e-03  -117.969  &lt; 2e-16 ***\nORIGIN_SZBTSZ07      -1.954e+00  9.134e-03  -213.896  &lt; 2e-16 ***\nORIGIN_SZBTSZ08      -1.247e+00  7.314e-03  -170.452  &lt; 2e-16 ***\nORIGIN_SZCBSZ01      -1.339e+00  3.659e-02   -36.603  &lt; 2e-16 ***\nORIGIN_SZCCSZ01      -1.665e+00  1.089e-02  -152.810  &lt; 2e-16 ***\nORIGIN_SZCHSZ01      -1.168e+00  9.436e-03  -123.806  &lt; 2e-16 ***\nORIGIN_SZCHSZ02      -5.865e-01  6.628e-03   -88.490  &lt; 2e-16 ***\nORIGIN_SZCHSZ03       6.401e-01  4.593e-03   139.354  &lt; 2e-16 ***\nORIGIN_SZCKSZ01       4.284e-01  4.044e-03   105.929  &lt; 2e-16 ***\nORIGIN_SZCKSZ02       8.584e-01  4.103e-03   209.225  &lt; 2e-16 ***\nORIGIN_SZCKSZ03       9.465e-01  3.690e-03   256.528  &lt; 2e-16 ***\nORIGIN_SZCKSZ04       1.417e+00  3.747e-03   378.210  &lt; 2e-16 ***\nORIGIN_SZCKSZ05       1.095e+00  4.321e-03   253.381  &lt; 2e-16 ***\nORIGIN_SZCKSZ06       1.330e+00  4.139e-03   321.308  &lt; 2e-16 ***\nORIGIN_SZCLSZ01      -3.466e-01  5.639e-03   -61.467  &lt; 2e-16 ***\nORIGIN_SZCLSZ02      -1.569e+00  1.047e-02  -149.856  &lt; 2e-16 ***\nORIGIN_SZCLSZ03      -6.001e-01  5.736e-03  -104.606  &lt; 2e-16 ***\nORIGIN_SZCLSZ04       9.266e-01  3.584e-03   258.496  &lt; 2e-16 ***\nORIGIN_SZCLSZ05      -1.782e+00  1.053e-02  -169.192  &lt; 2e-16 ***\nORIGIN_SZCLSZ06       9.700e-01  3.446e-03   281.495  &lt; 2e-16 ***\nORIGIN_SZCLSZ07      -8.809e-02  4.361e-03   -20.198  &lt; 2e-16 ***\nORIGIN_SZCLSZ08       3.162e-01  4.839e-03    65.347  &lt; 2e-16 ***\nORIGIN_SZCLSZ09      -1.663e+00  1.328e-02  -125.239  &lt; 2e-16 ***\nORIGIN_SZDTSZ01      -1.671e+00  6.863e-03  -243.421  &lt; 2e-16 ***\nORIGIN_SZDTSZ02      -1.496e+00  6.219e-03  -240.516  &lt; 2e-16 ***\nORIGIN_SZDTSZ03      -2.426e+00  1.066e-02  -227.490  &lt; 2e-16 ***\nORIGIN_SZDTSZ04      -3.813e+00  9.627e-02   -39.611  &lt; 2e-16 ***\nORIGIN_SZDTSZ05      -2.902e+00  1.927e-02  -150.597  &lt; 2e-16 ***\nORIGIN_SZDTSZ06      -2.918e+00  1.585e-02  -184.061  &lt; 2e-16 ***\nORIGIN_SZDTSZ07      -1.949e+00  1.980e-02   -98.450  &lt; 2e-16 ***\nORIGIN_SZDTSZ08      -2.399e+00  9.864e-03  -243.188  &lt; 2e-16 ***\nORIGIN_SZDTSZ09      -2.958e+00  1.945e-02  -152.100  &lt; 2e-16 ***\nORIGIN_SZDTSZ10      -2.237e+00  1.072e-02  -208.693  &lt; 2e-16 ***\nORIGIN_SZDTSZ11      -2.275e+00  1.017e-02  -223.576  &lt; 2e-16 ***\nORIGIN_SZDTSZ12      -3.324e+00  2.239e-02  -148.449  &lt; 2e-16 ***\nORIGIN_SZDTSZ13      -2.317e+00  1.176e-02  -197.030  &lt; 2e-16 ***\nORIGIN_SZGLSZ01      -1.289e+00  7.039e-03  -183.171  &lt; 2e-16 ***\nORIGIN_SZGLSZ02       2.806e-01  4.003e-03    70.104  &lt; 2e-16 ***\nORIGIN_SZGLSZ03       2.891e-01  3.984e-03    72.574  &lt; 2e-16 ***\nORIGIN_SZGLSZ04       1.031e+00  3.381e-03   304.907  &lt; 2e-16 ***\nORIGIN_SZGLSZ05       7.491e-01  3.555e-03   210.720  &lt; 2e-16 ***\nORIGIN_SZHGSZ01       2.854e-01  3.958e-03    72.108  &lt; 2e-16 ***\nORIGIN_SZHGSZ02       6.359e-01  3.819e-03   166.482  &lt; 2e-16 ***\nORIGIN_SZHGSZ03       2.954e-01  4.188e-03    70.540  &lt; 2e-16 ***\nORIGIN_SZHGSZ04       9.978e-01  3.549e-03   281.156  &lt; 2e-16 ***\nORIGIN_SZHGSZ05       1.304e+00  3.491e-03   373.630  &lt; 2e-16 ***\nORIGIN_SZHGSZ06       4.524e-02  4.332e-03    10.442  &lt; 2e-16 ***\nORIGIN_SZHGSZ07       7.382e-01  3.697e-03   199.668  &lt; 2e-16 ***\nORIGIN_SZHGSZ08       3.001e-01  4.229e-03    70.960  &lt; 2e-16 ***\nORIGIN_SZHGSZ09      -4.800e-01  5.683e-03   -84.451  &lt; 2e-16 ***\nORIGIN_SZHGSZ10      -3.684e+00  3.980e-02   -92.580  &lt; 2e-16 ***\nORIGIN_SZJESZ01       4.732e-01  4.045e-03   117.000  &lt; 2e-16 ***\nORIGIN_SZJESZ02       3.301e-01  4.063e-03    81.243  &lt; 2e-16 ***\nORIGIN_SZJESZ03       2.999e-01  4.302e-03    69.703  &lt; 2e-16 ***\nORIGIN_SZJESZ04      -9.352e-01  7.062e-03  -132.435  &lt; 2e-16 ***\nORIGIN_SZJESZ05      -2.012e+00  1.135e-02  -177.223  &lt; 2e-16 ***\nORIGIN_SZJESZ06       3.172e-01  3.990e-03    79.501  &lt; 2e-16 ***\nORIGIN_SZJESZ07      -1.615e+00  8.865e-03  -182.163  &lt; 2e-16 ***\nORIGIN_SZJESZ08      -4.563e-01  7.844e-03   -58.178  &lt; 2e-16 ***\nORIGIN_SZJESZ09       5.375e-01  4.117e-03   130.544  &lt; 2e-16 ***\nORIGIN_SZJESZ10      -1.892e+00  1.608e-02  -117.633  &lt; 2e-16 ***\nORIGIN_SZJESZ11      -2.059e+00  1.674e-02  -122.984  &lt; 2e-16 ***\nORIGIN_SZJWSZ01       3.129e-01  5.248e-03    59.616  &lt; 2e-16 ***\nORIGIN_SZJWSZ02       9.110e-01  3.743e-03   243.403  &lt; 2e-16 ***\nORIGIN_SZJWSZ03       1.343e+00  3.477e-03   386.229  &lt; 2e-16 ***\nORIGIN_SZJWSZ04       1.419e+00  3.532e-03   401.821  &lt; 2e-16 ***\nORIGIN_SZJWSZ05      -1.303e+00  1.033e-02  -126.106  &lt; 2e-16 ***\nORIGIN_SZJWSZ06      -8.173e-01  8.371e-03   -97.626  &lt; 2e-16 ***\nORIGIN_SZJWSZ07      -2.339e+00  1.925e-02  -121.517  &lt; 2e-16 ***\nORIGIN_SZJWSZ08       2.040e+00  3.392e-03   601.430  &lt; 2e-16 ***\nORIGIN_SZJWSZ09       1.967e+00  3.221e-03   610.569  &lt; 2e-16 ***\nORIGIN_SZKLSZ01       2.366e-01  3.879e-03    61.006  &lt; 2e-16 ***\nORIGIN_SZKLSZ02      -3.671e-01  4.776e-03   -76.858  &lt; 2e-16 ***\nORIGIN_SZKLSZ03      -3.565e-01  4.801e-03   -74.245  &lt; 2e-16 ***\nORIGIN_SZKLSZ04      -1.610e+00  6.831e-03  -235.675  &lt; 2e-16 ***\nORIGIN_SZKLSZ05      -5.115e-01  5.637e-03   -90.732  &lt; 2e-16 ***\nORIGIN_SZKLSZ06      -5.180e-01  4.509e-03  -114.890  &lt; 2e-16 ***\nORIGIN_SZKLSZ07      -9.038e-01  5.938e-03  -152.218  &lt; 2e-16 ***\nORIGIN_SZKLSZ08      -7.426e-01  5.138e-03  -144.513  &lt; 2e-16 ***\nORIGIN_SZKLSZ09      -1.453e+00  6.449e-03  -225.271  &lt; 2e-16 ***\nORIGIN_SZLKSZ01      -3.013e+00  2.955e-02  -101.962  &lt; 2e-16 ***\nORIGIN_SZMDSZ01      -2.316e+00  2.098e-02  -110.417  &lt; 2e-16 ***\nORIGIN_SZMDSZ02      -9.612e-01  9.228e-03  -104.167  &lt; 2e-16 ***\nORIGIN_SZMDSZ03      -1.731e+00  1.292e-02  -133.993  &lt; 2e-16 ***\nORIGIN_SZMPSZ01      -6.389e-01  5.681e-03  -112.460  &lt; 2e-16 ***\nORIGIN_SZMPSZ02      -3.572e-01  5.156e-03   -69.271  &lt; 2e-16 ***\nORIGIN_SZMPSZ03       4.831e-01  3.895e-03   124.033  &lt; 2e-16 ***\nORIGIN_SZMSSZ01      -8.004e+00  3.780e-01   -21.177  &lt; 2e-16 ***\nORIGIN_SZMUSZ01      -1.113e+00  5.491e-03  -202.702  &lt; 2e-16 ***\nORIGIN_SZMUSZ02      -3.153e+00  1.465e-02  -215.292  &lt; 2e-16 ***\nORIGIN_SZMUSZ03      -1.685e+00  6.477e-03  -260.140  &lt; 2e-16 ***\nORIGIN_SZNTSZ01      -2.378e+00  2.497e-02   -95.235  &lt; 2e-16 ***\nORIGIN_SZNTSZ02      -2.571e+00  1.308e-02  -196.513  &lt; 2e-16 ***\nORIGIN_SZNTSZ03      -8.705e-01  5.982e-03  -145.509  &lt; 2e-16 ***\nORIGIN_SZNTSZ05      -3.166e+00  3.775e-02   -83.876  &lt; 2e-16 ***\nORIGIN_SZNTSZ06      -3.281e+00  3.721e-02   -88.181  &lt; 2e-16 ***\nORIGIN_SZNVSZ01       6.852e-01  3.533e-03   193.963  &lt; 2e-16 ***\nORIGIN_SZNVSZ02      -4.471e-01  4.787e-03   -93.407  &lt; 2e-16 ***\nORIGIN_SZNVSZ03      -1.068e+00  5.948e-03  -179.622  &lt; 2e-16 ***\nORIGIN_SZNVSZ04      -1.305e+00  7.131e-03  -183.054  &lt; 2e-16 ***\nORIGIN_SZNVSZ05      -2.572e+00  1.298e-02  -198.117  &lt; 2e-16 ***\nORIGIN_SZORSZ01      -3.274e+00  3.142e-02  -104.208  &lt; 2e-16 ***\nORIGIN_SZORSZ02      -1.142e+00  5.614e-03  -203.502  &lt; 2e-16 ***\nORIGIN_SZORSZ03      -1.549e+00  6.523e-03  -237.415  &lt; 2e-16 ***\nORIGIN_SZOTSZ01      -1.726e+00  7.143e-03  -241.695  &lt; 2e-16 ***\nORIGIN_SZOTSZ02      -1.783e+00  8.149e-03  -218.809  &lt; 2e-16 ***\nORIGIN_SZOTSZ03      -7.302e-01  5.163e-03  -141.430  &lt; 2e-16 ***\nORIGIN_SZOTSZ04      -6.910e-01  8.201e-03   -84.265  &lt; 2e-16 ***\nORIGIN_SZPGSZ01      -7.081e-01  9.319e-03   -75.982  &lt; 2e-16 ***\nORIGIN_SZPGSZ02      -2.873e-01  5.628e-03   -51.054  &lt; 2e-16 ***\nORIGIN_SZPGSZ03       1.117e+00  3.622e-03   308.236  &lt; 2e-16 ***\nORIGIN_SZPGSZ04       1.271e+00  3.600e-03   353.120  &lt; 2e-16 ***\nORIGIN_SZPGSZ05       4.576e-01  4.567e-03   100.194  &lt; 2e-16 ***\nORIGIN_SZPLSZ01      -4.863e-01  7.811e-03   -62.260  &lt; 2e-16 ***\nORIGIN_SZPLSZ02      -1.395e+00  1.127e-02  -123.798  &lt; 2e-16 ***\nORIGIN_SZPLSZ03      -2.887e+00  3.051e-02   -94.644  &lt; 2e-16 ***\nORIGIN_SZPLSZ04      -2.973e+00  2.745e-02  -108.318  &lt; 2e-16 ***\nORIGIN_SZPLSZ05      -2.266e+00  1.769e-02  -128.051  &lt; 2e-16 ***\nORIGIN_SZPNSZ01       1.587e+00  3.754e-03   422.759  &lt; 2e-16 ***\nORIGIN_SZPNSZ02      -5.415e-01  9.699e-03   -55.833  &lt; 2e-16 ***\nORIGIN_SZPNSZ03      -1.775e+00  1.598e-02  -111.097  &lt; 2e-16 ***\nORIGIN_SZPNSZ04      -2.621e+00  2.515e-02  -104.227  &lt; 2e-16 ***\nORIGIN_SZPNSZ05      -1.984e+00  1.964e-02  -101.014  &lt; 2e-16 ***\nORIGIN_SZPRSZ01      -6.841e-01  9.235e-03   -74.076  &lt; 2e-16 ***\nORIGIN_SZPRSZ02       1.090e+00  3.766e-03   289.455  &lt; 2e-16 ***\nORIGIN_SZPRSZ03       8.890e-01  3.779e-03   235.217  &lt; 2e-16 ***\nORIGIN_SZPRSZ04      -1.821e-01  5.928e-03   -30.727  &lt; 2e-16 ***\nORIGIN_SZPRSZ05       1.350e+00  3.582e-03   377.057  &lt; 2e-16 ***\nORIGIN_SZPRSZ06      -4.483e-01  6.701e-03   -66.894  &lt; 2e-16 ***\nORIGIN_SZPRSZ07      -2.476e+00  1.628e-02  -152.098  &lt; 2e-16 ***\nORIGIN_SZPRSZ08       1.937e-01  4.915e-03    39.400  &lt; 2e-16 ***\nORIGIN_SZQTSZ01      -3.243e-01  5.274e-03   -61.494  &lt; 2e-16 ***\nORIGIN_SZQTSZ02      -6.310e-01  4.961e-03  -127.181  &lt; 2e-16 ***\nORIGIN_SZQTSZ03      -1.513e-01  4.502e-03   -33.614  &lt; 2e-16 ***\nORIGIN_SZQTSZ04      -1.090e+00  6.159e-03  -176.985  &lt; 2e-16 ***\nORIGIN_SZQTSZ05      -1.867e-01  4.519e-03   -41.324  &lt; 2e-16 ***\nORIGIN_SZQTSZ06      -5.264e-01  5.161e-03  -101.989  &lt; 2e-16 ***\nORIGIN_SZQTSZ07      -1.487e+00  7.659e-03  -194.206  &lt; 2e-16 ***\nORIGIN_SZQTSZ08      -1.307e-01  4.646e-03   -28.139  &lt; 2e-16 ***\nORIGIN_SZQTSZ09      -5.076e-01  5.256e-03   -96.577  &lt; 2e-16 ***\nORIGIN_SZQTSZ10      -3.615e-01  5.173e-03   -69.883  &lt; 2e-16 ***\nORIGIN_SZQTSZ11      -1.213e+00  7.432e-03  -163.194  &lt; 2e-16 ***\nORIGIN_SZQTSZ12      -9.643e-01  7.175e-03  -134.403  &lt; 2e-16 ***\nORIGIN_SZQTSZ13      -6.932e-02  4.733e-03   -14.645  &lt; 2e-16 ***\nORIGIN_SZQTSZ14      -1.346e+00  7.128e-03  -188.850  &lt; 2e-16 ***\nORIGIN_SZQTSZ15      -7.666e-01  8.419e-03   -91.047  &lt; 2e-16 ***\nORIGIN_SZRCSZ01      -5.405e-01  5.128e-03  -105.401  &lt; 2e-16 ***\nORIGIN_SZRCSZ02      -2.127e+00  1.450e-02  -146.631  &lt; 2e-16 ***\nORIGIN_SZRCSZ03      -1.370e+00  7.041e-03  -194.615  &lt; 2e-16 ***\nORIGIN_SZRCSZ04      -2.080e+00  1.048e-02  -198.372  &lt; 2e-16 ***\nORIGIN_SZRCSZ05      -2.454e+00  1.149e-02  -213.662  &lt; 2e-16 ***\nORIGIN_SZRCSZ06      -4.617e-01  6.951e-03   -66.421  &lt; 2e-16 ***\nORIGIN_SZRCSZ08      -2.510e+00  1.503e-02  -167.065  &lt; 2e-16 ***\nORIGIN_SZRCSZ09      -1.980e+00  1.213e-02  -163.226  &lt; 2e-16 ***\nORIGIN_SZRCSZ10      -1.726e+00  6.757e-03  -255.384  &lt; 2e-16 ***\nORIGIN_SZRVSZ01      -2.625e+00  1.192e-02  -220.209  &lt; 2e-16 ***\nORIGIN_SZRVSZ02      -1.143e+00  6.674e-03  -171.222  &lt; 2e-16 ***\nORIGIN_SZRVSZ03      -1.888e+00  9.575e-03  -197.130  &lt; 2e-16 ***\nORIGIN_SZRVSZ04      -1.888e+00  1.406e-02  -134.282  &lt; 2e-16 ***\nORIGIN_SZRVSZ05      -2.167e+00  1.200e-02  -180.638  &lt; 2e-16 ***\nORIGIN_SZSBSZ01       7.978e-01  4.424e-03   180.325  &lt; 2e-16 ***\nORIGIN_SZSBSZ02      -5.494e-01  6.389e-03   -85.992  &lt; 2e-16 ***\nORIGIN_SZSBSZ03       1.033e+00  3.810e-03   271.140  &lt; 2e-16 ***\nORIGIN_SZSBSZ04       8.724e-01  4.296e-03   203.078  &lt; 2e-16 ***\nORIGIN_SZSBSZ05      -9.082e-02  5.473e-03   -16.595  &lt; 2e-16 ***\nORIGIN_SZSBSZ06      -1.653e+00  1.362e-02  -121.379  &lt; 2e-16 ***\nORIGIN_SZSBSZ07      -7.726e-01  9.103e-03   -84.868  &lt; 2e-16 ***\nORIGIN_SZSBSZ08      -9.319e-01  9.273e-03  -100.492  &lt; 2e-16 ***\nORIGIN_SZSBSZ09      -4.025e-01  6.936e-03   -58.030  &lt; 2e-16 ***\nORIGIN_SZSESZ02       1.153e+00  3.601e-03   320.082  &lt; 2e-16 ***\nORIGIN_SZSESZ03       1.313e+00  3.431e-03   382.572  &lt; 2e-16 ***\nORIGIN_SZSESZ04       1.021e+00  3.918e-03   260.688  &lt; 2e-16 ***\nORIGIN_SZSESZ05      -9.416e-02  4.788e-03   -19.666  &lt; 2e-16 ***\nORIGIN_SZSESZ06       9.837e-01  3.757e-03   261.855  &lt; 2e-16 ***\nORIGIN_SZSESZ07      -2.211e+00  1.414e-02  -156.357  &lt; 2e-16 ***\nORIGIN_SZSGSZ01      -8.292e-01  6.831e-03  -121.386  &lt; 2e-16 ***\nORIGIN_SZSGSZ02      -1.056e+00  8.079e-03  -130.724  &lt; 2e-16 ***\nORIGIN_SZSGSZ03       3.695e-01  4.229e-03    87.373  &lt; 2e-16 ***\nORIGIN_SZSGSZ04       3.816e-01  3.902e-03    97.792  &lt; 2e-16 ***\nORIGIN_SZSGSZ05      -1.552e+00  8.113e-03  -191.326  &lt; 2e-16 ***\nORIGIN_SZSGSZ06       4.913e-01  3.726e-03   131.859  &lt; 2e-16 ***\nORIGIN_SZSGSZ07      -4.579e-01  4.890e-03   -93.629  &lt; 2e-16 ***\nORIGIN_SZSKSZ01      -7.297e-02  6.287e-03   -11.606  &lt; 2e-16 ***\nORIGIN_SZSKSZ02       5.280e-01  4.561e-03   115.770  &lt; 2e-16 ***\nORIGIN_SZSKSZ03      -4.654e-01  6.207e-03   -74.979  &lt; 2e-16 ***\nORIGIN_SZSKSZ04      -2.210e+00  2.034e-02  -108.637  &lt; 2e-16 ***\nORIGIN_SZSKSZ05      -1.204e+00  1.240e-02   -97.096  &lt; 2e-16 ***\nORIGIN_SZSLSZ01      -2.981e+00  2.395e-02  -124.463  &lt; 2e-16 ***\nORIGIN_SZSLSZ04      -3.078e-01  5.752e-03   -53.511  &lt; 2e-16 ***\nORIGIN_SZSRSZ01      -1.549e+00  7.033e-03  -220.279  &lt; 2e-16 ***\nORIGIN_SZSRSZ02      -1.743e+00  7.087e-03  -245.969  &lt; 2e-16 ***\nORIGIN_SZSRSZ03      -2.717e+00  1.466e-02  -185.318  &lt; 2e-16 ***\nORIGIN_SZSVSZ01      -3.635e+00  4.302e-02   -84.502  &lt; 2e-16 ***\nORIGIN_SZTHSZ01      -1.777e+00  3.731e-02   -47.637  &lt; 2e-16 ***\nORIGIN_SZTHSZ03      -1.775e+00  1.432e-02  -123.915  &lt; 2e-16 ***\nORIGIN_SZTHSZ04      -2.848e+00  2.380e-02  -119.643  &lt; 2e-16 ***\nORIGIN_SZTHSZ06      -2.135e+00  1.412e-02  -151.214  &lt; 2e-16 ***\nORIGIN_SZTMSZ01       9.896e-01  4.127e-03   239.794  &lt; 2e-16 ***\nORIGIN_SZTMSZ02       2.245e+00  3.150e-03   712.723  &lt; 2e-16 ***\nORIGIN_SZTMSZ03       1.537e+00  3.371e-03   455.888  &lt; 2e-16 ***\nORIGIN_SZTMSZ04       9.674e-01  3.857e-03   250.824  &lt; 2e-16 ***\nORIGIN_SZTMSZ05      -1.698e-01  5.981e-03   -28.393  &lt; 2e-16 ***\nORIGIN_SZTNSZ01      -1.139e+00  6.062e-03  -187.873  &lt; 2e-16 ***\nORIGIN_SZTNSZ02      -1.007e+00  5.702e-03  -176.645  &lt; 2e-16 ***\nORIGIN_SZTNSZ03      -1.493e+00  7.513e-03  -198.737  &lt; 2e-16 ***\nORIGIN_SZTNSZ04      -6.752e-01  5.499e-03  -122.794  &lt; 2e-16 ***\nORIGIN_SZTPSZ01      -6.285e-01  5.089e-03  -123.511  &lt; 2e-16 ***\nORIGIN_SZTPSZ02       5.057e-01  3.574e-03   141.478  &lt; 2e-16 ***\nORIGIN_SZTPSZ03      -3.873e-01  5.002e-03   -77.421  &lt; 2e-16 ***\nORIGIN_SZTPSZ04      -1.657e-01  4.690e-03   -35.332  &lt; 2e-16 ***\nORIGIN_SZTPSZ05      -1.269e-01  4.886e-03   -25.978  &lt; 2e-16 ***\nORIGIN_SZTPSZ06       3.002e-01  4.901e-03    61.246  &lt; 2e-16 ***\nORIGIN_SZTPSZ07      -1.179e-01  5.015e-03   -23.520  &lt; 2e-16 ***\nORIGIN_SZTPSZ08      -7.176e-01  6.731e-03  -106.604  &lt; 2e-16 ***\nORIGIN_SZTPSZ09      -3.290e-01  5.129e-03   -64.154  &lt; 2e-16 ***\nORIGIN_SZTPSZ10      -4.900e-01  5.651e-03   -86.702  &lt; 2e-16 ***\nORIGIN_SZTPSZ11       3.176e-01  4.184e-03    75.899  &lt; 2e-16 ***\nORIGIN_SZTPSZ12      -4.276e-01  5.223e-03   -81.858  &lt; 2e-16 ***\nORIGIN_SZTSSZ01      -3.300e+00  3.679e-02   -89.695  &lt; 2e-16 ***\nORIGIN_SZTSSZ02       5.044e-01  5.837e-03    86.423  &lt; 2e-16 ***\nORIGIN_SZTSSZ03       5.370e-01  5.710e-03    94.046  &lt; 2e-16 ***\nORIGIN_SZTSSZ04       5.531e-01  5.946e-03    93.021  &lt; 2e-16 ***\nORIGIN_SZTSSZ05      -9.746e-01  1.155e-02   -84.378  &lt; 2e-16 ***\nORIGIN_SZTSSZ06      -1.041e+00  1.300e-02   -80.057  &lt; 2e-16 ***\nORIGIN_SZWCSZ01       3.218e-01  5.643e-03    57.033  &lt; 2e-16 ***\nORIGIN_SZWCSZ02      -2.578e+00  2.362e-02  -109.116  &lt; 2e-16 ***\nORIGIN_SZWCSZ03      -4.267e+00  1.155e-01   -36.940  &lt; 2e-16 ***\nORIGIN_SZWDSZ01       1.470e+00  3.430e-03   428.530  &lt; 2e-16 ***\nORIGIN_SZWDSZ02       1.159e+00  3.876e-03   299.057  &lt; 2e-16 ***\nORIGIN_SZWDSZ03       2.249e+00  3.332e-03   675.098  &lt; 2e-16 ***\nORIGIN_SZWDSZ04       1.225e+00  4.061e-03   301.621  &lt; 2e-16 ***\nORIGIN_SZWDSZ05       6.568e-01  4.076e-03   161.123  &lt; 2e-16 ***\nORIGIN_SZWDSZ06       1.316e+00  3.781e-03   348.077  &lt; 2e-16 ***\nORIGIN_SZWDSZ07       9.897e-02  5.807e-03    17.043  &lt; 2e-16 ***\nORIGIN_SZWDSZ08      -2.550e-01  6.309e-03   -40.414  &lt; 2e-16 ***\nORIGIN_SZWDSZ09       1.767e+00  3.515e-03   502.735  &lt; 2e-16 ***\nORIGIN_SZYSSZ01       2.239e-02  4.533e-03     4.940 7.80e-07 ***\nORIGIN_SZYSSZ02       1.026e+00  4.102e-03   250.200  &lt; 2e-16 ***\nORIGIN_SZYSSZ03       1.992e+00  3.445e-03   578.233  &lt; 2e-16 ***\nORIGIN_SZYSSZ04       9.793e-01  3.675e-03   266.488  &lt; 2e-16 ***\nORIGIN_SZYSSZ05       1.752e-01  4.583e-03    38.230  &lt; 2e-16 ***\nORIGIN_SZYSSZ06      -8.329e-01  7.646e-03  -108.937  &lt; 2e-16 ***\nORIGIN_SZYSSZ07      -6.547e-01  7.564e-03   -86.551  &lt; 2e-16 ***\nORIGIN_SZYSSZ08       2.995e-02  5.065e-03     5.913 3.37e-09 ***\nORIGIN_SZYSSZ09       1.425e+00  3.513e-03   405.785  &lt; 2e-16 ***\nlog(DESTIN_AGE25_64)  2.555e-02  6.516e-05   392.206  &lt; 2e-16 ***\nlog(dist)            -7.112e-01  1.006e-04 -7070.144  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 107870462  on 21078  degrees of freedom\nResidual deviance:  47272393  on 20767  degrees of freedom\nAIC: 47407255\n\nNumber of Fisher Scoring iterations: 7\n\n\n\nCalcRSquared(orcSIM$data$TRIPS, orcSIM$fitted.values)\n\n[1] 0.381021"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#destination-constrained",
    "href": "In-class_Ex3/In-class_Ex3.html#destination-constrained",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Destination constrained",
    "text": "Destination constrained\nThe general formula of Destination Constrained Spatial Interaction Model\n\\[\n\\lambda_{ij} = exp(k + \\mu lnV_i + \\alpha_j + \\beta ln d_{ij})\n\\]\nThis takes a while to run so be patient\n\n# Set eval = true when need to recompute\ndecSIM &lt;- glm(formula = TRIPS ~ \n                DESTIN_SZ + \n                log(ORIGIN_AGE25_64) + \n                log(dist),\n              family = poisson(link = \"log\"),\n              data = SIM_data,\n              na.action = na.exclude)\nwrite_rds(decSIM, \"data/rds/decSIM.rds\") # This is 52 MB file\n\n\ndecSIM &lt;-read_rds(\"data/rds/decSIM.rds\")\nsummary(decSIM)\n\n\nCall:\nglm(formula = TRIPS ~ DESTIN_SZ + log(ORIGIN_AGE25_64) + log(dist), \n    family = poisson(link = \"log\"), data = SIM_data, na.action = na.exclude)\n\nCoefficients:\n                       Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept)          11.1410882  0.0027772  4011.579  &lt; 2e-16 ***\nDESTIN_SZAMSZ02       0.2557910  0.0035112    72.849  &lt; 2e-16 ***\nDESTIN_SZAMSZ03       0.3942740  0.0033889   116.344  &lt; 2e-16 ***\nDESTIN_SZAMSZ04      -0.9146437  0.0051428  -177.851  &lt; 2e-16 ***\nDESTIN_SZAMSZ05      -1.0065173  0.0050124  -200.806  &lt; 2e-16 ***\nDESTIN_SZAMSZ06      -0.7854138  0.0048020  -163.560  &lt; 2e-16 ***\nDESTIN_SZAMSZ07      -1.7922909  0.0083032  -215.856  &lt; 2e-16 ***\nDESTIN_SZAMSZ08      -0.8768811  0.0052724  -166.316  &lt; 2e-16 ***\nDESTIN_SZAMSZ09      -0.9312194  0.0051073  -182.330  &lt; 2e-16 ***\nDESTIN_SZAMSZ10       0.4505467  0.0035376   127.360  &lt; 2e-16 ***\nDESTIN_SZAMSZ11       0.1890523  0.0059079    32.000  &lt; 2e-16 ***\nDESTIN_SZAMSZ12      -0.0604611  0.0043965   -13.752  &lt; 2e-16 ***\nDESTIN_SZBDSZ01       0.6054462  0.0031528   192.033  &lt; 2e-16 ***\nDESTIN_SZBDSZ02      -0.1435718  0.0040408   -35.530  &lt; 2e-16 ***\nDESTIN_SZBDSZ03       0.1182554  0.0035216    33.580  &lt; 2e-16 ***\nDESTIN_SZBDSZ04       1.0744574  0.0029056   369.788  &lt; 2e-16 ***\nDESTIN_SZBDSZ05       0.5188524  0.0032147   161.398  &lt; 2e-16 ***\nDESTIN_SZBDSZ06       0.2319272  0.0035570    65.203  &lt; 2e-16 ***\nDESTIN_SZBDSZ07      -1.0406382  0.0077629  -134.053  &lt; 2e-16 ***\nDESTIN_SZBDSZ08      -1.7004628  0.0087167  -195.081  &lt; 2e-16 ***\nDESTIN_SZBKSZ01      -1.0227576  0.0052976  -193.062  &lt; 2e-16 ***\nDESTIN_SZBKSZ02      -0.2292656  0.0044189   -51.883  &lt; 2e-16 ***\nDESTIN_SZBKSZ03      -0.3693980  0.0044297   -83.390  &lt; 2e-16 ***\nDESTIN_SZBKSZ04       0.1193148  0.0039610    30.123  &lt; 2e-16 ***\nDESTIN_SZBKSZ05      -0.6140193  0.0045147  -136.005  &lt; 2e-16 ***\nDESTIN_SZBKSZ06      -0.8077332  0.0050823  -158.931  &lt; 2e-16 ***\nDESTIN_SZBKSZ07       0.3204621  0.0033596    95.386  &lt; 2e-16 ***\nDESTIN_SZBKSZ08      -1.0587197  0.0056921  -185.997  &lt; 2e-16 ***\nDESTIN_SZBKSZ09      -0.1710273  0.0040114   -42.635  &lt; 2e-16 ***\nDESTIN_SZBLSZ01      -0.1716574  0.0055233   -31.079  &lt; 2e-16 ***\nDESTIN_SZBLSZ02       0.7748624  0.0053914   143.722  &lt; 2e-16 ***\nDESTIN_SZBLSZ03       1.5658660  0.0058875   265.965  &lt; 2e-16 ***\nDESTIN_SZBLSZ04       0.1720564  0.0104307    16.495  &lt; 2e-16 ***\nDESTIN_SZBMSZ01      -0.0932428  0.0036719   -25.394  &lt; 2e-16 ***\nDESTIN_SZBMSZ02      -0.4481271  0.0038953  -115.043  &lt; 2e-16 ***\nDESTIN_SZBMSZ03      -0.8353412  0.0046863  -178.250  &lt; 2e-16 ***\nDESTIN_SZBMSZ04      -0.5044261  0.0040142  -125.661  &lt; 2e-16 ***\nDESTIN_SZBMSZ05      -0.6064471  0.0049481  -122.561  &lt; 2e-16 ***\nDESTIN_SZBMSZ06      -1.9289578  0.0089309  -215.986  &lt; 2e-16 ***\nDESTIN_SZBMSZ07      -0.1215508  0.0035900   -33.858  &lt; 2e-16 ***\nDESTIN_SZBMSZ08      -1.1364431  0.0048569  -233.987  &lt; 2e-16 ***\nDESTIN_SZBMSZ09      -1.9481019  0.0074063  -263.031  &lt; 2e-16 ***\nDESTIN_SZBMSZ10      -1.5701680  0.0061711  -254.437  &lt; 2e-16 ***\nDESTIN_SZBMSZ11      -1.4324587  0.0057202  -250.420  &lt; 2e-16 ***\nDESTIN_SZBMSZ12      -0.9642301  0.0059452  -162.185  &lt; 2e-16 ***\nDESTIN_SZBMSZ13      -0.2102320  0.0038408   -54.736  &lt; 2e-16 ***\nDESTIN_SZBMSZ14      -1.0138679  0.0060453  -167.712  &lt; 2e-16 ***\nDESTIN_SZBMSZ15      -1.2276893  0.0057103  -214.994  &lt; 2e-16 ***\nDESTIN_SZBMSZ16      -1.5167958  0.0058527  -259.164  &lt; 2e-16 ***\nDESTIN_SZBMSZ17      -1.4557260  0.0070383  -206.828  &lt; 2e-16 ***\nDESTIN_SZBPSZ01      -0.5101095  0.0044678  -114.175  &lt; 2e-16 ***\nDESTIN_SZBPSZ02      -1.4610932  0.0070465  -207.350  &lt; 2e-16 ***\nDESTIN_SZBPSZ03      -1.1088582  0.0065318  -169.762  &lt; 2e-16 ***\nDESTIN_SZBPSZ04      -0.5694082  0.0047977  -118.685  &lt; 2e-16 ***\nDESTIN_SZBPSZ05       0.5121919  0.0032354   158.311  &lt; 2e-16 ***\nDESTIN_SZBPSZ06      -0.9324099  0.0064284  -145.045  &lt; 2e-16 ***\nDESTIN_SZBPSZ07      -0.4858707  0.0061650   -78.811  &lt; 2e-16 ***\nDESTIN_SZBSSZ01      -0.2014456  0.0037492   -53.731  &lt; 2e-16 ***\nDESTIN_SZBSSZ02      -0.7650851  0.0043491  -175.920  &lt; 2e-16 ***\nDESTIN_SZBSSZ03       0.3341571  0.0031673   105.501  &lt; 2e-16 ***\nDESTIN_SZBTSZ01       0.3248408  0.0033222    97.780  &lt; 2e-16 ***\nDESTIN_SZBTSZ02      -0.6003898  0.0052201  -115.016  &lt; 2e-16 ***\nDESTIN_SZBTSZ03       0.0609513  0.0037697    16.169  &lt; 2e-16 ***\nDESTIN_SZBTSZ04      -1.6207328  0.0080255  -201.947  &lt; 2e-16 ***\nDESTIN_SZBTSZ05      -0.8152297  0.0058112  -140.286  &lt; 2e-16 ***\nDESTIN_SZBTSZ06      -0.8646583  0.0050475  -171.305  &lt; 2e-16 ***\nDESTIN_SZBTSZ07      -1.8823303  0.0077719  -242.197  &lt; 2e-16 ***\nDESTIN_SZBTSZ08      -1.3177929  0.0069672  -189.142  &lt; 2e-16 ***\nDESTIN_SZCBSZ01      -4.8241179  0.3015219   -15.999  &lt; 2e-16 ***\nDESTIN_SZCCSZ01      -0.6939258  0.0058965  -117.685  &lt; 2e-16 ***\nDESTIN_SZCHSZ01      -0.9996470  0.0072272  -138.317  &lt; 2e-16 ***\nDESTIN_SZCHSZ02       0.0707234  0.0044595    15.859  &lt; 2e-16 ***\nDESTIN_SZCHSZ03       1.6864183  0.0031898   528.687  &lt; 2e-16 ***\nDESTIN_SZCKSZ01      -0.2182965  0.0041059   -53.167  &lt; 2e-16 ***\nDESTIN_SZCKSZ02      -0.4191666  0.0043847   -95.597  &lt; 2e-16 ***\nDESTIN_SZCKSZ03       0.7573985  0.0032111   235.870  &lt; 2e-16 ***\nDESTIN_SZCKSZ04      -0.7092606  0.0051730  -137.107  &lt; 2e-16 ***\nDESTIN_SZCKSZ05      -0.4268250  0.0056033   -76.174  &lt; 2e-16 ***\nDESTIN_SZCKSZ06       0.8266463  0.0037184   222.311  &lt; 2e-16 ***\nDESTIN_SZCLSZ01       0.5412025  0.0038632   140.092  &lt; 2e-16 ***\nDESTIN_SZCLSZ02      -2.2240933  0.0108450  -205.080  &lt; 2e-16 ***\nDESTIN_SZCLSZ03      -0.9122278  0.0061150  -149.178  &lt; 2e-16 ***\nDESTIN_SZCLSZ04       0.1257946  0.0035891    35.049  &lt; 2e-16 ***\nDESTIN_SZCLSZ05      -1.1720426  0.0067604  -173.368  &lt; 2e-16 ***\nDESTIN_SZCLSZ06       0.2397373  0.0033636    71.274  &lt; 2e-16 ***\nDESTIN_SZCLSZ07      -0.5905545  0.0043720  -135.077  &lt; 2e-16 ***\nDESTIN_SZCLSZ08      -0.4080784  0.0047930   -85.140  &lt; 2e-16 ***\nDESTIN_SZCLSZ09       0.4452463  0.0052671    84.533  &lt; 2e-16 ***\nDESTIN_SZDTSZ01      -0.7310478  0.0042890  -170.445  &lt; 2e-16 ***\nDESTIN_SZDTSZ02      -0.7964532  0.0042464  -187.561  &lt; 2e-16 ***\nDESTIN_SZDTSZ03      -1.0098416  0.0049261  -205.000  &lt; 2e-16 ***\nDESTIN_SZDTSZ04      -1.2108484  0.0110676  -109.404  &lt; 2e-16 ***\nDESTIN_SZDTSZ05      -1.1776465  0.0080031  -147.148  &lt; 2e-16 ***\nDESTIN_SZDTSZ06      -1.0064926  0.0052377  -192.164  &lt; 2e-16 ***\nDESTIN_SZDTSZ07      -1.9297373  0.0175979  -109.657  &lt; 2e-16 ***\nDESTIN_SZDTSZ08      -0.5761982  0.0040221  -143.258  &lt; 2e-16 ***\nDESTIN_SZDTSZ09      -1.4832177  0.0092718  -159.970  &lt; 2e-16 ***\nDESTIN_SZDTSZ10      -1.4979732  0.0078320  -191.263  &lt; 2e-16 ***\nDESTIN_SZDTSZ11      -0.6156417  0.0043013  -143.128  &lt; 2e-16 ***\nDESTIN_SZDTSZ12      -2.1682980  0.0129083  -167.977  &lt; 2e-16 ***\nDESTIN_SZDTSZ13      -1.8394504  0.0086458  -212.756  &lt; 2e-16 ***\nDESTIN_SZGLSZ01       0.0815050  0.0041206    19.780  &lt; 2e-16 ***\nDESTIN_SZGLSZ02      -0.2539348  0.0038109   -66.634  &lt; 2e-16 ***\nDESTIN_SZGLSZ03       0.4451898  0.0032052   138.898  &lt; 2e-16 ***\nDESTIN_SZGLSZ04       0.4426972  0.0031328   141.312  &lt; 2e-16 ***\nDESTIN_SZGLSZ05       0.2821448  0.0032396    87.093  &lt; 2e-16 ***\nDESTIN_SZHGSZ01       0.4616834  0.0031971   144.406  &lt; 2e-16 ***\nDESTIN_SZHGSZ02      -0.5466345  0.0043802  -124.796  &lt; 2e-16 ***\nDESTIN_SZHGSZ03      -1.1161776  0.0053520  -208.555  &lt; 2e-16 ***\nDESTIN_SZHGSZ04      -0.2557616  0.0037562   -68.090  &lt; 2e-16 ***\nDESTIN_SZHGSZ05      -0.1189317  0.0037386   -31.812  &lt; 2e-16 ***\nDESTIN_SZHGSZ06      -0.7439163  0.0045418  -163.794  &lt; 2e-16 ***\nDESTIN_SZHGSZ07       0.2224311  0.0034349    64.756  &lt; 2e-16 ***\nDESTIN_SZHGSZ08      -0.2507307  0.0039491   -63.491  &lt; 2e-16 ***\nDESTIN_SZHGSZ09       0.3431666  0.0041135    83.425  &lt; 2e-16 ***\nDESTIN_SZHGSZ10      -3.2527757  0.0278285  -116.887  &lt; 2e-16 ***\nDESTIN_SZJESZ01      -0.1177017  0.0041723   -28.210  &lt; 2e-16 ***\nDESTIN_SZJESZ02      -0.4453466  0.0043314  -102.817  &lt; 2e-16 ***\nDESTIN_SZJESZ03      -0.5117189  0.0046474  -110.109  &lt; 2e-16 ***\nDESTIN_SZJESZ04      -0.0863556  0.0050217   -17.196  &lt; 2e-16 ***\nDESTIN_SZJESZ05      -0.8807140  0.0076992  -114.391  &lt; 2e-16 ***\nDESTIN_SZJESZ06       0.4778151  0.0033735   141.638  &lt; 2e-16 ***\nDESTIN_SZJESZ07      -0.8444668  0.0060107  -140.493  &lt; 2e-16 ***\nDESTIN_SZJESZ08      -0.5016467  0.0063098   -79.503  &lt; 2e-16 ***\nDESTIN_SZJESZ09      -0.4379818  0.0046557   -94.074  &lt; 2e-16 ***\nDESTIN_SZJESZ10       0.7082718  0.0058424   121.230  &lt; 2e-16 ***\nDESTIN_SZJESZ11       0.9453462  0.0054406   173.759  &lt; 2e-16 ***\nDESTIN_SZJWSZ01      -0.3035883  0.0052309   -58.038  &lt; 2e-16 ***\nDESTIN_SZJWSZ02      -0.3028412  0.0044219   -68.487  &lt; 2e-16 ***\nDESTIN_SZJWSZ03       0.6912746  0.0033353   207.262  &lt; 2e-16 ***\nDESTIN_SZJWSZ04       1.0747916  0.0031061   346.022  &lt; 2e-16 ***\nDESTIN_SZJWSZ05      -0.1227583  0.0049735   -24.683  &lt; 2e-16 ***\nDESTIN_SZJWSZ06       0.4849237  0.0044513   108.939  &lt; 2e-16 ***\nDESTIN_SZJWSZ07      -0.9250006  0.0209277   -44.200  &lt; 2e-16 ***\nDESTIN_SZJWSZ08       0.4925645  0.0038032   129.512  &lt; 2e-16 ***\nDESTIN_SZJWSZ09       1.5444488  0.0028475   542.381  &lt; 2e-16 ***\nDESTIN_SZKLSZ01      -0.5660644  0.0040166  -140.932  &lt; 2e-16 ***\nDESTIN_SZKLSZ02      -0.7193809  0.0045690  -157.448  &lt; 2e-16 ***\nDESTIN_SZKLSZ03      -1.1860022  0.0050543  -234.651  &lt; 2e-16 ***\nDESTIN_SZKLSZ04      -1.6442159  0.0064182  -256.181  &lt; 2e-16 ***\nDESTIN_SZKLSZ05      -0.7215501  0.0056221  -128.342  &lt; 2e-16 ***\nDESTIN_SZKLSZ06      -0.8580631  0.0043659  -196.539  &lt; 2e-16 ***\nDESTIN_SZKLSZ07      -1.0290677  0.0049917  -206.155  &lt; 2e-16 ***\nDESTIN_SZKLSZ08      -0.0654458  0.0035962   -18.199  &lt; 2e-16 ***\nDESTIN_SZKLSZ09      -1.7941594  0.0064870  -276.578  &lt; 2e-16 ***\nDESTIN_SZLKSZ01      -1.5330310  0.0182228   -84.127  &lt; 2e-16 ***\nDESTIN_SZMDSZ01      -1.3631242  0.0152333   -89.483  &lt; 2e-16 ***\nDESTIN_SZMDSZ02      -1.0878945  0.0087183  -124.782  &lt; 2e-16 ***\nDESTIN_SZMDSZ03      -2.8395898  0.0192569  -147.458  &lt; 2e-16 ***\nDESTIN_SZMPSZ01      -0.9996139  0.0058493  -170.895  &lt; 2e-16 ***\nDESTIN_SZMPSZ02      -0.7893842  0.0047821  -165.069  &lt; 2e-16 ***\nDESTIN_SZMPSZ03      -0.0746966  0.0037806   -19.758  &lt; 2e-16 ***\nDESTIN_SZMSSZ01      -3.7645509  0.0711141   -52.937  &lt; 2e-16 ***\nDESTIN_SZMUSZ01      -1.1130223  0.0047914  -232.295  &lt; 2e-16 ***\nDESTIN_SZMUSZ02      -1.4200938  0.0069307  -204.898  &lt; 2e-16 ***\nDESTIN_SZMUSZ03      -1.0375226  0.0046101  -225.056  &lt; 2e-16 ***\nDESTIN_SZNTSZ01      -2.5204937  0.0197963  -127.322  &lt; 2e-16 ***\nDESTIN_SZNTSZ02      -2.1038832  0.0088508  -237.706  &lt; 2e-16 ***\nDESTIN_SZNTSZ03      -1.2953111  0.0061747  -209.779  &lt; 2e-16 ***\nDESTIN_SZNTSZ05      -1.6975407  0.0152792  -111.102  &lt; 2e-16 ***\nDESTIN_SZNTSZ06      -2.7387049  0.0248296  -110.300  &lt; 2e-16 ***\nDESTIN_SZNVSZ01      -0.2197281  0.0035429   -62.019  &lt; 2e-16 ***\nDESTIN_SZNVSZ02      -0.5038221  0.0040978  -122.949  &lt; 2e-16 ***\nDESTIN_SZNVSZ03      -0.6097426  0.0043454  -140.320  &lt; 2e-16 ***\nDESTIN_SZNVSZ04      -2.1768443  0.0088137  -246.983  &lt; 2e-16 ***\nDESTIN_SZNVSZ05      -1.8788605  0.0074956  -250.661  &lt; 2e-16 ***\nDESTIN_SZORSZ01      -1.7805694  0.0156980  -113.426  &lt; 2e-16 ***\nDESTIN_SZORSZ02      -0.0624431  0.0035972   -17.359  &lt; 2e-16 ***\nDESTIN_SZORSZ03      -0.8053448  0.0045194  -178.198  &lt; 2e-16 ***\nDESTIN_SZOTSZ01      -1.5313807  0.0060083  -254.876  &lt; 2e-16 ***\nDESTIN_SZOTSZ02      -0.7905666  0.0052145  -151.608  &lt; 2e-16 ***\nDESTIN_SZOTSZ03      -1.4244612  0.0055908  -254.786  &lt; 2e-16 ***\nDESTIN_SZOTSZ04      -1.5126835  0.0079353  -190.627  &lt; 2e-16 ***\nDESTIN_SZPGSZ01      -1.7383380  0.0122455  -141.957  &lt; 2e-16 ***\nDESTIN_SZPGSZ02      -0.7168389  0.0052936  -135.417  &lt; 2e-16 ***\nDESTIN_SZPGSZ03       0.5490926  0.0032816   167.327  &lt; 2e-16 ***\nDESTIN_SZPGSZ04       0.1733669  0.0036627    47.334  &lt; 2e-16 ***\nDESTIN_SZPGSZ05      -0.8968938  0.0060753  -147.629  &lt; 2e-16 ***\nDESTIN_SZPLSZ01       0.0313275  0.0052086     6.015 1.80e-09 ***\nDESTIN_SZPLSZ02      -1.1313930  0.0100427  -112.659  &lt; 2e-16 ***\nDESTIN_SZPLSZ03       0.0434166  0.0078653     5.520 3.39e-08 ***\nDESTIN_SZPLSZ04      -0.0495779  0.0074118    -6.689 2.25e-11 ***\nDESTIN_SZPLSZ05      -0.4640883  0.0090629   -51.207  &lt; 2e-16 ***\nDESTIN_SZPNSZ01       1.2045838  0.0041391   291.026  &lt; 2e-16 ***\nDESTIN_SZPNSZ02       1.7512371  0.0053846   325.234  &lt; 2e-16 ***\nDESTIN_SZPNSZ03       1.0757831  0.0060945   176.516  &lt; 2e-16 ***\nDESTIN_SZPNSZ04       1.8615721  0.0061205   304.153  &lt; 2e-16 ***\nDESTIN_SZPNSZ05       1.0742860  0.0086610   124.037  &lt; 2e-16 ***\nDESTIN_SZPRSZ01      -0.6224846  0.0060827  -102.337  &lt; 2e-16 ***\nDESTIN_SZPRSZ02      -0.0952483  0.0041134   -23.156  &lt; 2e-16 ***\nDESTIN_SZPRSZ03       0.7850760  0.0031505   249.192  &lt; 2e-16 ***\nDESTIN_SZPRSZ04      -0.6962574  0.0068133  -102.190  &lt; 2e-16 ***\nDESTIN_SZPRSZ05      -0.0461901  0.0039172   -11.792  &lt; 2e-16 ***\nDESTIN_SZPRSZ06       0.4961619  0.0040918   121.256  &lt; 2e-16 ***\nDESTIN_SZPRSZ07      -1.4513259  0.0097546  -148.784  &lt; 2e-16 ***\nDESTIN_SZPRSZ08      -0.7015024  0.0053776  -130.448  &lt; 2e-16 ***\nDESTIN_SZQTSZ01      -1.5333235  0.0080706  -189.989  &lt; 2e-16 ***\nDESTIN_SZQTSZ02      -1.4486407  0.0060325  -240.138  &lt; 2e-16 ***\nDESTIN_SZQTSZ03      -0.7813416  0.0051884  -150.595  &lt; 2e-16 ***\nDESTIN_SZQTSZ04      -1.1045790  0.0055093  -200.494  &lt; 2e-16 ***\nDESTIN_SZQTSZ05      -0.9768182  0.0049179  -198.626  &lt; 2e-16 ***\nDESTIN_SZQTSZ06      -1.1507833  0.0051462  -223.619  &lt; 2e-16 ***\nDESTIN_SZQTSZ07      -1.5672898  0.0084224  -186.086  &lt; 2e-16 ***\nDESTIN_SZQTSZ08       0.1015547  0.0037555    27.042  &lt; 2e-16 ***\nDESTIN_SZQTSZ09      -0.3697598  0.0045662   -80.978  &lt; 2e-16 ***\nDESTIN_SZQTSZ10      -0.4011342  0.0042859   -93.594  &lt; 2e-16 ***\nDESTIN_SZQTSZ11       0.2647289  0.0040109    66.002  &lt; 2e-16 ***\nDESTIN_SZQTSZ12      -0.2799756  0.0051396   -54.474  &lt; 2e-16 ***\nDESTIN_SZQTSZ13       0.1132740  0.0039640    28.576  &lt; 2e-16 ***\nDESTIN_SZQTSZ14      -0.0826046  0.0044267   -18.660  &lt; 2e-16 ***\nDESTIN_SZQTSZ15       0.0009792  0.0053379     0.183    0.854    \nDESTIN_SZRCSZ01      -0.9383329  0.0050183  -186.983  &lt; 2e-16 ***\nDESTIN_SZRCSZ02      -2.1739995  0.0132680  -163.852  &lt; 2e-16 ***\nDESTIN_SZRCSZ03      -1.0850298  0.0069631  -155.825  &lt; 2e-16 ***\nDESTIN_SZRCSZ04      -2.4810799  0.0103808  -239.008  &lt; 2e-16 ***\nDESTIN_SZRCSZ05      -2.2928951  0.0093577  -245.028  &lt; 2e-16 ***\nDESTIN_SZRCSZ06      -1.9480166  0.0113983  -170.904  &lt; 2e-16 ***\nDESTIN_SZRCSZ08      -2.1030164  0.0104174  -201.875  &lt; 2e-16 ***\nDESTIN_SZRCSZ09      -1.4595227  0.0090417  -161.422  &lt; 2e-16 ***\nDESTIN_SZRCSZ10      -1.1132535  0.0050022  -222.555  &lt; 2e-16 ***\nDESTIN_SZRVSZ01      -2.0419441  0.0084075  -242.872  &lt; 2e-16 ***\nDESTIN_SZRVSZ02      -2.1942670  0.0106208  -206.600  &lt; 2e-16 ***\nDESTIN_SZRVSZ03      -2.4290704  0.0100537  -241.611  &lt; 2e-16 ***\nDESTIN_SZRVSZ04      -2.0932370  0.0136112  -153.787  &lt; 2e-16 ***\nDESTIN_SZRVSZ05      -2.0853301  0.0115187  -181.038  &lt; 2e-16 ***\nDESTIN_SZSBSZ01      -0.0263033  0.0046850    -5.614 1.97e-08 ***\nDESTIN_SZSBSZ02      -0.8590224  0.0059907  -143.393  &lt; 2e-16 ***\nDESTIN_SZSBSZ03       0.7131890  0.0034251   208.223  &lt; 2e-16 ***\nDESTIN_SZSBSZ04       0.1923890  0.0043035    44.705  &lt; 2e-16 ***\nDESTIN_SZSBSZ05      -0.8147559  0.0058808  -138.545  &lt; 2e-16 ***\nDESTIN_SZSBSZ06      -2.3935314  0.0194619  -122.986  &lt; 2e-16 ***\nDESTIN_SZSBSZ07      -0.5722997  0.0143225   -39.958  &lt; 2e-16 ***\nDESTIN_SZSBSZ08       1.5683623  0.0041246   380.250  &lt; 2e-16 ***\nDESTIN_SZSBSZ09       0.9318417  0.0040652   229.226  &lt; 2e-16 ***\nDESTIN_SZSESZ02      -0.1326584  0.0038586   -34.380  &lt; 2e-16 ***\nDESTIN_SZSESZ03       0.7127808  0.0030634   232.678  &lt; 2e-16 ***\nDESTIN_SZSESZ04      -0.5533655  0.0044680  -123.851  &lt; 2e-16 ***\nDESTIN_SZSESZ05      -0.1122364  0.0037925   -29.594  &lt; 2e-16 ***\nDESTIN_SZSESZ06      -0.4466519  0.0046230   -96.615  &lt; 2e-16 ***\nDESTIN_SZSESZ07      -2.7468333  0.0180150  -152.475  &lt; 2e-16 ***\nDESTIN_SZSGSZ01      -0.3029221  0.0046409   -65.273  &lt; 2e-16 ***\nDESTIN_SZSGSZ02       0.1347783  0.0041858    32.199  &lt; 2e-16 ***\nDESTIN_SZSGSZ03      -0.2020929  0.0038669   -52.263  &lt; 2e-16 ***\nDESTIN_SZSGSZ04      -0.2097664  0.0038994   -53.795  &lt; 2e-16 ***\nDESTIN_SZSGSZ05      -1.9800270  0.0074638  -265.285  &lt; 2e-16 ***\nDESTIN_SZSGSZ06       0.4800476  0.0031165   154.036  &lt; 2e-16 ***\nDESTIN_SZSGSZ07      -0.3757532  0.0040596   -92.559  &lt; 2e-16 ***\nDESTIN_SZSISZ01      -1.3098094  0.0156233   -83.837  &lt; 2e-16 ***\nDESTIN_SZSKSZ01       0.0475755  0.0054682     8.700  &lt; 2e-16 ***\nDESTIN_SZSKSZ02       0.9315266  0.0040591   229.493  &lt; 2e-16 ***\nDESTIN_SZSKSZ03       0.0534964  0.0048827    10.956  &lt; 2e-16 ***\nDESTIN_SZSKSZ04      -0.5400391  0.0120767   -44.717  &lt; 2e-16 ***\nDESTIN_SZSKSZ05       0.1915550  0.0089868    21.315  &lt; 2e-16 ***\nDESTIN_SZSLSZ01      -0.3217850  0.0064644   -49.778  &lt; 2e-16 ***\nDESTIN_SZSLSZ04      -0.4431299  0.0052624   -84.207  &lt; 2e-16 ***\nDESTIN_SZSRSZ01      -1.5978643  0.0061388  -260.287  &lt; 2e-16 ***\nDESTIN_SZSRSZ02      -1.5915826  0.0073605  -216.234  &lt; 2e-16 ***\nDESTIN_SZSRSZ03      -1.4864486  0.0066144  -224.731  &lt; 2e-16 ***\nDESTIN_SZSVSZ01      -2.8882760  0.0411808   -70.137  &lt; 2e-16 ***\nDESTIN_SZTHSZ01      -2.9923350  0.0342129   -87.462  &lt; 2e-16 ***\nDESTIN_SZTHSZ03      -1.8922709  0.0170363  -111.073  &lt; 2e-16 ***\nDESTIN_SZTHSZ04      -2.2349279  0.0181834  -122.910  &lt; 2e-16 ***\nDESTIN_SZTHSZ06      -1.0747609  0.0095979  -111.979  &lt; 2e-16 ***\nDESTIN_SZTMSZ01       0.0408686  0.0042386     9.642  &lt; 2e-16 ***\nDESTIN_SZTMSZ02       1.7278433  0.0027471   628.973  &lt; 2e-16 ***\nDESTIN_SZTMSZ03       0.7873116  0.0031075   253.359  &lt; 2e-16 ***\nDESTIN_SZTMSZ04       0.8928932  0.0031754   281.192  &lt; 2e-16 ***\nDESTIN_SZTMSZ05       0.7400349  0.0039545   187.136  &lt; 2e-16 ***\nDESTIN_SZTNSZ01      -0.6870594  0.0044822  -153.288  &lt; 2e-16 ***\nDESTIN_SZTNSZ02      -1.3747410  0.0056164  -244.771  &lt; 2e-16 ***\nDESTIN_SZTNSZ03      -1.3980205  0.0068475  -204.165  &lt; 2e-16 ***\nDESTIN_SZTNSZ04      -1.0427422  0.0053344  -195.476  &lt; 2e-16 ***\nDESTIN_SZTPSZ01      -0.5463608  0.0044506  -122.761  &lt; 2e-16 ***\nDESTIN_SZTPSZ02       0.2420971  0.0031091    77.866  &lt; 2e-16 ***\nDESTIN_SZTPSZ03      -0.4535571  0.0044900  -101.016  &lt; 2e-16 ***\nDESTIN_SZTPSZ04      -1.4915297  0.0060259  -247.520  &lt; 2e-16 ***\nDESTIN_SZTPSZ05      -0.8888208  0.0047928  -185.448  &lt; 2e-16 ***\nDESTIN_SZTPSZ06      -0.2437218  0.0053143   -45.861  &lt; 2e-16 ***\nDESTIN_SZTPSZ07      -1.7251458  0.0087196  -197.847  &lt; 2e-16 ***\nDESTIN_SZTPSZ08      -1.3672687  0.0066542  -205.475  &lt; 2e-16 ***\nDESTIN_SZTPSZ09      -0.5402341  0.0048138  -112.225  &lt; 2e-16 ***\nDESTIN_SZTPSZ10      -0.7353788  0.0061864  -118.871  &lt; 2e-16 ***\nDESTIN_SZTPSZ11      -0.3379998  0.0040670   -83.107  &lt; 2e-16 ***\nDESTIN_SZTPSZ12      -0.8197097  0.0050090  -163.649  &lt; 2e-16 ***\nDESTIN_SZTSSZ01      -0.4158931  0.0170520   -24.390  &lt; 2e-16 ***\nDESTIN_SZTSSZ02       1.0895448  0.0072714   149.841  &lt; 2e-16 ***\nDESTIN_SZTSSZ03       1.7993092  0.0053831   334.252  &lt; 2e-16 ***\nDESTIN_SZTSSZ04       1.7075685  0.0054807   311.558  &lt; 2e-16 ***\nDESTIN_SZTSSZ05       1.8635850  0.0059695   312.185  &lt; 2e-16 ***\nDESTIN_SZTSSZ06       0.8701581  0.0098882    87.999  &lt; 2e-16 ***\nDESTIN_SZWCSZ01       1.6528704  0.0036371   454.442  &lt; 2e-16 ***\nDESTIN_SZWCSZ02       0.2096030  0.0078434    26.724  &lt; 2e-16 ***\nDESTIN_SZWCSZ03      -1.1702769  0.0221558   -52.820  &lt; 2e-16 ***\nDESTIN_SZWDSZ01       1.7222946  0.0028932   595.284  &lt; 2e-16 ***\nDESTIN_SZWDSZ02      -0.3265339  0.0048030   -67.986  &lt; 2e-16 ***\nDESTIN_SZWDSZ03       1.3196506  0.0030353   434.762  &lt; 2e-16 ***\nDESTIN_SZWDSZ04       0.1144794  0.0045018    25.429  &lt; 2e-16 ***\nDESTIN_SZWDSZ05       0.2512039  0.0042419    59.219  &lt; 2e-16 ***\nDESTIN_SZWDSZ06       0.5895357  0.0033892   173.945  &lt; 2e-16 ***\nDESTIN_SZWDSZ07       0.9440308  0.0047083   200.502  &lt; 2e-16 ***\nDESTIN_SZWDSZ08       0.8350312  0.0049445   168.882  &lt; 2e-16 ***\nDESTIN_SZWDSZ09       0.8051030  0.0036472   220.743  &lt; 2e-16 ***\nDESTIN_SZYSSZ01       1.2988936  0.0031119   417.391  &lt; 2e-16 ***\nDESTIN_SZYSSZ02       0.2630129  0.0041110    63.978  &lt; 2e-16 ***\nDESTIN_SZYSSZ03      -0.0402553  0.0043200    -9.318  &lt; 2e-16 ***\nDESTIN_SZYSSZ04       0.0730722  0.0040706    17.951  &lt; 2e-16 ***\nDESTIN_SZYSSZ05      -1.4876605  0.0082979  -179.281  &lt; 2e-16 ***\nDESTIN_SZYSSZ06      -1.3190335  0.0067242  -196.163  &lt; 2e-16 ***\nDESTIN_SZYSSZ07      -0.8848626  0.0080963  -109.293  &lt; 2e-16 ***\nDESTIN_SZYSSZ08       0.7305517  0.0032310   226.105  &lt; 2e-16 ***\nDESTIN_SZYSSZ09       0.4109047  0.0033189   123.807  &lt; 2e-16 ***\nlog(ORIGIN_AGE25_64)  0.2298031  0.0001026  2240.432  &lt; 2e-16 ***\nlog(dist)            -0.7121576  0.0001007 -7073.640  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 107870462  on 21078  degrees of freedom\nResidual deviance:  45794075  on 20766  degrees of freedom\nAIC: 45928939\n\nNumber of Fisher Scoring iterations: 7\n\n\n\nCalcRSquared(decSIM$data$TRIPS, decSIM$fitted.values)\n\n[1] 0.4828129"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#doubly-constrained",
    "href": "In-class_Ex3/In-class_Ex3.html#doubly-constrained",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Doubly constrained",
    "text": "Doubly constrained\nThe general formula of Doubly Constrained Spatial Interaction Model\n\\[\n\\lambda_{ij} = exp(k + \\mu_i + \\alpha_j + \\beta ln d_{ij})\n\\]\nThis code chunk takes a while to run (longer than previous ones) so be extrapatient. I’ll save thre results so no need to rerun it when I rered\n\n# Set eval: true when we need to recompute\ndbcSIM &lt;- glm(formula = TRIPS ~ \n                ORIGIN_SZ + \n                DESTIN_SZ + \n                log(dist),\n              family = poisson(link = \"log\"),\n              data = SIM_data,\n              na.action = na.exclude)\n# Save so we do not recompute again when rendering\nwrite_rds(dbcSIM, \"data/rds/dbcSIM.rds\") # 82MB file\n\n\ndbcSIM &lt;- read_rds(\"data/rds/dbcSIM.rds\")\nsummary(dbcSIM)\n\n\nCall:\nglm(formula = TRIPS ~ ORIGIN_SZ + DESTIN_SZ + log(dist), family = poisson(link = \"log\"), \n    data = SIM_data, na.action = na.exclude)\n\nCoefficients:\n                  Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept)     12.7762137  0.0036904  3461.985  &lt; 2e-16 ***\nORIGIN_SZAMSZ02  1.0613712  0.0037340   284.242  &lt; 2e-16 ***\nORIGIN_SZAMSZ03  0.6138256  0.0038090   161.150  &lt; 2e-16 ***\nORIGIN_SZAMSZ04  0.2015671  0.0042506    47.421  &lt; 2e-16 ***\nORIGIN_SZAMSZ05  0.0746673  0.0048845    15.287  &lt; 2e-16 ***\nORIGIN_SZAMSZ06  0.5810827  0.0044381   130.930  &lt; 2e-16 ***\nORIGIN_SZAMSZ07 -0.7674066  0.0073744  -104.064  &lt; 2e-16 ***\nORIGIN_SZAMSZ08 -0.6170493  0.0068455   -90.139  &lt; 2e-16 ***\nORIGIN_SZAMSZ09  0.4604832  0.0045938   100.241  &lt; 2e-16 ***\nORIGIN_SZAMSZ10  0.4723211  0.0040060   117.904  &lt; 2e-16 ***\nORIGIN_SZAMSZ11 -1.5642237  0.0091611  -170.746  &lt; 2e-16 ***\nORIGIN_SZAMSZ12 -1.6438166  0.0089443  -183.785  &lt; 2e-16 ***\nORIGIN_SZBDSZ01  0.8293184  0.0037271   222.508  &lt; 2e-16 ***\nORIGIN_SZBDSZ02  0.4359540  0.0042921   101.571  &lt; 2e-16 ***\nORIGIN_SZBDSZ03  0.8994728  0.0037968   236.901  &lt; 2e-16 ***\nORIGIN_SZBDSZ04  1.4540393  0.0033454   434.643  &lt; 2e-16 ***\nORIGIN_SZBDSZ05  0.6169243  0.0037524   164.407  &lt; 2e-16 ***\nORIGIN_SZBDSZ06  0.8550131  0.0038337   223.024  &lt; 2e-16 ***\nORIGIN_SZBDSZ07 -0.5763151  0.0063868   -90.235  &lt; 2e-16 ***\nORIGIN_SZBDSZ08 -0.4370618  0.0060178   -72.628  &lt; 2e-16 ***\nORIGIN_SZBKSZ01 -0.1496031  0.0053844   -27.784  &lt; 2e-16 ***\nORIGIN_SZBKSZ02  0.6037144  0.0044187   136.627  &lt; 2e-16 ***\nORIGIN_SZBKSZ03  0.9471518  0.0041494   228.261  &lt; 2e-16 ***\nORIGIN_SZBKSZ04 -0.0759537  0.0050404   -15.069  &lt; 2e-16 ***\nORIGIN_SZBKSZ05  0.1838346  0.0046847    39.241  &lt; 2e-16 ***\nORIGIN_SZBKSZ06  0.3430252  0.0049949    68.675  &lt; 2e-16 ***\nORIGIN_SZBKSZ07  0.8094643  0.0038028   212.862  &lt; 2e-16 ***\nORIGIN_SZBKSZ08  0.3296643  0.0044201    74.583  &lt; 2e-16 ***\nORIGIN_SZBKSZ09  0.0800666  0.0047045    17.019  &lt; 2e-16 ***\nORIGIN_SZBLSZ01 -1.7418178  0.0104800  -166.203  &lt; 2e-16 ***\nORIGIN_SZBLSZ02 -2.7444810  0.0147933  -185.521  &lt; 2e-16 ***\nORIGIN_SZBLSZ03 -5.0390908  0.0329205  -153.069  &lt; 2e-16 ***\nORIGIN_SZBLSZ04 -2.7508979  0.0192633  -142.805  &lt; 2e-16 ***\nORIGIN_SZBMSZ01  0.2838060  0.0040343    70.348  &lt; 2e-16 ***\nORIGIN_SZBMSZ02 -0.9582468  0.0057123  -167.753  &lt; 2e-16 ***\nORIGIN_SZBMSZ03 -0.0375031  0.0046027    -8.148 3.70e-16 ***\nORIGIN_SZBMSZ04  0.3610678  0.0041063    87.930  &lt; 2e-16 ***\nORIGIN_SZBMSZ05 -0.9398597  0.0059614  -157.657  &lt; 2e-16 ***\nORIGIN_SZBMSZ06 -1.2156711  0.0095685  -127.050  &lt; 2e-16 ***\nORIGIN_SZBMSZ07 -0.2469994  0.0045462   -54.331  &lt; 2e-16 ***\nORIGIN_SZBMSZ08 -0.1207828  0.0045330   -26.645  &lt; 2e-16 ***\nORIGIN_SZBMSZ09 -0.6967339  0.0058554  -118.990  &lt; 2e-16 ***\nORIGIN_SZBMSZ10 -0.9127748  0.0064361  -141.821  &lt; 2e-16 ***\nORIGIN_SZBMSZ11 -0.4477587  0.0053936   -83.017  &lt; 2e-16 ***\nORIGIN_SZBMSZ12 -0.7584160  0.0071339  -106.311  &lt; 2e-16 ***\nORIGIN_SZBMSZ13  0.0219772  0.0045624     4.817 1.46e-06 ***\nORIGIN_SZBMSZ14 -0.2303478  0.0053940   -42.704  &lt; 2e-16 ***\nORIGIN_SZBMSZ15  0.1298292  0.0048807    26.601  &lt; 2e-16 ***\nORIGIN_SZBMSZ16 -0.9530186  0.0061525  -154.899  &lt; 2e-16 ***\nORIGIN_SZBMSZ17 -1.3226877  0.0093030  -142.179  &lt; 2e-16 ***\nORIGIN_SZBPSZ01  0.5775036  0.0046028   125.467  &lt; 2e-16 ***\nORIGIN_SZBPSZ02  0.8368029  0.0051248   163.286  &lt; 2e-16 ***\nORIGIN_SZBPSZ03  1.0646965  0.0048189   220.942  &lt; 2e-16 ***\nORIGIN_SZBPSZ04  0.7836209  0.0041877   187.125  &lt; 2e-16 ***\nORIGIN_SZBPSZ05  0.5907260  0.0039113   151.032  &lt; 2e-16 ***\nORIGIN_SZBPSZ06 -0.9198393  0.0069884  -131.624  &lt; 2e-16 ***\nORIGIN_SZBPSZ07 -0.6986015  0.0066322  -105.335  &lt; 2e-16 ***\nORIGIN_SZBSSZ01  0.1449520  0.0043983    32.956  &lt; 2e-16 ***\nORIGIN_SZBSSZ02  0.5468233  0.0039498   138.444  &lt; 2e-16 ***\nORIGIN_SZBSSZ03  0.2401545  0.0039266    61.160  &lt; 2e-16 ***\nORIGIN_SZBTSZ01  0.0435107  0.0043522     9.997  &lt; 2e-16 ***\nORIGIN_SZBTSZ02 -0.8041310  0.0061107  -131.594  &lt; 2e-16 ***\nORIGIN_SZBTSZ03 -0.0269388  0.0045497    -5.921 3.20e-09 ***\nORIGIN_SZBTSZ04 -0.3898678  0.0079841   -48.830  &lt; 2e-16 ***\nORIGIN_SZBTSZ05 -1.2888510  0.0085409  -150.903  &lt; 2e-16 ***\nORIGIN_SZBTSZ06 -0.4743758  0.0057436   -82.592  &lt; 2e-16 ***\nORIGIN_SZBTSZ07 -1.6201702  0.0091853  -176.386  &lt; 2e-16 ***\nORIGIN_SZBTSZ08 -1.0120608  0.0074373  -136.079  &lt; 2e-16 ***\nORIGIN_SZCBSZ01 -2.5130438  0.0366145   -68.635  &lt; 2e-16 ***\nORIGIN_SZCCSZ01 -1.5479198  0.0110230  -140.426  &lt; 2e-16 ***\nORIGIN_SZCHSZ01 -1.2075754  0.0095668  -126.226  &lt; 2e-16 ***\nORIGIN_SZCHSZ02 -0.8809258  0.0068110  -129.338  &lt; 2e-16 ***\nORIGIN_SZCHSZ03 -0.4220566  0.0048757   -86.564  &lt; 2e-16 ***\nORIGIN_SZCKSZ01  0.5096981  0.0042139   120.956  &lt; 2e-16 ***\nORIGIN_SZCKSZ02  1.0681715  0.0043571   245.156  &lt; 2e-16 ***\nORIGIN_SZCKSZ03  0.8736769  0.0039169   223.056  &lt; 2e-16 ***\nORIGIN_SZCKSZ04  1.8686285  0.0040664   459.529  &lt; 2e-16 ***\nORIGIN_SZCKSZ05  1.6098567  0.0050104   321.302  &lt; 2e-16 ***\nORIGIN_SZCKSZ06  1.1819260  0.0053980   218.957  &lt; 2e-16 ***\nORIGIN_SZCLSZ01 -0.5993487  0.0058387  -102.651  &lt; 2e-16 ***\nORIGIN_SZCLSZ02 -1.1160165  0.0105581  -105.702  &lt; 2e-16 ***\nORIGIN_SZCLSZ03 -0.4850603  0.0058862   -82.407  &lt; 2e-16 ***\nORIGIN_SZCLSZ04  0.9638542  0.0037219   258.965  &lt; 2e-16 ***\nORIGIN_SZCLSZ05 -1.6253340  0.0106467  -152.660  &lt; 2e-16 ***\nORIGIN_SZCLSZ06  0.9394801  0.0035750   262.791  &lt; 2e-16 ***\nORIGIN_SZCLSZ07 -0.0244913  0.0044813    -5.465 4.62e-08 ***\nORIGIN_SZCLSZ08  0.2847792  0.0051565    55.227  &lt; 2e-16 ***\nORIGIN_SZCLSZ09 -2.2284995  0.0137181  -162.450  &lt; 2e-16 ***\nORIGIN_SZDTSZ01 -1.3290105  0.0069184  -192.099  &lt; 2e-16 ***\nORIGIN_SZDTSZ02 -1.2100118  0.0062646  -193.150  &lt; 2e-16 ***\nORIGIN_SZDTSZ03 -2.1417804  0.0106967  -200.229  &lt; 2e-16 ***\nORIGIN_SZDTSZ04 -3.6002445  0.0963012   -37.385  &lt; 2e-16 ***\nORIGIN_SZDTSZ05 -2.6750218  0.0192809  -138.739  &lt; 2e-16 ***\nORIGIN_SZDTSZ06 -2.7341262  0.0158702  -172.281  &lt; 2e-16 ***\nORIGIN_SZDTSZ07 -1.4844927  0.0198692   -74.713  &lt; 2e-16 ***\nORIGIN_SZDTSZ08 -2.1537294  0.0099160  -217.198  &lt; 2e-16 ***\nORIGIN_SZDTSZ09 -2.5870080  0.0195157  -132.560  &lt; 2e-16 ***\nORIGIN_SZDTSZ10 -1.8813307  0.0107730  -174.633  &lt; 2e-16 ***\nORIGIN_SZDTSZ11 -1.9626701  0.0102594  -191.304  &lt; 2e-16 ***\nORIGIN_SZDTSZ12 -2.9403937  0.0224233  -131.131  &lt; 2e-16 ***\nORIGIN_SZDTSZ13 -1.9850507  0.0118057  -168.143  &lt; 2e-16 ***\nORIGIN_SZGLSZ01 -1.3444205  0.0071257  -188.672  &lt; 2e-16 ***\nORIGIN_SZGLSZ02  0.2943216  0.0040886    71.985  &lt; 2e-16 ***\nORIGIN_SZGLSZ03  0.1120475  0.0040727    27.512  &lt; 2e-16 ***\nORIGIN_SZGLSZ04  1.0076191  0.0034727   290.152  &lt; 2e-16 ***\nORIGIN_SZGLSZ05  0.6507498  0.0036386   178.845  &lt; 2e-16 ***\nORIGIN_SZHGSZ01  0.1934308  0.0040425    47.850  &lt; 2e-16 ***\nORIGIN_SZHGSZ02  0.7061993  0.0039062   180.790  &lt; 2e-16 ***\nORIGIN_SZHGSZ03  0.4607193  0.0042602   108.145  &lt; 2e-16 ***\nORIGIN_SZHGSZ04  1.0138817  0.0036303   279.283  &lt; 2e-16 ***\nORIGIN_SZHGSZ05  1.3196005  0.0035870   367.884  &lt; 2e-16 ***\nORIGIN_SZHGSZ06  0.1697308  0.0043960    38.610  &lt; 2e-16 ***\nORIGIN_SZHGSZ07  0.6568400  0.0037863   173.480  &lt; 2e-16 ***\nORIGIN_SZHGSZ08  0.2033478  0.0043213    47.057  &lt; 2e-16 ***\nORIGIN_SZHGSZ09 -0.6928230  0.0058625  -118.179  &lt; 2e-16 ***\nORIGIN_SZHGSZ10 -3.2062481  0.0398679   -80.422  &lt; 2e-16 ***\nORIGIN_SZJESZ01  0.5328330  0.0042352   125.810  &lt; 2e-16 ***\nORIGIN_SZJESZ02  0.4223881  0.0042167   100.171  &lt; 2e-16 ***\nORIGIN_SZJESZ03  0.3837628  0.0044639    85.970  &lt; 2e-16 ***\nORIGIN_SZJESZ04 -1.0243932  0.0072468  -141.359  &lt; 2e-16 ***\nORIGIN_SZJESZ05 -1.9791823  0.0114449  -172.931  &lt; 2e-16 ***\nORIGIN_SZJESZ06  0.2291274  0.0041423    55.314  &lt; 2e-16 ***\nORIGIN_SZJESZ07 -1.5672736  0.0089515  -175.085  &lt; 2e-16 ***\nORIGIN_SZJESZ08 -0.4439991  0.0081901   -54.212  &lt; 2e-16 ***\nORIGIN_SZJESZ09  0.5470525  0.0043287   126.377  &lt; 2e-16 ***\nORIGIN_SZJESZ10 -2.5883909  0.0165482  -156.415  &lt; 2e-16 ***\nORIGIN_SZJESZ11 -2.9419898  0.0172092  -170.955  &lt; 2e-16 ***\nORIGIN_SZJWSZ01  0.5316998  0.0056149    94.694  &lt; 2e-16 ***\nORIGIN_SZJWSZ02  0.9948443  0.0039223   253.637  &lt; 2e-16 ***\nORIGIN_SZJWSZ03  1.2375047  0.0037200   332.665  &lt; 2e-16 ***\nORIGIN_SZJWSZ04  0.9971958  0.0038278   260.514  &lt; 2e-16 ***\nORIGIN_SZJWSZ05 -1.7086782  0.0104955  -162.802  &lt; 2e-16 ***\nORIGIN_SZJWSZ06 -1.1623182  0.0086215  -134.817  &lt; 2e-16 ***\nORIGIN_SZJWSZ07 -2.0304381  0.0197845  -102.628  &lt; 2e-16 ***\nORIGIN_SZJWSZ08  2.0400726  0.0037462   544.571  &lt; 2e-16 ***\nORIGIN_SZJWSZ09  1.4614167  0.0034849   419.353  &lt; 2e-16 ***\nORIGIN_SZKLSZ01  0.3534249  0.0039546    89.370  &lt; 2e-16 ***\nORIGIN_SZKLSZ02 -0.2375841  0.0048534   -48.952  &lt; 2e-16 ***\nORIGIN_SZKLSZ03 -0.2288697  0.0048681   -47.014  &lt; 2e-16 ***\nORIGIN_SZKLSZ04 -1.2689575  0.0068734  -184.618  &lt; 2e-16 ***\nORIGIN_SZKLSZ05 -0.2325427  0.0058144   -39.994  &lt; 2e-16 ***\nORIGIN_SZKLSZ06 -0.2587476  0.0045747   -56.561  &lt; 2e-16 ***\nORIGIN_SZKLSZ07 -0.6567093  0.0060098  -109.273  &lt; 2e-16 ***\nORIGIN_SZKLSZ08 -0.6576123  0.0052140  -126.125  &lt; 2e-16 ***\nORIGIN_SZKLSZ09 -1.1318963  0.0064934  -174.315  &lt; 2e-16 ***\nORIGIN_SZLKSZ01 -2.4682747  0.0299997   -82.277  &lt; 2e-16 ***\nORIGIN_SZMDSZ01 -1.3926591  0.0222908   -62.477  &lt; 2e-16 ***\nORIGIN_SZMDSZ02 -0.9046267  0.0094453   -95.775  &lt; 2e-16 ***\nORIGIN_SZMDSZ03 -1.2303046  0.0130774   -94.079  &lt; 2e-16 ***\nORIGIN_SZMPSZ01 -0.5397192  0.0057838   -93.315  &lt; 2e-16 ***\nORIGIN_SZMPSZ02 -0.3418827  0.0052399   -65.246  &lt; 2e-16 ***\nORIGIN_SZMPSZ03  0.4494199  0.0040005   112.342  &lt; 2e-16 ***\nORIGIN_SZMSSZ01 -7.2751444  0.3811976   -19.085  &lt; 2e-16 ***\nORIGIN_SZMUSZ01 -0.7886457  0.0055460  -142.201  &lt; 2e-16 ***\nORIGIN_SZMUSZ02 -2.7507048  0.0146878  -187.278  &lt; 2e-16 ***\nORIGIN_SZMUSZ03 -1.3888219  0.0065207  -212.986  &lt; 2e-16 ***\nORIGIN_SZNTSZ01 -2.3132628  0.0249833   -92.592  &lt; 2e-16 ***\nORIGIN_SZNTSZ02 -2.1648169  0.0131248  -164.941  &lt; 2e-16 ***\nORIGIN_SZNTSZ03 -0.5393612  0.0060662   -88.913  &lt; 2e-16 ***\nORIGIN_SZNTSZ05 -2.8187700  0.0377745   -74.621  &lt; 2e-16 ***\nORIGIN_SZNTSZ06 -2.8773282  0.0372394   -77.266  &lt; 2e-16 ***\nORIGIN_SZNVSZ01  0.8600924  0.0036205   237.563  &lt; 2e-16 ***\nORIGIN_SZNVSZ02 -0.2693976  0.0048695   -55.324  &lt; 2e-16 ***\nORIGIN_SZNVSZ03 -0.9483669  0.0060139  -157.696  &lt; 2e-16 ***\nORIGIN_SZNVSZ04 -1.0655908  0.0071727  -148.562  &lt; 2e-16 ***\nORIGIN_SZNVSZ05 -2.3434382  0.0130068  -180.170  &lt; 2e-16 ***\nORIGIN_SZORSZ01 -2.9424266  0.0314453   -93.573  &lt; 2e-16 ***\nORIGIN_SZORSZ02 -0.9787090  0.0056855  -172.142  &lt; 2e-16 ***\nORIGIN_SZORSZ03 -1.2448593  0.0065805  -189.173  &lt; 2e-16 ***\nORIGIN_SZOTSZ01 -1.3707221  0.0072042  -190.267  &lt; 2e-16 ***\nORIGIN_SZOTSZ02 -1.5432832  0.0082360  -187.383  &lt; 2e-16 ***\nORIGIN_SZOTSZ03 -0.3884079  0.0052469   -74.026  &lt; 2e-16 ***\nORIGIN_SZOTSZ04 -0.4537754  0.0082764   -54.828  &lt; 2e-16 ***\nORIGIN_SZPGSZ01  0.4190939  0.0117091    35.792  &lt; 2e-16 ***\nORIGIN_SZPGSZ02 -0.2245270  0.0057566   -39.003  &lt; 2e-16 ***\nORIGIN_SZPGSZ03  0.9693079  0.0037614   257.701  &lt; 2e-16 ***\nORIGIN_SZPGSZ04  1.1818495  0.0037319   316.693  &lt; 2e-16 ***\nORIGIN_SZPGSZ05  0.5936403  0.0047466   125.065  &lt; 2e-16 ***\nORIGIN_SZPLSZ01 -0.6436762  0.0081143   -79.326  &lt; 2e-16 ***\nORIGIN_SZPLSZ02 -1.1354083  0.0114675   -99.011  &lt; 2e-16 ***\nORIGIN_SZPLSZ03 -3.2947075  0.0311101  -105.905  &lt; 2e-16 ***\nORIGIN_SZPLSZ04 -3.3473399  0.0278363  -120.251  &lt; 2e-16 ***\nORIGIN_SZPLSZ05 -2.3429244  0.0180209  -130.012  &lt; 2e-16 ***\nORIGIN_SZPNSZ01  0.9303976  0.0045852   202.915  &lt; 2e-16 ***\nORIGIN_SZPNSZ02 -1.9054972  0.0110752  -172.051  &lt; 2e-16 ***\nORIGIN_SZPNSZ03 -2.6540255  0.0164558  -161.282  &lt; 2e-16 ***\nORIGIN_SZPNSZ04 -4.8107003  0.0258674  -185.975  &lt; 2e-16 ***\nORIGIN_SZPNSZ05 -3.6034740  0.0214652  -167.875  &lt; 2e-16 ***\nORIGIN_SZPRSZ01 -0.6681473  0.0094792   -70.485  &lt; 2e-16 ***\nORIGIN_SZPRSZ02  1.0129353  0.0039186   258.495  &lt; 2e-16 ***\nORIGIN_SZPRSZ03  0.4900451  0.0039274   124.777  &lt; 2e-16 ***\nORIGIN_SZPRSZ04 -0.3466709  0.0062240   -55.699  &lt; 2e-16 ***\nORIGIN_SZPRSZ05  1.1754545  0.0037249   315.563  &lt; 2e-16 ***\nORIGIN_SZPRSZ06 -0.9175714  0.0068924  -133.129  &lt; 2e-16 ***\nORIGIN_SZPRSZ07 -2.5368497  0.0163834  -154.843  &lt; 2e-16 ***\nORIGIN_SZPRSZ08  0.0729474  0.0050652    14.402  &lt; 2e-16 ***\nORIGIN_SZQTSZ01  0.1555824  0.0054276    28.665  &lt; 2e-16 ***\nORIGIN_SZQTSZ02 -0.3822720  0.0050397   -75.852  &lt; 2e-16 ***\nORIGIN_SZQTSZ03  0.1554544  0.0046402    33.502  &lt; 2e-16 ***\nORIGIN_SZQTSZ04 -0.9342678  0.0062487  -149.515  &lt; 2e-16 ***\nORIGIN_SZQTSZ05  0.1309227  0.0046404    28.213  &lt; 2e-16 ***\nORIGIN_SZQTSZ06 -0.2257612  0.0052682   -42.854  &lt; 2e-16 ***\nORIGIN_SZQTSZ07 -1.2142202  0.0077352  -156.973  &lt; 2e-16 ***\nORIGIN_SZQTSZ08 -0.1650278  0.0048069   -34.332  &lt; 2e-16 ***\nORIGIN_SZQTSZ09 -0.4258736  0.0053674   -79.344  &lt; 2e-16 ***\nORIGIN_SZQTSZ10 -0.3352035  0.0053090   -63.139  &lt; 2e-16 ***\nORIGIN_SZQTSZ11 -1.4013793  0.0075778  -184.932  &lt; 2e-16 ***\nORIGIN_SZQTSZ12 -0.9972389  0.0074068  -134.639  &lt; 2e-16 ***\nORIGIN_SZQTSZ13 -0.1967334  0.0049213   -39.976  &lt; 2e-16 ***\nORIGIN_SZQTSZ14 -1.3716454  0.0072556  -189.046  &lt; 2e-16 ***\nORIGIN_SZQTSZ15 -1.0734616  0.0088673  -121.058  &lt; 2e-16 ***\nORIGIN_SZRCSZ01 -0.3118246  0.0052166   -59.776  &lt; 2e-16 ***\nORIGIN_SZRCSZ02 -1.8209593  0.0145321  -125.306  &lt; 2e-16 ***\nORIGIN_SZRCSZ03 -0.8606245  0.0071598  -120.202  &lt; 2e-16 ***\nORIGIN_SZRCSZ04 -1.8956537  0.0105087  -180.389  &lt; 2e-16 ***\nORIGIN_SZRCSZ05 -2.0018155  0.0115207  -173.758  &lt; 2e-16 ***\nORIGIN_SZRCSZ06 -0.1540624  0.0070074   -21.986  &lt; 2e-16 ***\nORIGIN_SZRCSZ08 -2.2111335  0.0150941  -146.490  &lt; 2e-16 ***\nORIGIN_SZRCSZ09 -1.6705949  0.0121700  -137.271  &lt; 2e-16 ***\nORIGIN_SZRCSZ10 -1.4039876  0.0068064  -206.274  &lt; 2e-16 ***\nORIGIN_SZRVSZ01 -2.2959792  0.0119650  -191.891  &lt; 2e-16 ***\nORIGIN_SZRVSZ02 -0.6149302  0.0067748   -90.767  &lt; 2e-16 ***\nORIGIN_SZRVSZ03 -1.2970378  0.0096484  -134.431  &lt; 2e-16 ***\nORIGIN_SZRVSZ04 -1.5069632  0.0140984  -106.889  &lt; 2e-16 ***\nORIGIN_SZRVSZ05 -1.5616063  0.0121901  -128.105  &lt; 2e-16 ***\nORIGIN_SZSBSZ01  0.8597200  0.0049912   172.246  &lt; 2e-16 ***\nORIGIN_SZSBSZ02 -0.6353564  0.0065525   -96.963  &lt; 2e-16 ***\nORIGIN_SZSBSZ03  0.6117473  0.0041099   148.847  &lt; 2e-16 ***\nORIGIN_SZSBSZ04  0.4683425  0.0047405    98.796  &lt; 2e-16 ***\nORIGIN_SZSBSZ05 -0.0451646  0.0056922    -7.934 2.12e-15 ***\nORIGIN_SZSBSZ06 -1.0535456  0.0143390   -73.474  &lt; 2e-16 ***\nORIGIN_SZSBSZ07 -0.1368624  0.0098544   -13.888  &lt; 2e-16 ***\nORIGIN_SZSBSZ08 -1.9778267  0.0097033  -203.831  &lt; 2e-16 ***\nORIGIN_SZSBSZ09 -1.0945021  0.0072619  -150.719  &lt; 2e-16 ***\nORIGIN_SZSESZ02  1.2002737  0.0037019   324.233  &lt; 2e-16 ***\nORIGIN_SZSESZ03  1.1252145  0.0035349   318.320  &lt; 2e-16 ***\nORIGIN_SZSESZ04  1.1579952  0.0040703   284.497  &lt; 2e-16 ***\nORIGIN_SZSESZ05 -0.1233168  0.0048752   -25.295  &lt; 2e-16 ***\nORIGIN_SZSESZ06  1.1296884  0.0038719   291.768  &lt; 2e-16 ***\nORIGIN_SZSESZ07 -1.9733903  0.0141617  -139.347  &lt; 2e-16 ***\nORIGIN_SZSGSZ01 -0.8346869  0.0069949  -119.327  &lt; 2e-16 ***\nORIGIN_SZSGSZ02 -1.1927328  0.0081869  -145.688  &lt; 2e-16 ***\nORIGIN_SZSGSZ03  0.3496020  0.0043188    80.948  &lt; 2e-16 ***\nORIGIN_SZSGSZ04  0.4336720  0.0039788   108.997  &lt; 2e-16 ***\nORIGIN_SZSGSZ05 -1.4201793  0.0081408  -174.452  &lt; 2e-16 ***\nORIGIN_SZSGSZ06  0.3975356  0.0038009   104.591  &lt; 2e-16 ***\nORIGIN_SZSGSZ07 -0.4001257  0.0049530   -80.785  &lt; 2e-16 ***\nORIGIN_SZSKSZ01 -0.1672307  0.0066837   -25.021  &lt; 2e-16 ***\nORIGIN_SZSKSZ02  0.2258282  0.0050667    44.571  &lt; 2e-16 ***\nORIGIN_SZSKSZ03 -0.4428259  0.0064263   -68.908  &lt; 2e-16 ***\nORIGIN_SZSKSZ04 -1.9678400  0.0217531   -90.463  &lt; 2e-16 ***\nORIGIN_SZSKSZ05 -1.2594037  0.0145894   -86.323  &lt; 2e-16 ***\nORIGIN_SZSLSZ01 -2.8310988  0.0240771  -117.585  &lt; 2e-16 ***\nORIGIN_SZSLSZ04 -0.2939469  0.0058522   -50.229  &lt; 2e-16 ***\nORIGIN_SZSRSZ01 -1.1190117  0.0071113  -157.357  &lt; 2e-16 ***\nORIGIN_SZSRSZ02 -1.4068290  0.0071421  -196.977  &lt; 2e-16 ***\nORIGIN_SZSRSZ03 -2.2813969  0.0147491  -154.681  &lt; 2e-16 ***\nORIGIN_SZSVSZ01 -2.8880836  0.0546834   -52.815  &lt; 2e-16 ***\nORIGIN_SZTHSZ01 -2.3668153  0.0373678   -63.338  &lt; 2e-16 ***\nORIGIN_SZTHSZ03 -1.0165316  0.0146094   -69.581  &lt; 2e-16 ***\nORIGIN_SZTHSZ04 -2.2224809  0.0239246   -92.895  &lt; 2e-16 ***\nORIGIN_SZTHSZ06 -1.7681836  0.0142588  -124.006  &lt; 2e-16 ***\nORIGIN_SZTMSZ01  0.7025950  0.0043391   161.923  &lt; 2e-16 ***\nORIGIN_SZTMSZ02  1.6324769  0.0032929   495.764  &lt; 2e-16 ***\nORIGIN_SZTMSZ03  1.1842953  0.0035077   337.625  &lt; 2e-16 ***\nORIGIN_SZTMSZ04  0.4037194  0.0040717    99.152  &lt; 2e-16 ***\nORIGIN_SZTMSZ05 -0.8454661  0.0063323  -133.517  &lt; 2e-16 ***\nORIGIN_SZTNSZ01 -0.8914013  0.0061431  -145.105  &lt; 2e-16 ***\nORIGIN_SZTNSZ02 -0.7032036  0.0057796  -121.671  &lt; 2e-16 ***\nORIGIN_SZTNSZ03 -1.1756049  0.0076009  -154.667  &lt; 2e-16 ***\nORIGIN_SZTNSZ04 -0.3320635  0.0056023   -59.273  &lt; 2e-16 ***\nORIGIN_SZTPSZ01 -0.4329595  0.0051765   -83.639  &lt; 2e-16 ***\nORIGIN_SZTPSZ02  0.5543129  0.0036477   151.964  &lt; 2e-16 ***\nORIGIN_SZTPSZ03 -0.3808213  0.0050968   -74.718  &lt; 2e-16 ***\nORIGIN_SZTPSZ04  0.0207226  0.0047652     4.349 1.37e-05 ***\nORIGIN_SZTPSZ05  0.1534198  0.0049889    30.752  &lt; 2e-16 ***\nORIGIN_SZTPSZ06  0.6617769  0.0056062   118.043  &lt; 2e-16 ***\nORIGIN_SZTPSZ07  0.0702685  0.0051105    13.750  &lt; 2e-16 ***\nORIGIN_SZTPSZ08 -0.3841251  0.0068424   -56.139  &lt; 2e-16 ***\nORIGIN_SZTPSZ09 -0.3589040  0.0052508   -68.353  &lt; 2e-16 ***\nORIGIN_SZTPSZ10 -0.2841897  0.0057647   -49.299  &lt; 2e-16 ***\nORIGIN_SZTPSZ11  0.3628775  0.0042980    84.430  &lt; 2e-16 ***\nORIGIN_SZTPSZ12 -0.3870331  0.0053001   -73.023  &lt; 2e-16 ***\nORIGIN_SZTSSZ01 -3.3154334  0.0379468   -87.371  &lt; 2e-16 ***\nORIGIN_SZTSSZ02  0.0806767  0.0077143    10.458  &lt; 2e-16 ***\nORIGIN_SZTSSZ03 -0.1934922  0.0077972   -24.816  &lt; 2e-16 ***\nORIGIN_SZTSSZ04 -0.5128461  0.0080874   -63.413  &lt; 2e-16 ***\nORIGIN_SZTSSZ05 -2.7358286  0.0133134  -205.495  &lt; 2e-16 ***\nORIGIN_SZTSSZ06 -2.7997982  0.0190125  -147.261  &lt; 2e-16 ***\nORIGIN_SZWCSZ01 -0.8131149  0.0064097  -126.856  &lt; 2e-16 ***\nORIGIN_SZWCSZ02 -2.6613423  0.0239483  -111.129  &lt; 2e-16 ***\nORIGIN_SZWCSZ03 -4.5082871  0.1155229   -39.025  &lt; 2e-16 ***\nORIGIN_SZWDSZ01  0.8911247  0.0036053   247.172  &lt; 2e-16 ***\nORIGIN_SZWDSZ02  1.0387189  0.0040854   254.252  &lt; 2e-16 ***\nORIGIN_SZWDSZ03  1.7104108  0.0036962   462.753  &lt; 2e-16 ***\nORIGIN_SZWDSZ04  1.2090220  0.0044533   271.491  &lt; 2e-16 ***\nORIGIN_SZWDSZ05  0.5162706  0.0043060   119.896  &lt; 2e-16 ***\nORIGIN_SZWDSZ06  1.0045245  0.0040576   247.564  &lt; 2e-16 ***\nORIGIN_SZWDSZ07 -0.2866857  0.0062002   -46.238  &lt; 2e-16 ***\nORIGIN_SZWDSZ08 -0.7548912  0.0066389  -113.708  &lt; 2e-16 ***\nORIGIN_SZWDSZ09  1.6190137  0.0038620   419.215  &lt; 2e-16 ***\nORIGIN_SZYSSZ01 -0.3989663  0.0047009   -84.870  &lt; 2e-16 ***\nORIGIN_SZYSSZ02  1.0280266  0.0044633   230.328  &lt; 2e-16 ***\nORIGIN_SZYSSZ03  2.2856940  0.0037115   615.837  &lt; 2e-16 ***\nORIGIN_SZYSSZ04  0.9458162  0.0038085   248.346  &lt; 2e-16 ***\nORIGIN_SZYSSZ05  0.3848719  0.0046724    82.372  &lt; 2e-16 ***\nORIGIN_SZYSSZ06 -0.5454746  0.0077588   -70.304  &lt; 2e-16 ***\nORIGIN_SZYSSZ07 -0.4998069  0.0081011   -61.696  &lt; 2e-16 ***\nORIGIN_SZYSSZ08 -0.3444412  0.0052562   -65.530  &lt; 2e-16 ***\nORIGIN_SZYSSZ09  1.2613656  0.0036958   341.295  &lt; 2e-16 ***\nDESTIN_SZAMSZ02 -0.0090399  0.0036254    -2.494 0.012648 *  \nDESTIN_SZAMSZ03  0.2512895  0.0034768    72.277  &lt; 2e-16 ***\nDESTIN_SZAMSZ04 -0.9253731  0.0051982  -178.018  &lt; 2e-16 ***\nDESTIN_SZAMSZ05 -0.9173432  0.0050746  -180.773  &lt; 2e-16 ***\nDESTIN_SZAMSZ06 -0.7664811  0.0049152  -155.942  &lt; 2e-16 ***\nDESTIN_SZAMSZ07 -1.5920761  0.0083692  -190.230  &lt; 2e-16 ***\nDESTIN_SZAMSZ08 -0.7715751  0.0053565  -144.045  &lt; 2e-16 ***\nDESTIN_SZAMSZ09 -0.9908946  0.0052019  -190.488  &lt; 2e-16 ***\nDESTIN_SZAMSZ10  0.0747666  0.0036210    20.648  &lt; 2e-16 ***\nDESTIN_SZAMSZ11  0.0650384  0.0061036    10.656  &lt; 2e-16 ***\nDESTIN_SZAMSZ12  0.1094086  0.0044608    24.527  &lt; 2e-16 ***\nDESTIN_SZBDSZ01  0.4606543  0.0032805   140.421  &lt; 2e-16 ***\nDESTIN_SZBDSZ02 -0.2851305  0.0041832   -68.161  &lt; 2e-16 ***\nDESTIN_SZBDSZ03 -0.0502638  0.0036528   -13.760  &lt; 2e-16 ***\nDESTIN_SZBDSZ04  0.7572217  0.0030360   249.417  &lt; 2e-16 ***\nDESTIN_SZBDSZ05  0.4713216  0.0033306   141.514  &lt; 2e-16 ***\nDESTIN_SZBDSZ06  0.0664937  0.0037174    17.887  &lt; 2e-16 ***\nDESTIN_SZBDSZ07 -0.8151564  0.0080089  -101.782  &lt; 2e-16 ***\nDESTIN_SZBDSZ08 -1.7619262  0.0087946  -200.341  &lt; 2e-16 ***\nDESTIN_SZBKSZ01 -1.1580572  0.0054228  -213.553  &lt; 2e-16 ***\nDESTIN_SZBKSZ02 -0.4331678  0.0046774   -92.608  &lt; 2e-16 ***\nDESTIN_SZBKSZ03 -0.7371361  0.0046709  -157.815  &lt; 2e-16 ***\nDESTIN_SZBKSZ04 -0.0155780  0.0041846    -3.723 0.000197 ***\nDESTIN_SZBKSZ05 -0.6106025  0.0046676  -130.816  &lt; 2e-16 ***\nDESTIN_SZBKSZ06 -0.9901477  0.0052671  -187.986  &lt; 2e-16 ***\nDESTIN_SZBKSZ07  0.1450589  0.0035450    40.920  &lt; 2e-16 ***\nDESTIN_SZBKSZ08 -1.1298867  0.0058374  -193.561  &lt; 2e-16 ***\nDESTIN_SZBKSZ09 -0.1228396  0.0041792   -29.393  &lt; 2e-16 ***\nDESTIN_SZBLSZ01 -0.5720031  0.0056589  -101.081  &lt; 2e-16 ***\nDESTIN_SZBLSZ02  0.5167119  0.0056189    91.960  &lt; 2e-16 ***\nDESTIN_SZBLSZ03  1.7440804  0.0062115   280.782  &lt; 2e-16 ***\nDESTIN_SZBLSZ04 -0.1927581  0.0108946   -17.693  &lt; 2e-16 ***\nDESTIN_SZBMSZ01 -0.0057613  0.0037810    -1.524 0.127571    \nDESTIN_SZBMSZ02 -0.1858456  0.0039734   -46.773  &lt; 2e-16 ***\nDESTIN_SZBMSZ03 -0.5191446  0.0048004  -108.145  &lt; 2e-16 ***\nDESTIN_SZBMSZ04 -0.2665002  0.0041422   -64.337  &lt; 2e-16 ***\nDESTIN_SZBMSZ05 -0.4309725  0.0050386   -85.534  &lt; 2e-16 ***\nDESTIN_SZBMSZ06 -1.4289091  0.0090428  -158.015  &lt; 2e-16 ***\nDESTIN_SZBMSZ07  0.1697895  0.0036969    45.928  &lt; 2e-16 ***\nDESTIN_SZBMSZ08 -0.8042249  0.0049408  -162.773  &lt; 2e-16 ***\nDESTIN_SZBMSZ09 -1.4976945  0.0074776  -200.290  &lt; 2e-16 ***\nDESTIN_SZBMSZ10 -1.1040876  0.0062484  -176.699  &lt; 2e-16 ***\nDESTIN_SZBMSZ11 -1.1176271  0.0057986  -192.740  &lt; 2e-16 ***\nDESTIN_SZBMSZ12 -0.5052794  0.0061462   -82.210  &lt; 2e-16 ***\nDESTIN_SZBMSZ13  0.0469214  0.0039850    11.774  &lt; 2e-16 ***\nDESTIN_SZBMSZ14 -0.6065308  0.0062775   -96.619  &lt; 2e-16 ***\nDESTIN_SZBMSZ15 -0.9269201  0.0058628  -158.102  &lt; 2e-16 ***\nDESTIN_SZBMSZ16 -1.1967420  0.0059108  -202.467  &lt; 2e-16 ***\nDESTIN_SZBMSZ17 -1.2600105  0.0071081  -177.263  &lt; 2e-16 ***\nDESTIN_SZBPSZ01 -0.8215475  0.0046830  -175.433  &lt; 2e-16 ***\nDESTIN_SZBPSZ02 -1.7232481  0.0073394  -234.795  &lt; 2e-16 ***\nDESTIN_SZBPSZ03 -1.5211793  0.0069230  -219.728  &lt; 2e-16 ***\nDESTIN_SZBPSZ04 -0.8198739  0.0050141  -163.515  &lt; 2e-16 ***\nDESTIN_SZBPSZ05  0.2906362  0.0034327    84.668  &lt; 2e-16 ***\nDESTIN_SZBPSZ06 -0.8235794  0.0065818  -125.130  &lt; 2e-16 ***\nDESTIN_SZBPSZ07 -0.4339289  0.0064190   -67.601  &lt; 2e-16 ***\nDESTIN_SZBSSZ01 -0.0084053  0.0038356    -2.191 0.028425 *  \nDESTIN_SZBSSZ02 -0.7632220  0.0044126  -172.964  &lt; 2e-16 ***\nDESTIN_SZBSSZ03  0.4159526  0.0032329   128.662  &lt; 2e-16 ***\nDESTIN_SZBTSZ01  0.3719534  0.0034537   107.696  &lt; 2e-16 ***\nDESTIN_SZBTSZ02 -0.5304604  0.0053375   -99.383  &lt; 2e-16 ***\nDESTIN_SZBTSZ03  0.2220414  0.0039089    56.804  &lt; 2e-16 ***\nDESTIN_SZBTSZ04 -1.2322756  0.0083086  -148.314  &lt; 2e-16 ***\nDESTIN_SZBTSZ05 -0.4784112  0.0059442   -80.484  &lt; 2e-16 ***\nDESTIN_SZBTSZ06 -0.6296966  0.0051396  -122.519  &lt; 2e-16 ***\nDESTIN_SZBTSZ07 -1.4992633  0.0078238  -191.629  &lt; 2e-16 ***\nDESTIN_SZBTSZ08 -0.7888866  0.0071119  -110.925  &lt; 2e-16 ***\nDESTIN_SZCBSZ01 -5.7490675  0.3015260   -19.067  &lt; 2e-16 ***\nDESTIN_SZCCSZ01 -0.6228152  0.0059934  -103.916  &lt; 2e-16 ***\nDESTIN_SZCHSZ01 -1.0599084  0.0073391  -144.420  &lt; 2e-16 ***\nDESTIN_SZCHSZ02  0.0225333  0.0046037     4.895 9.85e-07 ***\nDESTIN_SZCHSZ03  1.3001540  0.0033650   386.377  &lt; 2e-16 ***\nDESTIN_SZCKSZ01 -0.4607876  0.0043246  -106.551  &lt; 2e-16 ***\nDESTIN_SZCKSZ02 -0.8514408  0.0046569  -182.836  &lt; 2e-16 ***\nDESTIN_SZCKSZ03  0.3081439  0.0034954    88.157  &lt; 2e-16 ***\nDESTIN_SZCKSZ04 -1.5911473  0.0055533  -286.520  &lt; 2e-16 ***\nDESTIN_SZCKSZ05 -1.3747403  0.0064799  -212.155  &lt; 2e-16 ***\nDESTIN_SZCKSZ06  0.1656912  0.0050193    33.011  &lt; 2e-16 ***\nDESTIN_SZCLSZ01  0.2873713  0.0040546    70.875  &lt; 2e-16 ***\nDESTIN_SZCLSZ02 -2.0750563  0.0109437  -189.611  &lt; 2e-16 ***\nDESTIN_SZCLSZ03 -0.7343668  0.0062776  -116.983  &lt; 2e-16 ***\nDESTIN_SZCLSZ04 -0.0731781  0.0037658   -19.432  &lt; 2e-16 ***\nDESTIN_SZCLSZ05 -0.9054319  0.0068687  -131.820  &lt; 2e-16 ***\nDESTIN_SZCLSZ06  0.0950843  0.0035038    27.138  &lt; 2e-16 ***\nDESTIN_SZCLSZ07 -0.4829716  0.0044875  -107.626  &lt; 2e-16 ***\nDESTIN_SZCLSZ08 -0.3769961  0.0050913   -74.048  &lt; 2e-16 ***\nDESTIN_SZCLSZ09  0.4773761  0.0055158    86.547  &lt; 2e-16 ***\nDESTIN_SZDTSZ01 -0.4654749  0.0043477  -107.063  &lt; 2e-16 ***\nDESTIN_SZDTSZ02 -0.7471213  0.0042961  -173.906  &lt; 2e-16 ***\nDESTIN_SZDTSZ03 -0.9544640  0.0049695  -192.063  &lt; 2e-16 ***\nDESTIN_SZDTSZ04 -0.6207128  0.0111608   -55.615  &lt; 2e-16 ***\nDESTIN_SZDTSZ05 -0.6792127  0.0080674   -84.192  &lt; 2e-16 ***\nDESTIN_SZDTSZ06 -0.9191055  0.0052744  -174.257  &lt; 2e-16 ***\nDESTIN_SZDTSZ07 -1.9505227  0.0176421  -110.561  &lt; 2e-16 ***\nDESTIN_SZDTSZ08 -0.4558191  0.0040750  -111.858  &lt; 2e-16 ***\nDESTIN_SZDTSZ09 -1.6168459  0.0093023  -173.812  &lt; 2e-16 ***\nDESTIN_SZDTSZ10 -1.2996110  0.0078989  -164.530  &lt; 2e-16 ***\nDESTIN_SZDTSZ11 -0.5333164  0.0043526  -122.528  &lt; 2e-16 ***\nDESTIN_SZDTSZ12 -2.1322909  0.0129407  -164.775  &lt; 2e-16 ***\nDESTIN_SZDTSZ13 -1.5641658  0.0086931  -179.932  &lt; 2e-16 ***\nDESTIN_SZGLSZ01  0.0706629  0.0042080    16.792  &lt; 2e-16 ***\nDESTIN_SZGLSZ02 -0.1905650  0.0039049   -48.801  &lt; 2e-16 ***\nDESTIN_SZGLSZ03  0.4282643  0.0032861   130.328  &lt; 2e-16 ***\nDESTIN_SZGLSZ04  0.3789046  0.0032380   117.017  &lt; 2e-16 ***\nDESTIN_SZGLSZ05  0.2492438  0.0033276    74.903  &lt; 2e-16 ***\nDESTIN_SZHGSZ01  0.3240772  0.0032762    98.917  &lt; 2e-16 ***\nDESTIN_SZHGSZ02 -0.7408270  0.0044767  -165.486  &lt; 2e-16 ***\nDESTIN_SZHGSZ03 -1.2648715  0.0054331  -232.808  &lt; 2e-16 ***\nDESTIN_SZHGSZ04 -0.4916254  0.0038505  -127.677  &lt; 2e-16 ***\nDESTIN_SZHGSZ05 -0.5099161  0.0038656  -131.912  &lt; 2e-16 ***\nDESTIN_SZHGSZ06 -0.7974575  0.0046093  -173.011  &lt; 2e-16 ***\nDESTIN_SZHGSZ07  0.0741845  0.0035304    21.013  &lt; 2e-16 ***\nDESTIN_SZHGSZ08 -0.2910869  0.0040350   -72.140  &lt; 2e-16 ***\nDESTIN_SZHGSZ09  0.0147042  0.0042104     3.492 0.000479 ***\nDESTIN_SZHGSZ10 -3.3192590  0.0278660  -119.115  &lt; 2e-16 ***\nDESTIN_SZJESZ01 -0.3513262  0.0043899   -80.031  &lt; 2e-16 ***\nDESTIN_SZJESZ02 -0.6287145  0.0044890  -140.058  &lt; 2e-16 ***\nDESTIN_SZJESZ03 -0.6600254  0.0048237  -136.829  &lt; 2e-16 ***\nDESTIN_SZJESZ04 -0.2143484  0.0052218   -41.049  &lt; 2e-16 ***\nDESTIN_SZJESZ05 -1.0464437  0.0077971  -134.209  &lt; 2e-16 ***\nDESTIN_SZJESZ06  0.2556833  0.0035299    72.433  &lt; 2e-16 ***\nDESTIN_SZJESZ07 -0.9388694  0.0061058  -153.766  &lt; 2e-16 ***\nDESTIN_SZJESZ08 -0.8873531  0.0065828  -134.799  &lt; 2e-16 ***\nDESTIN_SZJESZ09 -0.5175288  0.0048724  -106.216  &lt; 2e-16 ***\nDESTIN_SZJESZ10  0.4235479  0.0061252    69.149  &lt; 2e-16 ***\nDESTIN_SZJESZ11  0.8124224  0.0057291   141.807  &lt; 2e-16 ***\nDESTIN_SZJWSZ01 -0.8287239  0.0056309  -147.176  &lt; 2e-16 ***\nDESTIN_SZJWSZ02 -0.7487269  0.0046465  -161.140  &lt; 2e-16 ***\nDESTIN_SZJWSZ03  0.2458626  0.0036316    67.701  &lt; 2e-16 ***\nDESTIN_SZJWSZ04  0.7540798  0.0034046   221.487  &lt; 2e-16 ***\nDESTIN_SZJWSZ05 -0.4339697  0.0051107   -84.914  &lt; 2e-16 ***\nDESTIN_SZJWSZ06 -0.1685553  0.0046727   -36.073  &lt; 2e-16 ***\nDESTIN_SZJWSZ07 -1.7066370  0.0216172   -78.948  &lt; 2e-16 ***\nDESTIN_SZJWSZ08 -0.6130563  0.0042838  -143.109  &lt; 2e-16 ***\nDESTIN_SZJWSZ09  0.9790610  0.0031231   313.492  &lt; 2e-16 ***\nDESTIN_SZKLSZ01 -0.4333011  0.0040995  -105.696  &lt; 2e-16 ***\nDESTIN_SZKLSZ02 -0.6144506  0.0046468  -132.230  &lt; 2e-16 ***\nDESTIN_SZKLSZ03 -1.0360469  0.0051172  -202.462  &lt; 2e-16 ***\nDESTIN_SZKLSZ04 -1.4805613  0.0064591  -229.220  &lt; 2e-16 ***\nDESTIN_SZKLSZ05 -0.6173818  0.0058050  -106.353  &lt; 2e-16 ***\nDESTIN_SZKLSZ06 -0.6096238  0.0044349  -137.460  &lt; 2e-16 ***\nDESTIN_SZKLSZ07 -0.7846468  0.0050656  -154.897  &lt; 2e-16 ***\nDESTIN_SZKLSZ08  0.0327319  0.0036692     8.921  &lt; 2e-16 ***\nDESTIN_SZKLSZ09 -1.5041583  0.0065291  -230.378  &lt; 2e-16 ***\nDESTIN_SZLKSZ01 -1.8762329  0.0185297  -101.255  &lt; 2e-16 ***\nDESTIN_SZMDSZ01 -1.6915164  0.0161910  -104.473  &lt; 2e-16 ***\nDESTIN_SZMDSZ02 -1.3455764  0.0089035  -151.128  &lt; 2e-16 ***\nDESTIN_SZMDSZ03 -2.5797766  0.0194380  -132.718  &lt; 2e-16 ***\nDESTIN_SZMPSZ01 -0.7336083  0.0059618  -123.051  &lt; 2e-16 ***\nDESTIN_SZMPSZ02 -0.6800348  0.0048649  -139.784  &lt; 2e-16 ***\nDESTIN_SZMPSZ03 -0.0588278  0.0038990   -15.088  &lt; 2e-16 ***\nDESTIN_SZMSSZ01 -1.2315397  0.0720247   -17.099  &lt; 2e-16 ***\nDESTIN_SZMUSZ01 -0.9982577  0.0048452  -206.028  &lt; 2e-16 ***\nDESTIN_SZMUSZ02 -0.9777965  0.0069929  -139.827  &lt; 2e-16 ***\nDESTIN_SZMUSZ03 -0.9364483  0.0046579  -201.044  &lt; 2e-16 ***\nDESTIN_SZNTSZ01 -2.1675081  0.0198299  -109.305  &lt; 2e-16 ***\nDESTIN_SZNTSZ02 -1.7472243  0.0088938  -196.455  &lt; 2e-16 ***\nDESTIN_SZNTSZ03 -1.1454774  0.0062563  -183.091  &lt; 2e-16 ***\nDESTIN_SZNTSZ05 -1.7381130  0.0153286  -113.390  &lt; 2e-16 ***\nDESTIN_SZNTSZ06 -2.7926947  0.0248656  -112.311  &lt; 2e-16 ***\nDESTIN_SZNVSZ01 -0.2038169  0.0036335   -56.093  &lt; 2e-16 ***\nDESTIN_SZNVSZ02 -0.3113161  0.0041825   -74.433  &lt; 2e-16 ***\nDESTIN_SZNVSZ03 -0.3770824  0.0044136   -85.436  &lt; 2e-16 ***\nDESTIN_SZNVSZ04 -1.9320410  0.0088561  -218.160  &lt; 2e-16 ***\nDESTIN_SZNVSZ05 -1.5428117  0.0075292  -204.909  &lt; 2e-16 ***\nDESTIN_SZORSZ01 -1.2355081  0.0157731   -78.330  &lt; 2e-16 ***\nDESTIN_SZORSZ02  0.1250163  0.0036777    33.993  &lt; 2e-16 ***\nDESTIN_SZORSZ03 -0.6452476  0.0045825  -140.806  &lt; 2e-16 ***\nDESTIN_SZOTSZ01 -1.0704906  0.0060731  -176.268  &lt; 2e-16 ***\nDESTIN_SZOTSZ02 -0.3781679  0.0053173   -71.120  &lt; 2e-16 ***\nDESTIN_SZOTSZ03 -1.1481810  0.0056670  -202.608  &lt; 2e-16 ***\nDESTIN_SZOTSZ04 -1.4202254  0.0079652  -178.304  &lt; 2e-16 ***\nDESTIN_SZPGSZ01 -1.8350720  0.0154368  -118.877  &lt; 2e-16 ***\nDESTIN_SZPGSZ02 -0.7564661  0.0054524  -138.741  &lt; 2e-16 ***\nDESTIN_SZPGSZ03  0.2505301  0.0034370    72.892  &lt; 2e-16 ***\nDESTIN_SZPGSZ04 -0.2173963  0.0038323   -56.727  &lt; 2e-16 ***\nDESTIN_SZPGSZ05 -1.0267257  0.0063632  -161.353  &lt; 2e-16 ***\nDESTIN_SZPLSZ01 -0.4035506  0.0054252   -74.384  &lt; 2e-16 ***\nDESTIN_SZPLSZ02 -1.5062324  0.0102290  -147.251  &lt; 2e-16 ***\nDESTIN_SZPLSZ03 -0.0236522  0.0080940    -2.922 0.003476 ** \nDESTIN_SZPLSZ04  0.0180567  0.0076703     2.354 0.018567 *  \nDESTIN_SZPLSZ05 -0.6094267  0.0092680   -65.756  &lt; 2e-16 ***\nDESTIN_SZPNSZ01 -0.1368197  0.0047254   -28.954  &lt; 2e-16 ***\nDESTIN_SZPNSZ02  1.1051298  0.0063275   174.654  &lt; 2e-16 ***\nDESTIN_SZPNSZ03  0.2257901  0.0064458    35.029  &lt; 2e-16 ***\nDESTIN_SZPNSZ04  2.0035937  0.0070722   283.306  &lt; 2e-16 ***\nDESTIN_SZPNSZ05  1.3067158  0.0098823   132.228  &lt; 2e-16 ***\nDESTIN_SZPRSZ01 -0.9820082  0.0062529  -157.049  &lt; 2e-16 ***\nDESTIN_SZPRSZ02 -0.4228424  0.0043189   -97.906  &lt; 2e-16 ***\nDESTIN_SZPRSZ03  0.6354546  0.0032898   193.157  &lt; 2e-16 ***\nDESTIN_SZPRSZ04 -0.5550102  0.0071849   -77.247  &lt; 2e-16 ***\nDESTIN_SZPRSZ05 -0.3612456  0.0041113   -87.867  &lt; 2e-16 ***\nDESTIN_SZPRSZ06  0.2081266  0.0042348    49.147  &lt; 2e-16 ***\nDESTIN_SZPRSZ07 -1.0449719  0.0098563  -106.020  &lt; 2e-16 ***\nDESTIN_SZPRSZ08 -0.7571762  0.0055397  -136.682  &lt; 2e-16 ***\nDESTIN_SZQTSZ01 -1.4857266  0.0082843  -179.343  &lt; 2e-16 ***\nDESTIN_SZQTSZ02 -1.2170144  0.0061081  -199.246  &lt; 2e-16 ***\nDESTIN_SZQTSZ03 -0.6219226  0.0053633  -115.959  &lt; 2e-16 ***\nDESTIN_SZQTSZ04 -0.7058853  0.0056012  -126.024  &lt; 2e-16 ***\nDESTIN_SZQTSZ05 -0.6660971  0.0050476  -131.962  &lt; 2e-16 ***\nDESTIN_SZQTSZ06 -0.8590703  0.0052480  -163.694  &lt; 2e-16 ***\nDESTIN_SZQTSZ07 -1.3480834  0.0085058  -158.490  &lt; 2e-16 ***\nDESTIN_SZQTSZ08  0.2234121  0.0038996    57.291  &lt; 2e-16 ***\nDESTIN_SZQTSZ09 -0.4155807  0.0046764   -88.867  &lt; 2e-16 ***\nDESTIN_SZQTSZ10 -0.2168239  0.0044081   -49.187  &lt; 2e-16 ***\nDESTIN_SZQTSZ11  0.3416454  0.0041325    82.673  &lt; 2e-16 ***\nDESTIN_SZQTSZ12 -0.0701328  0.0053253   -13.170  &lt; 2e-16 ***\nDESTIN_SZQTSZ13  0.2104927  0.0041398    50.846  &lt; 2e-16 ***\nDESTIN_SZQTSZ14  0.1741816  0.0045428    38.342  &lt; 2e-16 ***\nDESTIN_SZQTSZ15  0.0549791  0.0056229     9.778  &lt; 2e-16 ***\nDESTIN_SZRCSZ01 -0.7119701  0.0051113  -139.293  &lt; 2e-16 ***\nDESTIN_SZRCSZ02 -2.1115822  0.0132956  -158.818  &lt; 2e-16 ***\nDESTIN_SZRCSZ03 -0.9880653  0.0070701  -139.753  &lt; 2e-16 ***\nDESTIN_SZRCSZ04 -2.1593121  0.0104118  -207.392  &lt; 2e-16 ***\nDESTIN_SZRCSZ05 -2.1458138  0.0093879  -228.573  &lt; 2e-16 ***\nDESTIN_SZRCSZ06 -1.6533074  0.0114337  -144.599  &lt; 2e-16 ***\nDESTIN_SZRCSZ08 -1.5440384  0.0105007  -147.041  &lt; 2e-16 ***\nDESTIN_SZRCSZ09 -1.1534494  0.0090862  -126.946  &lt; 2e-16 ***\nDESTIN_SZRCSZ10 -0.7427657  0.0050578  -146.855  &lt; 2e-16 ***\nDESTIN_SZRVSZ01 -1.5307971  0.0084545  -181.063  &lt; 2e-16 ***\nDESTIN_SZRVSZ02 -2.0469520  0.0107004  -191.296  &lt; 2e-16 ***\nDESTIN_SZRVSZ03 -1.9563053  0.0101178  -193.352  &lt; 2e-16 ***\nDESTIN_SZRVSZ04 -1.7151110  0.0136535  -125.617  &lt; 2e-16 ***\nDESTIN_SZRVSZ05 -1.3973761  0.0117040  -119.393  &lt; 2e-16 ***\nDESTIN_SZSBSZ01 -0.3445065  0.0053709   -64.143  &lt; 2e-16 ***\nDESTIN_SZSBSZ02 -0.7901522  0.0061627  -128.216  &lt; 2e-16 ***\nDESTIN_SZSBSZ03  0.5686155  0.0037479   151.716  &lt; 2e-16 ***\nDESTIN_SZSBSZ04  0.0948650  0.0048030    19.751  &lt; 2e-16 ***\nDESTIN_SZSBSZ05 -0.8502140  0.0061259  -138.791  &lt; 2e-16 ***\nDESTIN_SZSBSZ06 -1.9470117  0.0204807   -95.066  &lt; 2e-16 ***\nDESTIN_SZSBSZ07 -1.8889857  0.0154736  -122.078  &lt; 2e-16 ***\nDESTIN_SZSBSZ08  1.0916206  0.0044551   245.027  &lt; 2e-16 ***\nDESTIN_SZSBSZ09  0.5717996  0.0043530   131.357  &lt; 2e-16 ***\nDESTIN_SZSESZ02 -0.4986952  0.0039968  -124.772  &lt; 2e-16 ***\nDESTIN_SZSESZ03  0.4180354  0.0031750   131.664  &lt; 2e-16 ***\nDESTIN_SZSESZ04 -0.8534378  0.0046680  -182.826  &lt; 2e-16 ***\nDESTIN_SZSESZ05 -0.2355405  0.0038785   -60.730  &lt; 2e-16 ***\nDESTIN_SZSESZ06 -0.7800669  0.0048106  -162.155  &lt; 2e-16 ***\nDESTIN_SZSESZ07 -3.0824755  0.0180415  -170.855  &lt; 2e-16 ***\nDESTIN_SZSGSZ01 -0.0878278  0.0047555   -18.469  &lt; 2e-16 ***\nDESTIN_SZSGSZ02 -0.1224880  0.0042568   -28.774  &lt; 2e-16 ***\nDESTIN_SZSGSZ03 -0.3033322  0.0039550   -76.696  &lt; 2e-16 ***\nDESTIN_SZSGSZ04 -0.2190187  0.0039866   -54.939  &lt; 2e-16 ***\nDESTIN_SZSGSZ05 -1.8582654  0.0074920  -248.035  &lt; 2e-16 ***\nDESTIN_SZSGSZ06  0.4255461  0.0031870   133.524  &lt; 2e-16 ***\nDESTIN_SZSGSZ07 -0.3695539  0.0041243   -89.604  &lt; 2e-16 ***\nDESTIN_SZSISZ01 -0.8587196  0.0156917   -54.725  &lt; 2e-16 ***\nDESTIN_SZSKSZ01 -0.5054408  0.0058211   -86.830  &lt; 2e-16 ***\nDESTIN_SZSKSZ02  0.3944638  0.0045641    86.428  &lt; 2e-16 ***\nDESTIN_SZSKSZ03 -0.4365958  0.0051246   -85.196  &lt; 2e-16 ***\nDESTIN_SZSKSZ04 -0.8139278  0.0129772   -62.720  &lt; 2e-16 ***\nDESTIN_SZSKSZ05 -0.2790184  0.0106256   -26.259  &lt; 2e-16 ***\nDESTIN_SZSLSZ01 -0.6970672  0.0065655  -106.171  &lt; 2e-16 ***\nDESTIN_SZSLSZ04 -0.8670220  0.0053636  -161.648  &lt; 2e-16 ***\nDESTIN_SZSRSZ01 -1.1143464  0.0062111  -179.413  &lt; 2e-16 ***\nDESTIN_SZSRSZ02 -1.2910974  0.0074195  -174.015  &lt; 2e-16 ***\nDESTIN_SZSRSZ03 -1.2856614  0.0066576  -193.111  &lt; 2e-16 ***\nDESTIN_SZSVSZ01 -1.3293635  0.0524662   -25.338  &lt; 2e-16 ***\nDESTIN_SZTHSZ01 -3.7060783  0.0342658  -108.157  &lt; 2e-16 ***\nDESTIN_SZTHSZ03 -2.4238238  0.0173564  -139.650  &lt; 2e-16 ***\nDESTIN_SZTHSZ04 -2.5493082  0.0182992  -139.313  &lt; 2e-16 ***\nDESTIN_SZTHSZ06 -1.4545368  0.0097474  -149.223  &lt; 2e-16 ***\nDESTIN_SZTMSZ01 -0.2986406  0.0044873   -66.553  &lt; 2e-16 ***\nDESTIN_SZTMSZ02  1.3094630  0.0029024   451.164  &lt; 2e-16 ***\nDESTIN_SZTMSZ03  0.5310564  0.0032698   162.411  &lt; 2e-16 ***\nDESTIN_SZTMSZ04  0.8505528  0.0033608   253.083  &lt; 2e-16 ***\nDESTIN_SZTMSZ05  0.7606646  0.0041987   181.167  &lt; 2e-16 ***\nDESTIN_SZTNSZ01 -0.4189245  0.0045601   -91.868  &lt; 2e-16 ***\nDESTIN_SZTNSZ02 -0.9565221  0.0056933  -168.009  &lt; 2e-16 ***\nDESTIN_SZTNSZ03 -0.9785646  0.0069362  -141.081  &lt; 2e-16 ***\nDESTIN_SZTNSZ04 -0.9047693  0.0054228  -166.846  &lt; 2e-16 ***\nDESTIN_SZTPSZ01 -0.3315217  0.0045312   -73.164  &lt; 2e-16 ***\nDESTIN_SZTPSZ02  0.2976676  0.0031834    93.506  &lt; 2e-16 ***\nDESTIN_SZTPSZ03 -0.2191873  0.0046109   -47.537  &lt; 2e-16 ***\nDESTIN_SZTPSZ04 -1.4816355  0.0060901  -243.284  &lt; 2e-16 ***\nDESTIN_SZTPSZ05 -0.9451304  0.0048844  -193.499  &lt; 2e-16 ***\nDESTIN_SZTPSZ06 -0.4402911  0.0061953   -71.068  &lt; 2e-16 ***\nDESTIN_SZTPSZ07 -1.7312387  0.0088635  -195.323  &lt; 2e-16 ***\nDESTIN_SZTPSZ08 -1.2593763  0.0067715  -185.983  &lt; 2e-16 ***\nDESTIN_SZTPSZ09 -0.2877616  0.0049488   -58.148  &lt; 2e-16 ***\nDESTIN_SZTPSZ10 -0.9697193  0.0063083  -153.721  &lt; 2e-16 ***\nDESTIN_SZTPSZ11 -0.3005951  0.0041849   -71.829  &lt; 2e-16 ***\nDESTIN_SZTPSZ12 -0.7000077  0.0050923  -137.464  &lt; 2e-16 ***\nDESTIN_SZTSSZ01 -0.8486651  0.0179055   -47.397  &lt; 2e-16 ***\nDESTIN_SZTSSZ02 -0.4612835  0.0091652   -50.330  &lt; 2e-16 ***\nDESTIN_SZTSSZ03  0.3855345  0.0073471    52.474  &lt; 2e-16 ***\nDESTIN_SZTSSZ04  0.5646875  0.0073620    76.703  &lt; 2e-16 ***\nDESTIN_SZTSSZ05  1.5412341  0.0075667   203.685  &lt; 2e-16 ***\nDESTIN_SZTSSZ06  1.4055634  0.0148658    94.550  &lt; 2e-16 ***\nDESTIN_SZWCSZ01  1.3002792  0.0041779   311.230  &lt; 2e-16 ***\nDESTIN_SZWCSZ02 -0.6410929  0.0081431   -78.729  &lt; 2e-16 ***\nDESTIN_SZWCSZ03 -1.8668639  0.0222402   -83.941  &lt; 2e-16 ***\nDESTIN_SZWDSZ01  0.9822644  0.0030806   318.860  &lt; 2e-16 ***\nDESTIN_SZWDSZ02 -0.8743246  0.0050725  -172.366  &lt; 2e-16 ***\nDESTIN_SZWDSZ03  0.5998313  0.0034643   173.146  &lt; 2e-16 ***\nDESTIN_SZWDSZ04 -0.5484603  0.0050578  -108.437  &lt; 2e-16 ***\nDESTIN_SZWDSZ05 -0.1276892  0.0045257   -28.214  &lt; 2e-16 ***\nDESTIN_SZWDSZ06  0.1425300  0.0036637    38.903  &lt; 2e-16 ***\nDESTIN_SZWDSZ07 -0.0488935  0.0051192    -9.551  &lt; 2e-16 ***\nDESTIN_SZWDSZ08  0.0907587  0.0052909    17.154  &lt; 2e-16 ***\nDESTIN_SZWDSZ09 -0.2041143  0.0040971   -49.819  &lt; 2e-16 ***\nDESTIN_SZYSSZ01  0.9543738  0.0033197   287.486  &lt; 2e-16 ***\nDESTIN_SZYSSZ02 -0.3337783  0.0045291   -73.696  &lt; 2e-16 ***\nDESTIN_SZYSSZ03 -1.2283386  0.0047357  -259.379  &lt; 2e-16 ***\nDESTIN_SZYSSZ04 -0.3435894  0.0042780   -80.316  &lt; 2e-16 ***\nDESTIN_SZYSSZ05 -1.7940024  0.0084381  -212.606  &lt; 2e-16 ***\nDESTIN_SZYSSZ06 -1.3894763  0.0068219  -203.680  &lt; 2e-16 ***\nDESTIN_SZYSSZ07 -0.7313014  0.0086863   -84.191  &lt; 2e-16 ***\nDESTIN_SZYSSZ08  0.6469980  0.0033582   192.662  &lt; 2e-16 ***\nDESTIN_SZYSSZ09 -0.0025843  0.0035030    -0.738 0.460676    \nlog(dist)       -0.6907747  0.0001061 -6507.641  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 107870462  on 21078  degrees of freedom\nResidual deviance:  36958927  on 20458  degrees of freedom\nAIC: 37094407\n\nNumber of Fisher Scoring iterations: 7\n\n\n\nCalcRSquared(dbcSIM$data$TRIPS, dbcSIM$fitted.values)\n\n[1] 0.5563317"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#model-comparison",
    "href": "In-class_Ex3/In-class_Ex3.html#model-comparison",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Model comparison",
    "text": "Model comparison\nCompare performance.\n\nmodel_list &lt;- list(unconstrained=uncSIM,\n                   originConstrained=orcSIM,\n                   destinationConstrained=decSIM,\n                   doublyConstrained=dbcSIM)\n\n\ncompare_performance(model_list,\n                    metrics = \"RMSE\")\n\n# Comparison of Model Performance Indices\n\nName                   | Model |     RMSE\n-----------------------------------------\nunconstrained          |   glm | 5581.548\noriginConstrained      |   glm | 4940.551\ndestinationConstrained |   glm | 4496.325\ndoublyConstrained      |   glm | 4323.664\n\n\nBest model has the smallest RMSE so it is doubleConstrained"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html#visualising-fitted",
    "href": "In-class_Ex3/In-class_Ex3.html#visualising-fitted",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Visualising fitted",
    "text": "Visualising fitted\nUnconstrained\n\ndf &lt;- as.data.frame(uncSIM$fitted.values) %&gt;%\n  round(digits = 0)\n\n\nSIM_data &lt;- SIM_data %&gt;%\n  cbind(df) %&gt;%\n  rename(uncTRIPS = \"uncSIM$fitted.values\")\n\nOrigin constrained\n\ndf &lt;- as.data.frame(orcSIM$fitted.values) %&gt;%\n  round(digits = 0)\n\n\nSIM_data &lt;- SIM_data %&gt;%\n  cbind(df) %&gt;%\n  rename(orcTRIPS = \"orcSIM$fitted.values\")\n\nDestination constrained\n\ndf &lt;- as.data.frame(decSIM$fitted.values) %&gt;%\n  round(digits = 0)\n\n\nSIM_data &lt;- SIM_data %&gt;%\n  cbind(df) %&gt;%\n  rename(decTRIPS = \"decSIM$fitted.values\")\n\nDoubly constrained\n\ndf &lt;- as.data.frame(dbcSIM$fitted.values) %&gt;%\n  round(digits = 0)\n\n\nSIM_data &lt;- SIM_data %&gt;%\n  cbind(df) %&gt;%\n  rename(dbcTRIPS = \"dbcSIM$fitted.values\")\n\nGenerating plots\n\nunc_p &lt;- ggplot(data = SIM_data,\n                aes(x = uncTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\norc_p &lt;- ggplot(data = SIM_data,\n                aes(x = orcTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\ndec_p &lt;- ggplot(data = SIM_data,\n                aes(x = decTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\ndbc_p &lt;- ggplot(data = SIM_data,\n                aes(x = dbcTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\nggarrange(unc_p, orc_p, dec_p, dbc_p,\n          ncol = 2,\n          nrow = 2)\n\n\n\n\nDouble constrained looks less scattered and more fitted to the line, consistent with RMSE results"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS624",
    "section": "",
    "text": "Welcome to ISSS624 Geospatial Analytics Applications!\nI am Kristine Joy Paas, or Joy for short.\nIn this webpage, I am going to share with you my learning journey of geospatial analytics.\nNetlify link: https://isss624-kjcpaas.netlify.app/\nGithub repo: https://github.com/kjcpaas/ISSS624"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "",
    "text": "This hands-on exercise covers Chapter 1: Data Wrangling with R.\nI learned about the following:\n\nPublic Data Sets like the ones on data.gov.sg, LTADataMall, and InsideAirbnb.\nHow to import data sets into RStudio\nWrangling geospatial data in using different R packages like sf, tidyverse, etc.\nCreating thematic/choropleth maps with tmap"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html#preparing-the-data-sets",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html#preparing-the-data-sets",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "Preparing the data sets",
    "text": "Preparing the data sets\nFirst, I downloaded the different data sets needed in this exercise.\n\nGeospatial\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\n\n\n\nAspatial\n\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\nNext, is putting them under the Hands-on_Ex1 directory, with the following file structure:\nHands-on_Ex1\n└── data\n    ├── aspatial\n    │   └── listings.csv\n    └── geospatial\n        ├── CyclingPathGazette.cpg\n        ├── CyclingPathGazette.dbf\n        ├── CyclingPathGazette.lyr\n        ├── CyclingPathGazette.prj\n        ├── CyclingPathGazette.sbn\n        ├── CyclingPathGazette.sbx\n        ├── CyclingPathGazette.shp\n        ├── CyclingPathGazette.shp.xml\n        ├── CyclingPathGazette.shx\n        ├── MP14_SUBZONE_WEB_PL.dbf\n        ├── MP14_SUBZONE_WEB_PL.prj\n        ├── MP14_SUBZONE_WEB_PL.sbn\n        ├── MP14_SUBZONE_WEB_PL.sbx\n        ├── MP14_SUBZONE_WEB_PL.shp\n        ├── MP14_SUBZONE_WEB_PL.shp.xml\n        ├── MP14_SUBZONE_WEB_PL.shx\n        └── PreSchoolsLocation.kml"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html#installing-r-packages",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html#installing-r-packages",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "Installing R packages",
    "text": "Installing R packages\nI used the code below to install the R packages used in the exercise:\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html#master-plan-2014-subzone-boundary-web",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html#master-plan-2014-subzone-boundary-web",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "Master Plan 2014 Subzone Boundary (Web)",
    "text": "Master Plan 2014 Subzone Boundary (Web)\nTo import the data set to RStudio, I used st_read() :\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/Hands-on_Ex1/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nI encountered the error below along the way:\nCannot open layer MasterPlan2014SubzoneBoundaryWebKML\nThis is because I originally downloaded the kml file instead of the shp file. After using the shp file, the st_read() succeeded.\n\nℹ️ My biggest take-away for this is that st_read reads shp data set by default. (this would be debunked later)\n\nAfter running the code, we should see the mpsz data in the environment."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html#cycling-path-data",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html#cycling-path-data",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "Cycling Path Data",
    "text": "Cycling Path Data\nEquipped with my learning from the previous step, I was able to quickly figure out that importing this data set can be done by simply changing the layer parameter from the previous code:\n\ncyclingpath &lt;- st_read(dsn = \"data/geospatial\", layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/Hands-on_Ex1/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\nHowever, the difference is that this geometry has polyline features, while the previous has polygon features."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html#pre-schools-location-data",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html#pre-schools-location-data",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "Pre-Schools Location Data",
    "text": "Pre-Schools Location Data\nUnlike the others, this data set is in kml format instead of shp format. I used the following code to import:\n\npreschool &lt;- st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/Hands-on_Ex1/data/geospatial/PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nℹ️ Contrary to my previous take-away, st_read() can read kml files by default. In fact, reading shp files require more parameters like dsn and layer."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html#checking-the-geometry-of-data-frames",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html#checking-the-geometry-of-data-frames",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "Checking the geometry of data frames",
    "text": "Checking the geometry of data frames\nUsing st_geometry() returns information about the geometry of the data frame.\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nIt gave the same geometric information as when importing the shape data but with additional details like the first 5 geometries."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html#getting-overview-of-geospatial-data",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html#getting-overview-of-geospatial-data",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "Getting overview of geospatial data",
    "text": "Getting overview of geospatial data\nUsing glimpse() gives useful information about the columns, data types, values. For example:\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nThis will be very useful to scan for the available data and which columns are useful for analysis."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html#revealing-complete-information-of-feature-objects",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html#revealing-complete-information-of-feature-objects",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "Revealing complete information of feature objects",
    "text": "Revealing complete information of feature objects\nUsing head() can give full information about objects in the data set. For example:\n\nhead(mpsz, n=5)\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\nThis will return the first 5 objects, and the number of objects can be set by specifying a value for n.\nAnother function that is useful for this purpose is tail(), which returns items from the end of the data set. For example:\n\ntail(mpsz, n=2)\n\nSimple feature collection with 2 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 23378.74 ymin: 48569.62 xmax: 28343.2 ymax: 50256.33\nProjected CRS: SVY21\n    OBJECTID SUBZONE_NO    SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n322      322          7  THE WHARVES    SBSZ07      N  SEMBAWANG         SB\n323      323          8 SENOKO NORTH    SBSZ08      N  SEMBAWANG         SB\n        REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR\n322 NORTH REGION       NR 6D89875A351CF51C 2014-12-05 26945.07 49552.79\n323 NORTH REGION       NR A800CBEE879C1BF9 2014-12-05 24665.79 49482.60\n    SHAPE_Leng SHAPE_Area                       geometry\n322  11828.878    1635808 MULTIPOLYGON (((26219.89 50...\n323   7392.129    2241387 MULTIPOLYGON (((26047.11 50...\n\n\nThis returned the items on rows 322 and 323 instead of 1 and 2 if we were to use head().\nHowever, I wonder which use cases these functions would be useful as we can easily inspect the full data when looking at all the Environment Data in RStudio. 🤔"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html#plotting-geospatial-data",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html#plotting-geospatial-data",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "Plotting Geospatial Data",
    "text": "Plotting Geospatial Data\n\nplot(mpsz, max.plot = 15)\n\n\n\n\nI was pleasantly surprised to find that the rendering of the plots was so fast! It only took 1 second or so on my machine for 300+ features and 15 fields. This is really useful for quick look of the data.\nTrying it on the cycling path data was also very fast though the result was not so useful for me as it needs to be overlayed with a map like above.\n\nplot(cyclingpath)\n\n\n\n\nI wonder how long it would take once we have larger data sets. 🤔\nAs someone not originally from Singapore, I am still familiarizing myself with the countries geography so I’ll plot the regions first.\n\nplot(mpsz['REGION_N'])\n\n\n\n\nThis can still be visualized better, especially the names in the legend got cut off. From the Help pages on RStudio, this function has a lot more parameters and I’ll explore it once I have more time."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html#correcting-the-epsg-code",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html#correcting-the-epsg-code",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "Correcting the EPSG code",
    "text": "Correcting the EPSG code\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nI am not familiar with what the st_crs() returns and I wouldn’t have thought that EPSG needs correcting since I don’t have the domain knowledge. This is one of my biggest take away for this exercise."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html#projection-transformation",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html#projection-transformation",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "Projection Transformation",
    "text": "Projection Transformation\nWhen using st_set_crs(), I got the warning below:\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for that\nAs such, the mpsz3414 data before may not be projected properly despite having the correct EPSG value.\nNext, I will transform the pre-school data:\n\npreschool3414 &lt;- st_transform(preschool, crs=3414)\nst_geometry(preschool3414)\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html#importing-and-converting-aspatial-data",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html#importing-and-converting-aspatial-data",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "Importing and Converting Aspatial Data",
    "text": "Importing and Converting Aspatial Data\nFor aspatial data, I used read_csv() to import the data.\n\nlistings &lt;- read_csv(\"data/aspatial/listings.csv\")\n\nFrom this, we have candidate geospatial fields that we can use, longitude and latitude.\nChecking the data contents of these field can confirm if we can really use the data.\n\nlist(listings)\n\n[[1]]\n# A tibble: 3,483 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;            &lt;dbl&gt;\n 1  71609 Villa in…  367042 Belinda   East Region         Tampines          1.35\n 2  71896 Home in …  367042 Belinda   East Region         Tampines          1.35\n 3  71903 Home in …  367042 Belinda   East Region         Tampines          1.35\n 4 275343 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 5 275344 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Home in …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 Rental u… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330095 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 369141 Place to… 1521514 Elizabeth Central Region      Newton            1.31\n# ℹ 3,473 more rows\n# ℹ 11 more variables: longitude &lt;dbl&gt;, room_type &lt;chr&gt;, price &lt;dbl&gt;,\n#   minimum_nights &lt;dbl&gt;, number_of_reviews &lt;dbl&gt;, last_review &lt;date&gt;,\n#   reviews_per_month &lt;dbl&gt;, calculated_host_listings_count &lt;dbl&gt;,\n#   availability_365 &lt;dbl&gt;, number_of_reviews_ltm &lt;dbl&gt;, license &lt;chr&gt;\n\n\nAfter confirming that longitude and latitude can be used as geospatial data, I transformed this data to geospatial data.\n\nlistings_sf &lt;- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\nChecking the new data frame, it was confirmed that it was transformed to a geospatial data.\n\nglimpse(listings_sf)\n\nRows: 3,483\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Villa in Singapore · ★4.44 · 2 bedroom…\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; 150, 80, 80, 55, 69, 220, 85, 75, 45, 7…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 92, 60, 60, 92,…\n$ number_of_reviews              &lt;dbl&gt; 20, 24, 47, 22, 17, 12, 133, 18, 6, 81,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.14, 0.16, 0.31, 0.17, 0.12, 0.09, 0.9…\n$ calculated_host_listings_count &lt;dbl&gt; 5, 5, 5, 52, 52, 5, 7, 52, 52, 7, 7, 1,…\n$ availability_365               &lt;dbl&gt; 89, 89, 89, 275, 274, 89, 365, 365, 365…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 3, 0, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html#buffering",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html#buffering",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "Buffering",
    "text": "Buffering\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\nsum(buffer_cycling$AREA)\n\n1774367 [m^2]\n\n\nAfter seeing my classmate’s question on Piazza, I was also curious about the effect of nQuadSegs to the area. From the documentation, this is the number of segments created per quadrant.\nMy understanding of this is since each quadrant has 90 degrees, having nQuadSegs = 30 means 1 segment per 3 degrees. If this correct, my hypothesis is that the higher nQuadSegs is, the more accurate it is. This is because nQuadSegs=1 would be a square, and it becomes a polygon with more sides the higher nQuadSegs is. The higher nQuadSegs, the smoother the polygon becomes and it gets closer to being a circle.\nI’m testing the theory below and if my hypothesis is correct, the area should not differ much past nQuadSegs=180\n\nbuffer_cycling0 &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 0)\nbuffer_cycling0$AREA &lt;- st_area(buffer_cycling0)\nsum(buffer_cycling0$AREA)\n\n1700331 [m^2]\n\n\n\nbuffer_cycling10 &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 10)\nbuffer_cycling10$AREA &lt;- st_area(buffer_cycling10)\nsum(buffer_cycling10$AREA)\n\n1773584 [m^2]\n\n\n\nbuffer_cycling45 &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 45)\nbuffer_cycling45$AREA &lt;- st_area(buffer_cycling45)\nsum(buffer_cycling45$AREA)\n\n1774421 [m^2]\n\n\n\nbuffer_cycling90 &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 90)\nbuffer_cycling90$AREA &lt;- st_area(buffer_cycling90)\nsum(buffer_cycling90$AREA)\n\n1774454 [m^2]\n\n\n\nbuffer_cycling180 &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 180)\nbuffer_cycling180$AREA &lt;- st_area(buffer_cycling180)\nsum(buffer_cycling180$AREA)\n\n1774462 [m^2]\n\n\n\nbuffer_cycling1800 &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 1800)\nbuffer_cycling1800$AREA &lt;- st_area(buffer_cycling1800)\nsum(buffer_cycling1800$AREA)\n\n1774465 [m^2]\n\n\nAs we can see, from 180 to 1800 the area only changed by 3m2 but the differences are larger in lower values. My conclusion is that the higher nQuadSegs, the more accurate the value we will get. However, the calculation took much longer. The extremely small accuracy benefit may not be worth the trade-off in most cases.\nThe result when using nQuadSegs of 30 is already very close to the result when it is 1800.\n\\[\n1774367/1774465 = 99.99\\%\n\\]"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html#point-in-polygon-count",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html#point-in-polygon-count",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "Point-in-polygon count",
    "text": "Point-in-polygon count\n\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nThis means that there are subzones without pre-school while some have as many as 72.\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nThe subzone with the most pre-schools is Tampines East.\nTo calculate the density of pre-schools:\n\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\nLet’s see the top 3 subzones with the highest pre-school density.\n\ntop_n(mpsz3414, 3, `PreSch Density`)\n\nSimple feature collection with 3 features and 18 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25594.22 ymin: 28623.75 xmax: 29976.93 ymax: 48182.13\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO         SUBZONE_N SUBZONE_C CA_IND    PLN_AREA_N\n1       27          8             CECIL    DTSZ08      Y DOWNTOWN CORE\n2      278          3     MANDAI ESTATE    MDSZ03      N        MANDAI\n3      291          3 SEMBAWANG CENTRAL    SBSZ03      N     SEMBAWANG\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         DT CENTRAL REGION       CR 65AA82AF6F4D925D 2014-12-05 29730.20\n2         MD   NORTH REGION       NR F6266F7368DBB9AB 2014-12-05 27082.70\n3         SB   NORTH REGION       NR 772A64AB9A93FC3A 2014-12-05 26268.73\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry PreSch Count\n1 29011.33   2116.095   196619.9 MULTIPOLYGON (((29808.18 28...            7\n2 45367.46   1633.708   143137.9 MULTIPOLYGON (((27119.56 45...            5\n3 47558.08   3955.118   962437.4 MULTIPOLYGON (((26311.14 46...           27\n            Area   PreSch Density\n1 196619.9 [m^2] 35.60169 [1/m^2]\n2 143137.9 [m^2] 34.93134 [1/m^2]\n3 962437.4 [m^2] 28.05377 [1/m^2]\n\n\nDespite Tampines East having the most pre-schools, Cecil has the highest pre-school density. Tampines East might be much bigger than Cecil so its Pre-school Density is lower."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html#changing-the-website-theme",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html#changing-the-website-theme",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "Changing the website theme",
    "text": "Changing the website theme\nAfter exploring Quarto docs, I found that we can change the theme. I decided on the zephyr theme as it looks most readable and aesthetic for me."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1A.html#issues-with-using-github-on-rstudio",
    "href": "Hands-on_Ex1/Hands-on_Ex1A.html#issues-with-using-github-on-rstudio",
    "title": "Hands-on Exercise 1A: Geospatial Data Wrangling with R",
    "section": "Issues with using Github on Rstudio",
    "text": "Issues with using Github on Rstudio\nIt was recommended to name the project with the convention &lt;github_username&gt;/ISSS624. However, due to restrictions on my machine, I had to deviate from this and create my project elsewhere.\nHence, I couldn’t use usethis::use_github() to setup my Github repository. However, as I use git Github intensively in my job, I did the setup manually myself to use the git functions on RStudio.\nI used these steps for the manual setup.\n\nCreate the repo manually on Github on https://github.com/kjcpaas/ISSS624\nAdd the Github remote on RStudio project\n&gt; git remote add origin git@github.com:kjcpaas/ISSS624.git\n&gt; git remote -v\norigin  git@github.com:kjcpaas/ISSS624.git (fetch)\norigin  git@github.com:kjcpaas/ISSS624.git (push)\nSet remote for head\n&gt; git remote set-head origin --auto\n&gt; git gc\n\nAfter all these, I was able to use the git functions on RStudio."
  },
  {
    "objectID": "Take-home_Ex2/data/geospatial/Business.html",
    "href": "Take-home_Ex2/data/geospatial/Business.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Take-home_Ex2/data/geospatial/Retails.html",
    "href": "Take-home_Ex2/data/geospatial/Retails.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Take-home_Ex2/data/geospatial/MPSZ-2019.html",
    "href": "Take-home_Ex2/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take-home_Ex2/data/geospatial/F&B.html",
    "href": "Take-home_Ex2/data/geospatial/F&B.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "In-class_Ex1/In-class_Ex1.html",
    "href": "In-class_Ex1/In-class_Ex1.html",
    "title": "In-class Exercise 1: Bus Commuter Flow by Origin",
    "section": "",
    "text": "In this exercise, we will learn how to create choropleth map from bus data and commuter data."
  },
  {
    "objectID": "In-class_Ex1/In-class_Ex1.html#importing-the-origin-and-destination-data",
    "href": "In-class_Ex1/In-class_Ex1.html#importing-the-origin-and-destination-data",
    "title": "In-class Exercise 1: Bus Commuter Flow by Origin",
    "section": "Importing the origin and destination data",
    "text": "Importing the origin and destination data\nFirstly, we will import the Passenger Volume By Origin Destination Bust Stops data set from LTA Data Mall by using read_csv .\n\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202308.csv\")\n\nTo make it easier to process the PT codes, we will convert the origin and destination PT codes to factor data type.\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE)\n\nNext, I want to investigate the commuter flows between 7-10am on weekdays.\n\n\n\n\n\n\nTip\n\n\n\nWe need to use the interval 7 &lt;= time &lt;= 9 as the data with time = 9 contains data from 9am to just before 10am.\n\n\n\norigin7_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 7 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\nNext, we save the data in rds format for future use.\n\n\n\n\n\n\nImportant\n\n\n\nIn order for the code below to work, we need to create the rds/ directory under data/.\n\n\n\nwrite_rds(origin7_9, \"data/rds/origin7_9.rds\")\n\nIt can be imported back later on with read_rds().\n\norigin7_9 &lt;- read_rds(\"data/rds/origin7_9.rds\")"
  },
  {
    "objectID": "In-class_Ex1/In-class_Ex1.html#importing-geospatial-data",
    "href": "In-class_Ex1/In-class_Ex1.html#importing-geospatial-data",
    "title": "In-class Exercise 1: Bus Commuter Flow by Origin",
    "section": "Importing geospatial data",
    "text": "Importing geospatial data\nNext we need to import the bus stop locations so that we can correlate them from the PT codes from the origin and destination data from before.\n\nbusstop &lt;- st_read(dsn = \"data/geospatial\",\n                  layer = \"BusStop\") %&gt;%\n  st_transform(crs=3414)\n\nReading layer `BusStop' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/In-class_Ex1/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nSince these are just points, we also need some polygon data to figure out where in the Singapore map the bus locations correspond to:\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/In-class_Ex1/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class_Ex1/In-class_Ex1.html#combining-busstop-and-mpsz",
    "href": "In-class_Ex1/In-class_Ex1.html#combining-busstop-and-mpsz",
    "title": "In-class Exercise 1: Bus Commuter Flow by Origin",
    "section": "Combining busstop and mpsz",
    "text": "Combining busstop and mpsz\nWe first combine that 2 data frames by figuring out which polygon in mpsz each points in busstop are contained in.\n\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\nglimpse(busstop_mpsz)\n\nRows: 5,156\nColumns: 2\n$ BUS_STOP_N &lt;chr&gt; \"13099\", \"13089\", \"06151\", \"13211\", \"13139\", \"13109\", \"1311…\n$ SUBZONE_C  &lt;chr&gt; \"RVSZ05\", \"RVSZ05\", \"SRSZ01\", \"SRSZ01\", \"SRSZ01\", \"SRSZ01\",…\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_intersection() is used to perform point and polygon overly and the output will be in point sf object.\nselect() of dplyr package is then use to retain only BUS_STOP_N and SUBZONE_C in the busstop_mpsz sf data frame.\nst_drop_geometry() removes the geometry from the data frame, making the data aspatial.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nbusstop_mpsz has 5 less items than busstop. This is because these bus stops are outside of Singapore 🇸🇬 border.\nFor example, some bus routes start/end in Johor Bahru in Malaysia 🇲🇾.\n\n\nFinally, we now now which subzone each bus stop is located in.\nBefore proceeding, we should save the busstop_mpsz so we won’t need to recalculate it later on.\n\nwrite_rds(busstop_mpsz, \"data/rds/busstop_mpsz.csv\")\n\nNext is to join the busstop_mpsz data with origin7_9.\n\norigin_data &lt;- left_join(origin7_9 , busstop_mpsz,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C)\n\nNext is to check for duplicate records.\n\nduplicate &lt;- origin_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\nglimpse(duplicate)\n\nRows: 26\nColumns: 3\n$ ORIGIN_BS &lt;chr&gt; \"11009\", \"11009\", \"22501\", \"22501\", \"43709\", \"43709\", \"47201…\n$ TRIPS     &lt;dbl&gt; 13826, 13826, 9743, 9743, 1118, 1118, 23998, 23998, 6218, 62…\n$ ORIGIN_SZ &lt;chr&gt; \"QTSZ01\", \"QTSZ01\", \"JWSZ09\", \"JWSZ09\", \"BKSZ07\", \"BKSZ07\", …\n\n\nIn this case, there are some duplicates so we need to clean it up further. This can be done using unique().\n\norigin_data &lt;- unique(origin_data)\n\nLet’s check if the duplicates have been removed.\n\nduplicate &lt;- origin_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\nglimpse(duplicate)\n\nRows: 0\nColumns: 3\n$ ORIGIN_BS &lt;chr&gt; \n$ TRIPS     &lt;dbl&gt; \n$ ORIGIN_SZ &lt;chr&gt; \n\n\nThe duplicates have been removed so we can proceed with merging origin_data with mpsz to figure out the subzone names on the bus stop locations.\n\nmpsz_origtrip &lt;- left_join(mpsz, \n                           origin_data,\n                           by = c(\"SUBZONE_C\" = \"ORIGIN_SZ\"))\n\nBefore proceeding, let’s save this data as rds.\n\nwrite_rds(origin7_9, \"data/rds/origin7_9.rds\")"
  },
  {
    "objectID": "In-class_Ex1/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex1/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take-home_Ex2/data/geospatial/Liesure&Recreation.html",
    "href": "Take-home_Ex2/data/geospatial/Liesure&Recreation.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Take-home_Ex2/data/geospatial/entertn.html",
    "href": "Take-home_Ex2/data/geospatial/entertn.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Take-home_Ex2/data/geospatial/FinServ.html",
    "href": "Take-home_Ex2/data/geospatial/FinServ.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Take-home_Ex2/Take-home_Ex2.html",
    "href": "Take-home_Ex2/Take-home_Ex2.html",
    "title": "Take Home Exercise 2: Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows",
    "section": "",
    "text": "pacman::p_load(sf, sp, tmap, tidyverse, knitr, sfdep, stplanr, reshape2)\ntmap_mode(\"plot\")\ntmap_style(\"natural\")\nset.seed(1234)"
  },
  {
    "objectID": "Take-home_Ex2/Take-home_Ex2.html#import-geospatial-data",
    "href": "Take-home_Ex2/Take-home_Ex2.html#import-geospatial-data",
    "title": "Take Home Exercise 2: Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows",
    "section": "Import Geospatial Data",
    "text": "Import Geospatial Data\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                layer = \"MPSZ-2019\") %&gt;% st_transform(crs=3414)\n\nReading layer `MPSZ-2019' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/Take-home_Ex2/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nbusstops &lt;- st_read(dsn = \"data/geospatial\",\n                    layer = \"BusStop\") %&gt;% st_transform(crs=3414)\n\nReading layer `BusStop' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/Take-home_Ex2/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\nkable(head(busstops))\n\n\n\n\n\n\n\n\n\n\nBUS_STOP_N\nBUS_ROOF_N\nLOC_DESC\ngeometry\n\n\n\n\n22069\nB06\nOPP CEVA LOGISTICS\nPOINT (13576.31 32883.65)\n\n\n32071\nB23\nAFT TRACK 13\nPOINT (13228.59 44206.38)\n\n\n44331\nB01\nBLK 239\nPOINT (21045.1 40242.08)\n\n\n96081\nB05\nGRACE INDEPENDENT CH\nPOINT (41603.76 35413.11)\n\n\n11561\nB05\nBLK 166\nPOINT (24568.74 30391.85)\n\n\n66191\nB03\nAFT CORFE PL\nPOINT (30951.58 38079.61)\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(\"lightgreen\", title = \"Singapore Boundary\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Map of bus stops in Singapore\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_shape(busstops) +\n  tm_dots(col = \"red\", size = 0.005, title = \"Bus Stops\") +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nFilter those only within Singapore\n\nbusstops &lt;- busstops %&gt;% st_intersection(mpsz) %&gt;% select(BUS_STOP_N, )\n\n\n\nShow the code\ntmap_style(\"natural\")\ntm_shape(mpsz) +\n  tm_fill(\"lightgreen\", title = \"Singapore Boundary\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Map of bus stops in Singapore\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_shape(busstops) +\n  tm_dots(col = \"red\", size = 0.005, title = \"Bus Stops\") +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)"
  },
  {
    "objectID": "Take-home_Ex2/Take-home_Ex2.html#create-hexagon",
    "href": "Take-home_Ex2/Take-home_Ex2.html#create-hexagon",
    "title": "Take Home Exercise 2: Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows",
    "section": "Create Hexagon",
    "text": "Create Hexagon\n\nhoneycomb &lt;- busstops %&gt;% st_make_grid(cellsize = 650,\n                                       what=\"polygons\",\n                                       square = FALSE) %&gt;%\n  st_sf() %&gt;%\n  filter(lengths(st_intersects(geometry, busstops)) &gt; 0)\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(\"green\", title = \"Singapore Boundary\", alpha = 0.5) +\n  tm_shape(honeycomb) +\n  tm_fill(col = \"white\", title = \"Hexagons\", alpha = 1) +\n  tm_borders(alpha = 0.2) +\n  tm_layout(main.title = \"Honeycomb grid corresponding to Singapore bus stops\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2, bg.color = \"white\", bg.alpha = 0.5) +\n  tm_scale_bar(bg.color = \"white\", bg.alpha = 0.5) +\n  tm_shape(busstops) +\n  tm_dots(col = \"red\", size = 0.001, title = \"Bus Stops\") +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nAssign id\n\nhoneycomb$HEX_ID &lt;- sprintf(\"H%04d\", seq_len(nrow(honeycomb))) %&gt;% as.factor()\n\n\nwrite_rds(honeycomb, \"data/rds/honeycomb.rds\")"
  },
  {
    "objectID": "Take-home_Ex2/Take-home_Ex2.html#generating-origin-and-destination-data-by-hexagon",
    "href": "Take-home_Ex2/Take-home_Ex2.html#generating-origin-and-destination-data-by-hexagon",
    "title": "Take Home Exercise 2: Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows",
    "section": "Generating Origin and Destination Data by Hexagon",
    "text": "Generating Origin and Destination Data by Hexagon"
  },
  {
    "objectID": "Take-home_Ex2/Take-home_Ex2.html#importing-bus-passenger-data",
    "href": "Take-home_Ex2/Take-home_Ex2.html#importing-bus-passenger-data",
    "title": "Take Home Exercise 2: Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows",
    "section": "Importing Bus Passenger data",
    "text": "Importing Bus Passenger data\nThe data set is an aspatial data in csv format so we will use read_csv() to import the data.\n\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202310.csv\")\nkable(head(odbus))\n\n\n\n\n\n\n\n\n\n\n\n\n\nYEAR_MONTH\nDAY_TYPE\nTIME_PER_HOUR\nPT_TYPE\nORIGIN_PT_CODE\nDESTINATION_PT_CODE\nTOTAL_TRIPS\n\n\n\n\n2023-10\nWEEKENDS/HOLIDAY\n16\nBUS\n04168\n10051\n3\n\n\n2023-10\nWEEKDAY\n16\nBUS\n04168\n10051\n5\n\n\n2023-10\nWEEKENDS/HOLIDAY\n14\nBUS\n80119\n90079\n3\n\n\n2023-10\nWEEKDAY\n14\nBUS\n80119\n90079\n5\n\n\n2023-10\nWEEKDAY\n17\nBUS\n44069\n17229\n4\n\n\n2023-10\nWEEKENDS/HOLIDAY\n17\nBUS\n20281\n20141\n1\n\n\n\n\n\n\nFiltering the study interest\nWeekend morning\n\nodbus &lt;- odbus %&gt;% filter(\n  DAY_TYPE == \"WEEKENDS/HOLIDAY\" &\n    TIME_PER_HOUR &gt;= 11 &\n    TIME_PER_HOUR &lt; 14\n)\n\n\n\nMapping Bus Stop to Hexagon\n\nbs_hex &lt;- st_intersection(busstops, honeycomb) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(c(BUS_STOP_N, HEX_ID))\nkable(head(bs_hex))\n\n\n\n\n\nBUS_STOP_N\nHEX_ID\n\n\n\n\n3269\n25059\nH0001\n\n\n3269.1\n25059\nH0002\n\n\n254\n26379\nH0003\n\n\n2570\n25751\nH0004\n\n\n2897\n25761\nH0004\n\n\n2403\n26369\nH0005\n\n\n\n\n\nWhat if bus stop is in 2 hexagons?\n\nA bus stop is included in 2 hexagons\nWe have to remove one of them.\n\nkable(head(bs_hex))\n\n\n\n\n\nBUS_STOP_N\nHEX_ID\n\n\n\n\n3269\n25059\nH0001\n\n\n3269.1\n25059\nH0002\n\n\n254\n26379\nH0003\n\n\n2570\n25751\nH0004\n\n\n2897\n25761\nH0004\n\n\n2403\n26369\nH0005\n\n\n\n\n\n\ntm_shape(honeycomb[1:10,]) +\n  tm_fill(col = \"white\", title = \"Hexagons\", alpha = 1) +\n  tm_text(\"HEX_ID\", size = 0.8) +\n  tm_borders(alpha = 0.2) +\n  tm_layout(main.title = \"Map showing bus stops in 2 hexagons\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_scale_bar(bg.color = \"white\", bg.alpha = 0.5, position = c(\"left\", \"top\")) +\n  tm_shape(busstops %&gt;% filter(BUS_STOP_N == \"25059\")) +\n  tm_dots(col = \"red\", size = 0.05, title = \"Bus Stops\") +\n  tm_grid(alpha = 0.2)\n\n\n\n\nThe bus stop is right on the boundary of H0001 and H0002. If we remove H0002, H0001 will be an island so let us remove H0001. This way, H0002 is still connected to another hexagon. Let’s also re-assign the ids of the hexagon.\n\nhoneycomb &lt;- honeycomb %&gt;% filter(HEX_ID != \"H0001\")\nhoneycomb$HEX_ID &lt;- sprintf(\"H%04d\", seq_len(nrow(honeycomb))) %&gt;% as.factor()\n\n\ntm_shape(honeycomb[1:10,]) +\n  tm_fill(col = \"white\", title = \"Hexagons\", alpha = 1) +\n  tm_text(\"HEX_ID\", size = 0.8) +\n  tm_borders(alpha = 0.2) +\n  tm_layout(main.title = \"Honeycomb grid corresponding to Singapore bus stops\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_scale_bar(bg.color = \"white\", bg.alpha = 0.5, position = c(\"left\", \"top\")) +\n  tm_shape(busstops %&gt;% filter(BUS_STOP_N == \"25059\")) +\n  tm_dots(col = \"red\", size = 0.05, title = \"Bus Stops\") +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nbs_hex &lt;- st_intersection(busstops, honeycomb) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(c(BUS_STOP_N, HEX_ID))\nkable(head(bs_hex))\n\n\n\n\n\nBUS_STOP_N\nHEX_ID\n\n\n\n\n3269\n25059\nH0001\n\n\n254\n26379\nH0002\n\n\n2570\n25751\nH0003\n\n\n2897\n25761\nH0003\n\n\n2403\n26369\nH0004\n\n\n1565\n26299\nH0005\n\n\n\n\n\n\n\n\nAdding hexagon information to bus passenger data\ninner_join() as some bus stops in odbus do not have point data.\n\nodbushex &lt;- odbus %&gt;% inner_join(bs_hex,\n                                by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIG_HEX_ID = HEX_ID) %&gt;% \n  inner_join(bs_hex,\n            by = c(\"DESTINATION_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(DEST_HEX_ID = HEX_ID) %&gt;% \n  rename(HOUR_OF_DAY = TIME_PER_HOUR)"
  },
  {
    "objectID": "Take-home_Ex2/Take-home_Ex2.html#aggregating-trips-based-on-origin-and-destination",
    "href": "Take-home_Ex2/Take-home_Ex2.html#aggregating-trips-based-on-origin-and-destination",
    "title": "Take Home Exercise 2: Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows",
    "section": "Aggregating trips based on origin and destination",
    "text": "Aggregating trips based on origin and destination\n\nodtrips &lt;- odbushex %&gt;% group_by(\n    ORIG_HEX_ID,\n    DEST_HEX_ID) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\nkable(head(odtrips))\n\n\n\n\nORIG_HEX_ID\nDEST_HEX_ID\nTRIPS\n\n\n\n\nH0002\nH0025\n2\n\n\nH0002\nH0032\n27\n\n\nH0002\nH0076\n12\n\n\nH0003\nH0007\n4\n\n\nH0003\nH0010\n12\n\n\nH0003\nH0019\n2\n\n\n\n\n\n\nrm(busstops)\nrm(odbus)\nrm(odbushex)"
  },
  {
    "objectID": "Take-home_Ex2/Take-home_Ex2.html#choosing-visualization-interval",
    "href": "Take-home_Ex2/Take-home_Ex2.html#choosing-visualization-interval",
    "title": "Take Home Exercise 2: Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows",
    "section": "Choosing visualization interval",
    "text": "Choosing visualization interval\n\nquantile(\n  flowline$TRIPS,\n  probs = c(0, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999, 1))\n\n       0%       25%       50%       75%       90%       95%       99%     99.9% \n    1.000     3.000    10.000    39.000   138.000   296.000  1179.720  4037.008 \n     100% \n12392.000 \n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(\"lightgreen\", title = \"Singapore Boundary\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Map of bus stops in Singapore\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_shape(honeycomb) +\n  tm_fill(col = \"white\", title = \"Hexagons\", alpha = 1) +\n  tm_borders(alpha = 0.2) +\n  \n  tm_shape(flowline[flowline$TRIPS &gt; 1179,]) +\n  tm_lines(lwd = \"TRIPS\",\n           col = \"red\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3) +\n  \n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)"
  },
  {
    "objectID": "Take-home_Ex2/Take-home_Ex2.html#converting-honeycomb-to-spatialpolygonsdataframe",
    "href": "Take-home_Ex2/Take-home_Ex2.html#converting-honeycomb-to-spatialpolygonsdataframe",
    "title": "Take Home Exercise 2: Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows",
    "section": "Converting honeycomb to SpatialPolygonsDataFrame",
    "text": "Converting honeycomb to SpatialPolygonsDataFrame\n\nhoneycomb_sp &lt;- as(honeycomb, \"Spatial\")"
  },
  {
    "objectID": "Take-home_Ex2/Take-home_Ex2.html#generating-distance-matrix",
    "href": "Take-home_Ex2/Take-home_Ex2.html#generating-distance-matrix",
    "title": "Take Home Exercise 2: Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows",
    "section": "Generating distance matrix",
    "text": "Generating distance matrix\n\ndist_mat &lt;- spDists(honeycomb_sp,\n                    longlat = FALSE)\ncolnames(dist_mat) &lt;- paste0(honeycomb$HEX_ID)\nrownames(dist_mat) &lt;- paste0(honeycomb$HEX_ID)\nkable(head(dist_mat, n=c(10, 10)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nH0001\nH0002\nH0003\nH0004\nH0005\nH0006\nH0007\nH0008\nH0009\nH0010\n\n\n\n\nH0001\n0.000\n2251.666\n650.000\n2833.284\n3953.796\n1300.000\n2343.608\n4550.000\n1950.000\n2978.674\n\n\nH0002\n2251.666\n0.000\n1719.738\n650.000\n1719.738\n1300.000\n650.000\n2343.608\n1125.833\n1125.833\n\n\nH0003\n650.000\n1719.738\n0.000\n2251.666\n3377.499\n650.000\n1719.738\n3953.796\n1300.000\n2343.608\n\n\nH0004\n2833.284\n650.000\n2251.666\n0.000\n1125.833\n1719.738\n650.000\n1719.738\n1300.000\n650.000\n\n\nH0005\n3953.796\n1719.738\n3377.499\n1125.833\n0.000\n2833.284\n1719.738\n650.000\n2343.608\n1300.000\n\n\nH0006\n1300.000\n1300.000\n650.000\n1719.738\n2833.284\n0.000\n1125.833\n3377.499\n650.000\n1719.738\n\n\nH0007\n2343.608\n650.000\n1719.738\n650.000\n1719.738\n1125.833\n0.000\n2251.666\n650.000\n650.000\n\n\nH0008\n4550.000\n2343.608\n3953.796\n1719.738\n650.000\n3377.499\n2251.666\n0.000\n2833.284\n1719.738\n\n\nH0009\n1950.000\n1125.833\n1300.000\n1300.000\n2343.608\n650.000\n650.000\n2833.284\n0.000\n1125.833\n\n\nH0010\n2978.674\n1125.833\n2343.608\n650.000\n1300.000\n1719.738\n650.000\n1719.738\n1125.833\n0.000"
  },
  {
    "objectID": "Take-home_Ex2/Take-home_Ex2.html#creating-pivot-table",
    "href": "Take-home_Ex2/Take-home_Ex2.html#creating-pivot-table",
    "title": "Take Home Exercise 2: Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows",
    "section": "Creating pivot table",
    "text": "Creating pivot table\n300 because of hexagon dimensions\n\ndist_tbl &lt;- melt(dist_mat) %&gt;%\n  rename(distance = value)\ndist_tbl$distance[dist_tbl$distance == 0] &lt;- 300\nkable(head(dist_tbl))\n\n\n\n\nVar1\nVar2\ndistance\n\n\n\n\nH0001\nH0001\n300.000\n\n\nH0002\nH0001\n2251.666\n\n\nH0003\nH0001\n650.000\n\n\nH0004\nH0001\n2833.284\n\n\nH0005\nH0001\n3953.796\n\n\nH0006\nH0001\n1300.000"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1B.html",
    "href": "Hands-on_Ex1/Hands-on_Ex1B.html",
    "title": "Hands on Exercise 1A: Choropleth Mapping with R",
    "section": "",
    "text": "This hands-on exercise covers Chapter 2: Choropleth Mapping with R.\nI learned about the following:\n\nCreating thematic/choropleth maps with tmap\nQuantile and equal classification"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1B.html#preparing-the-datasets",
    "href": "Hands-on_Ex1/Hands-on_Ex1B.html#preparing-the-datasets",
    "title": "Hands on Exercise 1A: Choropleth Mapping with R",
    "section": "Preparing the datasets",
    "text": "Preparing the datasets\n\nGeospatial\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\n\n\n\nAspatial\n\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 from Department of Statistics, Singapore\n\nNext, is putting them under the Hands-on_Ex1 directory, with the following file structure:\nHands-on_Ex1\n└── data\n    ├── aspatial\n    │   └── respopagesextod2011to2020.csv\n    └── geospatial\n        ├── MP14_SUBZONE_WEB_PL.dbf\n        ├── MP14_SUBZONE_WEB_PL.prj\n        ├── MP14_SUBZONE_WEB_PL.sbn\n        ├── MP14_SUBZONE_WEB_PL.sbx\n        ├── MP14_SUBZONE_WEB_PL.shp\n        ├── MP14_SUBZONE_WEB_PL.shp.xml\n        └── MP14_SUBZONE_WEB_PL.shx"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1B.html#installing-r-packages",
    "href": "Hands-on_Ex1/Hands-on_Ex1B.html#installing-r-packages",
    "title": "Hands on Exercise 1A: Choropleth Mapping with R",
    "section": "Installing R packages",
    "text": "Installing R packages\nI used the code below to install the R packages used in the exercise:\n\npacman::p_load(sf, tidyverse, tmap)"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1B.html#importing-geospatial-data",
    "href": "Hands-on_Ex1/Hands-on_Ex1B.html#importing-geospatial-data",
    "title": "Hands on Exercise 1A: Choropleth Mapping with R",
    "section": "Importing geospatial data",
    "text": "Importing geospatial data\nTo import the MPSZ data set to RStudio, I used the same code chunk in the previous exercise.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/Hands-on_Ex1/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1B.html#importing-aspatial-data",
    "href": "Hands-on_Ex1/Hands-on_Ex1B.html#importing-aspatial-data",
    "title": "Hands on Exercise 1A: Choropleth Mapping with R",
    "section": "Importing aspatial data",
    "text": "Importing aspatial data\nThe csv data is an aspatial data so read_csv() must be used instead of st_read():\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1B.html#data-preparation",
    "href": "Hands-on_Ex1/Hands-on_Ex1B.html#data-preparation",
    "title": "Hands on Exercise 1A: Choropleth Mapping with R",
    "section": "Data preparation",
    "text": "Data preparation\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\nI don’t fully understand this entire code chunk yet but I know it filtered for data from 2020 only and did some aggregations based on the age groups, PA, and AZ. New fields like YOUNG and AGED .\nNext, I joined this data with the mpsz data via SZ. However, we still need to make sure that the SZ values are uppercase to match mpsz’s SUBZONE_N.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nThen the 2 data sets can be joined.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nLastly, write the rds of the combined data set.\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n⚠️ This failed on the first try so I had to create the rds directory under data/ before trying again."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1B.html#using-qtm",
    "href": "Hands-on_Ex1/Hands-on_Ex1B.html#using-qtm",
    "title": "Hands on Exercise 1A: Choropleth Mapping with R",
    "section": "Using qtm()",
    "text": "Using qtm()\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\nI tried to create an interactive map by using view instead of plot but an error about invalid polygons was returned."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1B.html#using-tmap-elements",
    "href": "Hands-on_Ex1/Hands-on_Ex1B.html#using-tmap-elements",
    "title": "Hands on Exercise 1A: Choropleth Mapping with R",
    "section": "Using tmap() elements",
    "text": "Using tmap() elements\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nFrom this part, I saw that with tm_shape as base, thematic maps can be created by doing + with the tmap actions. I will explore this later but I found a good reference on where to start: https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1B.html#classification-methods",
    "href": "Hands-on_Ex1/Hands-on_Ex1B.html#classification-methods",
    "title": "Hands on Exercise 1A: Choropleth Mapping with R",
    "section": "Classification methods",
    "text": "Classification methods\n\nQuantile\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nEqual\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nMy main takeaway for this is that the quantile classification deals with outliers better. In this data set, there is an outlier subzone. If we use equal classification, the map looks homogeneous and does not provide much information as it cannot be seen how the values from one subzone to the other differ.\nWith quantile classification, these differences can be seen more easily despite the outlier value."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1B.html#color-scheme",
    "href": "Hands-on_Ex1/Hands-on_Ex1B.html#color-scheme",
    "title": "Hands on Exercise 1A: Choropleth Mapping with R",
    "section": "Color Scheme",
    "text": "Color Scheme\nThe color scheme can be changed by specifying the palette in tm_fill() like below:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nMore colors can be found when running tmaptools::palette_explorer() in the console. However, it requires shiny and shinyjs to work. This is a wonderful tool as it can also simulate how the color schemes look from the perspective of people with color blindness. As these maps aim to communicate, it is important for the color schemes chosen to not just be beautiful, but also inclusive."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1B.html#map-layouts",
    "href": "Hands-on_Ex1/Hands-on_Ex1B.html#map-layouts",
    "title": "Hands on Exercise 1A: Choropleth Mapping with R",
    "section": "Map Layouts",
    "text": "Map Layouts\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nAdding legends to maps is very useful as it provides additional information. However, this feature should not be abused to add multiple visualizations as legend as it can cause more confusion if there is too much information to present.\nMap style can also be changed and it is useful to enhance the visual presentation. After doing some research, I found the other available styles in https://cran.r-project.org/web/packages/tmap/vignettes/tmap-changes.html.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tmap_style(\"natural\")\n\n\n\n\nIn this example, I used the natural style to make it look like the map is surrounded by water (as it is naturally). Adding “furnitures” like compass and scale can also provide more perspective."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1B.html#smaller-maps",
    "href": "Hands-on_Ex1/Hands-on_Ex1B.html#smaller-maps",
    "title": "Hands on Exercise 1A: Choropleth Mapping with R",
    "section": "Smaller Maps",
    "text": "Smaller Maps\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\nI don’t find this very useful as the maps rendered might be too small to inspect. However, the facets with the region can be useful to see the data based on region.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Greens\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1B.html#mapping-according-to-criterion",
    "href": "Hands-on_Ex1/Hands-on_Ex1B.html#mapping-according-to-criterion",
    "title": "Hands on Exercise 1A: Choropleth Mapping with R",
    "section": "Mapping According to Criterion",
    "text": "Mapping According to Criterion\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Greens\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nI find this very useful especially if we just want to map a subset of the data. This can be used when we want to highlight information on certain regions."
  },
  {
    "objectID": "In-class_Ex4/In-class_Ex4.html",
    "href": "In-class_Ex4/In-class_Ex4.html",
    "title": "In-class Ex 4",
    "section": "",
    "text": "Setup Environment\nhttr - work with HTML pages\n\npacman::p_load(tidyverse, sf, httr, tmap)\n\n\n\nGeocoding\nUse https://www.onemap.gov.sg/apidocs/\nX, Y in SVY21, longitude latitude in WGS84\n\nurl &lt;- \"https://www.onemap.gov.sg/api/common/elastic/search\"\n\ncsv &lt;- read_csv(\"data/aspatial/GeneralInformationofschools.csv\")\npostcodes &lt;- csv$\"postal_code\"\n\nfound &lt;- data.frame()\nnot_found &lt;- data.frame()\n\nfor(postcode in postcodes){\n  query &lt;- list(\"searchVal\"=postcode,\"returnGeom\"=\"Y\", \"getAddrDetails\"=\"Y\",\"pageNum\"=\"1\")\n  res&lt;- GET(url, query=query)\n  \n  if((content(res)$found) !=0){\n    found &lt;- rbind(found, data.frame(content(res))[4:13])\n  } else {\n    not_found = data.frame(postcode)\n  }\n}\n\nwrite_rds(found, \"data/rds/found.rds\")\nwrite_rds(not_found, \"data/rds/not_found.rds\")\n\n\ncsv &lt;- read_csv(\"data/aspatial/GeneralInformationofschools.csv\")\nfound &lt;- read_rds(\"data/rds/found.rds\")\nnot_found &lt;- read_rds(\"data/rds/not_found.rds\")\nmerged = merge(csv, found, by.x = \"postal_code\", by.y = \"results.POSTAL\", all = TRUE)\nwrite.csv(merged, file = \"data/aspatial/schools.csv\")\nwrite.csv(not_found, file = \"data/aspatial/not_found.csv\")\n\n\n\n\n\n\n\nImportant\n\n\n\nUse Google to look for the school without geospatial data.\nReplace latitude with 1.3887, longitude with 103.7652.\nDo this before proceeding to the next step. Or else st_as_sf() will complain about missing longitude and latitude\n\n\n\n\n\n\n\n\n\nImport schools.csv\nRename column names\nRetain only relevant columns\n\n\n\n\n\nschools &lt;- read_csv(\"data/aspatial/schools.csv\") %&gt;%\n  rename(latitude = results.LATITUDE,\n         longitude = results.LONGITUDE) %&gt;%\n  select(postal_code, school_name, latitude, longitude)\n\nNew names:\nRows: 350 Columns: 41\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(35): school_name, url_address, address, telephone_no, telephone_no_2, f... dbl\n(6): ...1, postal_code, results.X, results.Y, results.LATITUDE, results...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\nschools_sf &lt;- st_as_sf(schools,\n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)"
  },
  {
    "objectID": "In-class_Ex3/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex3/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2C.html",
    "href": "In-class_Ex2/In-class_Ex2C.html",
    "title": "In-class Exercise 2C: EHSA",
    "section": "",
    "text": "For this exercise, we will do spatio-temporal analysis to understand spatial patterns with additional factor of time."
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2C.html#importing-the-data",
    "href": "In-class_Ex2/In-class_Ex2C.html#importing-the-data",
    "title": "In-class Exercise 2C: EHSA",
    "section": "Importing the data",
    "text": "Importing the data\nFirst, we will import the geospatial data in shp format.\n\nhunan = st_read(dsn = \"data/geospatial\",\n                layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/In-class_Ex2/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nSecond, we import the aspatial data Hunan_GDPPC, which contains the GDP Per Capita (GDPPC) of Chinese counties.\n\nGDPPC = read_csv(\"data/aspatial/Hunan_GDPPC.csv\")"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2C.html#computing-gi",
    "href": "In-class_Ex2/In-class_Ex2C.html#computing-gi",
    "title": "In-class Exercise 2C: EHSA",
    "section": "Computing Gi*",
    "text": "Computing Gi*\n\nDeriving spatial weights\nSimilar to the previous exercises, we calculate inverse distance first. However, we now have a time column in our data which is Year.\n\nGDPPC_nb &lt;- GDPPC_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\nGDPPC_nb\n\n# A tibble: 1,496 × 5\n    Year County    GDPPC nb        wt       \n   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;list&gt;    &lt;list&gt;   \n 1  2005 Anxiang    8184 &lt;int [6]&gt; &lt;dbl [6]&gt;\n 2  2005 Hanshou    6560 &lt;int [6]&gt; &lt;dbl [6]&gt;\n 3  2005 Jinshi     9956 &lt;int [5]&gt; &lt;dbl [5]&gt;\n 4  2005 Li         8394 &lt;int [5]&gt; &lt;dbl [5]&gt;\n 5  2005 Linli      8850 &lt;int [5]&gt; &lt;dbl [5]&gt;\n 6  2005 Shimen     9244 &lt;int [6]&gt; &lt;dbl [6]&gt;\n 7  2005 Liuyang   13406 &lt;int [5]&gt; &lt;dbl [5]&gt;\n 8  2005 Ningxiang 11687 &lt;int [8]&gt; &lt;dbl [8]&gt;\n 9  2005 Wangcheng 14659 &lt;int [7]&gt; &lt;dbl [7]&gt;\n10  2005 Anren      7423 &lt;int [9]&gt; &lt;dbl [9]&gt;\n# ℹ 1,486 more rows\n\n\n\n\nComputing local Gi*\n\ngi_stars &lt;- GDPPC_nb %&gt;%\n  group_by(Year) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    GDPPC, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)\ngi_stars\n\n# A tibble: 1,496 × 13\n# Groups:   Year [17]\n    Year County    GDPPC nb        wt     gi_star   e_gi  var_gi p_value   p_sim\n   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;list&gt;    &lt;list&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  2005 Anxiang    8184 &lt;int [6]&gt; &lt;dbl&gt;    0.398 0.0117 3.09e-6  0.241  0.810  \n 2  2005 Hanshou    6560 &lt;int [6]&gt; &lt;dbl&gt;   -0.237 0.0110 2.52e-6 -0.0306 0.976  \n 3  2005 Jinshi     9956 &lt;int [5]&gt; &lt;dbl&gt;    1.05  0.0127 3.66e-6  0.458  0.647  \n 4  2005 Li         8394 &lt;int [5]&gt; &lt;dbl&gt;    0.966 0.0117 2.99e-6  0.992  0.321  \n 5  2005 Linli      8850 &lt;int [5]&gt; &lt;dbl&gt;    1.05  0.0120 2.75e-6  0.944  0.345  \n 6  2005 Shimen     9244 &lt;int [6]&gt; &lt;dbl&gt;    0.210 0.0120 2.63e-6 -0.141  0.888  \n 7  2005 Liuyang   13406 &lt;int [5]&gt; &lt;dbl&gt;    3.91  0.0143 3.03e-6  3.00   0.00270\n 8  2005 Ningxiang 11687 &lt;int [8]&gt; &lt;dbl&gt;    1.61  0.0128 2.43e-6  0.833  0.405  \n 9  2005 Wangcheng 14659 &lt;int [7]&gt; &lt;dbl&gt;    3.88  0.0140 2.56e-6  2.69   0.00715\n10  2005 Anren      7423 &lt;int [9]&gt; &lt;dbl&gt;    1.67  0.0112 2.08e-6  1.92   0.0551 \n# ℹ 1,486 more rows\n# ℹ 3 more variables: p_folded_sim &lt;dbl&gt;, skewness &lt;dbl&gt;, kurtosis &lt;dbl&gt;"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2C.html#arrange-to-show-significant-emerging-hotcold-spots",
    "href": "In-class_Ex2/In-class_Ex2C.html#arrange-to-show-significant-emerging-hotcold-spots",
    "title": "In-class Exercise 2C: EHSA",
    "section": "Arrange to show significant emerging hot/cold spots",
    "text": "Arrange to show significant emerging hot/cold spots\n\nemerging &lt;- ehsa %&gt;% \n  arrange(sl, abs(tau)) %&gt;% \n  slice(1:5)\nemerging\n\n# A tibble: 5 × 6\n  County        tau         sl     S     D  varS\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Shuangfeng  0.868 0.00000143   118  136.  589.\n2 Xiangtan    0.868 0.00000143   118  136.  589.\n3 Xiangxiang  0.868 0.00000143   118  136.  589.\n4 Chengbu    -0.824 0.00000482  -112  136.  589.\n5 Dongan     -0.824 0.00000482  -112  136.  589."
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2C.html#performing-emerging-hot-spot-analysis",
    "href": "In-class_Ex2/In-class_Ex2C.html#performing-emerging-hot-spot-analysis",
    "title": "In-class Exercise 2C: EHSA",
    "section": "Performing Emerging Hot spot Analysis",
    "text": "Performing Emerging Hot spot Analysis\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = GDPPC_st,\n  .var = \"GDPPC\",\n  k = 1,\n  nsim = 99\n)\n\n\nVisualizing the distribution of EHSA classes\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar()\n\n\n\n\nFigure above shows that sporadic cold spots class has the high numbers of county. Visualizing EHSA\nTo generate a map, we have to add geospatial component to the data to we have join ehsa with hunan.\n\nhunan_ehsa &lt;- hunan %&gt;%\n  left_join(ehsa,\n            by = join_by(County == location))\n\nThen we can finally generate the map.\n\nehsa_sig &lt;- hunan_ehsa  %&gt;%\n  filter(p_value &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(hunan_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1B.html",
    "href": "Take-home_Ex1/Take-home_Ex1B.html",
    "title": "Take Home Exercise 1B: Geovisualization and Analysis",
    "section": "",
    "text": "The aim of this study is to uncover spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.\nThe main modes of analysis to be used here are Local Indicators of Spatial Association (GLISA) and Emerging Hot Spot Analysis (EHSA).\nIn doing these study, we will be looking at bus trips started during the hours below.\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday evening peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nMore details about the study can be found here.\nIn this part of the study, we will do thematic mapping on the bus commuter traffic data generated from Data Wrangling."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1B.html#setting-up-the-r-environment",
    "href": "Take-home_Ex1/Take-home_Ex1B.html#setting-up-the-r-environment",
    "title": "Take Home Exercise 1B: Geovisualization and Analysis",
    "section": "Setting Up the R Environment",
    "text": "Setting Up the R Environment\nWe will load the following R packages needed for this study.\n\ntmap: for thematic mapping\nsf: for geospatial data handling\ntidyverse: for non-spatial data handling\nknitr:for prettifying presentation\n\n\npacman::p_load(sf, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1B.html#environment-settings",
    "href": "Take-home_Ex1/Take-home_Ex1B.html#environment-settings",
    "title": "Take Home Exercise 1B: Geovisualization and Analysis",
    "section": "Environment settings",
    "text": "Environment settings\nWe will also set the default settings on for this document\n\ntmap_style to natural: for displaying the maps with preferred style\n\n\ntmap_mode(\"plot\")\ntmap_style(\"natural\")"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1B.html#loading-the-data",
    "href": "Take-home_Ex1/Take-home_Ex1B.html#loading-the-data",
    "title": "Take Home Exercise 1B: Geovisualization and Analysis",
    "section": "Loading the data",
    "text": "Loading the data\n\n\n\n\n\n\nImportant\n\n\n\nBefore running this part, please run all the code chunks in Data Wrangling as it generates the data needed for this document.\n\n\nUse read_rds() to load the rds data needed for geovisualization and analysis.\n\nmpsz &lt;- read_rds(\"data/rds/mpsz.rds\")\nhoneycomb &lt;- read_rds(\"data/rds/honeycomb202310.rds\")\ntrips_cube_wkdy &lt;- read_rds(\"data/rds/trips_cube_wkdy202310.rds\")\ntrips_cube_wknd &lt;- read_rds(\"data/rds/trips_cube_wknd202310.rds\")\n\n\nmpsz - contains Singapore subzone boundaries, will be used for visualizations\nhoneycomb - contains the geometry for the honeycomb grid\ntrips_cube_wkdy - hourly number bus trips originating from hexagons during weekday\ntrips_cube_wknd - hourly number bus trips originating from hexagons during weekend\n\n\n\n\n\n\n\nIf you want to use the August 2023 or September 2023 data sets, replace 202310 to 202308 or 202309, respectively."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1B.html#initial-look-at-the-data",
    "href": "Take-home_Ex1/Take-home_Ex1B.html#initial-look-at-the-data",
    "title": "Take Home Exercise 1B: Geovisualization and Analysis",
    "section": "Initial look at the data",
    "text": "Initial look at the data\nLet us plot maps for each peak period. I’m using tabsets for this so we can see the differences in data when switching from 1 tab to the other\n\nWeekday AM (6 - 9 AM)Weekday PM (5 - 8 PM)Weekend AM (11 AM - 2 PM)Weekend PM (4 - 7 PM)\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKDAY_AM_TRIPS\",\n    style = \"quantile\",\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekdays 6 - 9 AM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKDAY_PM_TRIPS\",\n    style = \"quantile\",\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekdays 5 - 8 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_AM_TRIPS\",\n    style = \"quantile\",\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 11 AM - 2 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_PM_TRIPS\",\n    style = \"quantile\",\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 4 - 7 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nThe visualizations above are satisfactory if we look at them individually as we can see which areas are busier than the others.\nHowever, comparing the commuter patterns from a peak period to another can be misleading.\nTo illustrate, consider the values of the darkest red on weekends. Hexagons with 6000 trips already are dark red on weekend maps. However, these values fall under the middle category, visualized as orange, on weekdays.\nDue to this, one might misinterpret that an area is busier on certain peak periods due to the difference in colors when in fact, they are actually as busy or even less busy. Colors are powerful and easier to interpret than looking at the categories in the legend.\nAs it is, we are comparing apples to oranges. We need a way to compare these maps apples to apples."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1B.html#deriving-the-break-points",
    "href": "Take-home_Ex1/Take-home_Ex1B.html#deriving-the-break-points",
    "title": "Take Home Exercise 1B: Geovisualization and Analysis",
    "section": "Deriving the break points",
    "text": "Deriving the break points\nTo compare the maps apples to apples, we need to calculate the break points that our maps can use. When tmap uses style = quantile, it calculates the styles depending on the number of categories.\nWe can replicate this by using quantile() and using the full range of data from all the peak periods. To see the differences in more detail, we will use 8 categories instead of the default 5 categories.\n\n\n\n\n\n\nHow to calculate probs?\n\n\n\n\n\nSince we want 8 categories, we will divide 100 by 8.\n\\[\n\\frac{100}{8} = 12.5\n\\]\nHence, we will supply multiples of 0.125 as probs, e.g. 0, 0.125, 0.25, … 1\n\n\n\n\nquantile(\n  c(\n    peak_trips_sf$WEEKDAY_AM_TRIPS,\n    peak_trips_sf$WEEKDAY_PM_TRIPS,\n    peak_trips_sf$WEEKEND_AM_TRIPS,\n    peak_trips_sf$WEEKEND_PM_TRIPS\n  ),\n  probs = c(0, 0.125, 0.25, 0.375, 0.50, 0.625, 0.75, 0.875, 1)\n)\n\n        0%      12.5%        25%      37.5%        50%      62.5%        75% \n     0.000    165.000    631.750   1544.375   2985.000   5275.875   9179.750 \n     87.5%       100% \n 17850.750 462160.000 \n\n\nWith this result, instead of using style = quantile in our maps, we can specify these values to breaks.\n\n\n\n\n\n\nWhy not just use summary()?\n\n\n\n\n\nsummary() uses quartiles or every 25th quantile. This results in only 4 categories. This is not enough for the level detail we want to present so it’s better to use quantile() to generate more categories ourselves."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1B.html#remapping-with-the-breaks",
    "href": "Take-home_Ex1/Take-home_Ex1B.html#remapping-with-the-breaks",
    "title": "Take Home Exercise 1B: Geovisualization and Analysis",
    "section": "Remapping with the breaks",
    "text": "Remapping with the breaks\nWe will supply the values generated (rounded to the nearest integer) to breaks, instead of style = \"quantile\".\nbreaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160)\n\nWeekday AM (6 - 9 AM)Weekday PM (5 - 8 PM)Weekend AM (11 AM - 2 PM)Weekend PM (4 - 7 PM)\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKDAY_AM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekdays 6 - 9 AM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nThere is a stark difference is colors in this map, showing a concentration of places with the darkest reds, along with areas of lighter colors.\nWe may infer that this concentration could be from residential areas due to people leaving their homes for their daily activities like work or school.\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKDAY_PM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekdays 5 - 8 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nThis peak period looks busiest because it looks reddest. However we must take some caution on this interpretation as the reds are more scattered so our brains may interpret this as red all throughout, compared to clusters of red and white.\nThis peak period is when workers and student most likely go home so it may look more scattered as workplaces or schools may be more scattered compared to residential areas.\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_AM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 11 AM - 2 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nThis map looks a lot paler compared to the weekday maps. This could be because it there is no work or school, people tend to stay home.\nHowever, while paler, people going out should still come from their homes so this should just look like the lighter weekday morning map. We will explore this later when we compare these 2 peak periods.\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_PM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 4 - 7 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\nThis map also looks lighter due to it being a weekend and people staying home. However, another possible explanation is that people have more freedom to schedule their activities on weekends or holidays so the traffic may be more scattered throughout the day.\nA spatio-temporal analysis may reveal more information that can validate this inference.\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nDoing the adjustments made it much easier to compare peak hours compared to the others.\nFor example, we are able to see that weekdays are indeed generally busier than weekends due to the weekends due to the maps looking darker.\nWe can also observe that the West and Northwest parts of Singapore has less bus trips compared to the rest of Singapore. The cause could be fewer residents in that area, sparse distribution of the population, or people preferring to use other modes of transportation (e.g., MRT, cars).\nHowever, some details were lost with this adjustment. For example, we are not able to see what is the highest and lowest number of trips for each peak period. If this is the intention or the narrative we want, the previous unadjusted visualizations are more effective for these purposes.\nFor our analysis, we want to compare the number of bus trips from one peak period to the other so making the adjustment in the break points is very helpful for this purpose."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1B.html#comparing-peak-periods",
    "href": "Take-home_Ex1/Take-home_Ex1B.html#comparing-peak-periods",
    "title": "Take Home Exercise 1B: Geovisualization and Analysis",
    "section": "Comparing peak periods",
    "text": "Comparing peak periods\nNow that our maps can be compared apples to apples, we will compare 2 peak periods side by side.\n\n\n\n\n\n\nTo compare each pair of maps more easily, we will use tabs so we can just switch between them to see how the patterns change from one map to the other.\n\n\n\n\nWeekday peak periods\n\nWeekday AM (6 - 9 AM)Weekday PM (5 - 8 PM)\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKDAY_AM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekdays 6 - 9 AM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKDAY_PM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekdays 5 - 8 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nAn interesting observation is that when switch from weekend AM to weekend PM map, there is perceived outward scattering from the darker areas to the neighboring areas.\nSwitching to weekend PM to weekend AM shows the reds converging to the darker points.\nThis is consistent with real world pattern of people going out of their homes for their daily activities in the morning and going home in the evening.\nHowever, what cannot be verified from this comparison is whether people really going to work or school in places close to their homes, as suggested by outside scattering. This may not be necessarily true as people from different areas of Singapore may travel long distances or so to their places of work and school, not the neighboring areas. A flow analysis will give us better insights about this.\n\n\n\n\nWeekend peak periods\n\n\n\n\n\n\nAdjusting the break points\n\n\n\n\n\nFor this comparison, we adjusted the break points using the same method as in Deriving the break points. This is because the traffic is much less over the weekend so we are not able to use the full range of categories we previously derived.\nSee the maps below.\n\nWeekend AM (11 AM - 2 PM)Weekend PM (4 - 7 PM)\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_AM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 11 AM - 2 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_PM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 4 - 7 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\nHence, we couldn’t get insights to the same level of details as the other comparisons. We will use the following breaks for this comparison:\n\nquantile(\n  c(\n    peak_trips_sf$WEEKEND_AM_TRIPS,\n    peak_trips_sf$WEEKEND_PM_TRIPS\n  ),\n  probs = c(0, 0.125, 0.25, 0.375, 0.50, 0.625, 0.75, 0.875, 1)\n)\n\n        0%      12.5%        25%      37.5%        50%      62.5%        75% \n     0.000    113.000    382.000    854.750   1686.000   2861.625   4616.250 \n     87.5%       100% \n  7785.375 111171.000 \n\n\nWith this change, we have shifted 7786 from the 6th to 8th category, providing us with the better level of detail on values between 0 to 7786.\nThis just shows that the breaks we calculated before is not appropriate for all comparisons. We still need to apply the scale appropriate to the data we have,\n\n\n\n\nWeekend AM (11 AM - 2 PM)Weekend PM (4 - 7 PM)\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_AM_TRIPS\",\n    breaks = c(0, 113, 382, 855, 1686, 2862, 4616, 7785, 111171),\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 11 AM - 2 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_PM_TRIPS\",\n    breaks = c(0, 113, 382, 855, 1686, 2862, 4616, 7785, 111171),\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 4 - 7 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nLike the weekday comparison, there is a slight scattering observed, although not as stark. The difference is not as big despite Adjusting the break points.\nThis very small shift could be because people are coming out of their homes throughout the day as they are free to schedule their activities throughout the day on weekends or holidays.\nThis is consistent with the insights in Weekend PM (4 - 7 PM). Again, a spatio-temporal analysis may provide us more information to verify this inference.\n\n\n\n\nMorning peaks\n\nWeekday AM (6 - 9 AM)Weekend AM (11 AM - 2 PM)\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKDAY_AM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekdays 6 - 9 AM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_AM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 11 AM - 2 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nThe weekend map generally looks like a lighter version of the weekday map. This may mean that less people go out of their homes on weekends.\nHowever, there are some notable things that can be observed.\n\nSome busy areas on the West during weekdays have low bus trips on weekends\n\nThis area corresponds to what seems to be an industrial area near the Tuas checkpoint.\nThis looks strange to me as it contradicts our hypothesis that morning bus rides are mostly due to people commuting from their home.\n\nLocations that coincide with bus interchanges are constantly busy\n\nThere are multiple bus lines originating from these interchanges so they are expected to have constant flow of commuters.\n\nThe bus stops for international travel are constantly busy\n\nExamples are those in Woodlands Checkpoint, Kranji Station, Changi Airport bus stops\nThese are popular for tourists and locals alike as Sinagapore is a business and tourism hub in Asia. Workers also commute regularly between Johor Bahru and Singapore, while locals visit Johor Bahru for weekend recreation.\n\nThe bus stops to areas popular to tourists are constantly busy\n\nSome examples are the Vivo City bus stops and the bus stop near Singapore Zoo (North of Central Water Catchment). Tourists come to this places throughout the week, and locals also go too these places for recreation.\n\n\n\n\n\n\nEvening peaks\n\nWeekday PM (5 - 8 PM)Weekend PM (4 - 7 PM)\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKDAY_PM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekdays 5 - 8 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(col= \"green\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(peak_trips_sf) +\n  tm_polygons(\n    \"WEEKEND_PM_TRIPS\",\n    breaks = c(0, 165, 632, 1545, 2985, 5276, 9180, 17851, 462160),\n    palette = \"YlOrRd\",\n    title = \"# of trips\"\n  ) +\n  tm_layout(\n    main.title = \"Bus Trips Originating from Each Location (Weekends/Holidays 4 - 7 PM)\",\n    main.title.position = \"center\",\n    main.title.size = 1,\n    legend.height = 0.35, \n    legend.width = 0.25,\n    legend.position = c(\"right\", \"bottom\")\n  )+\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nThere is not much difference in insights compared to Morning peaks aside from the active bus stop near Tuas Checkpoint on weekday mornings is not active in the evenings.\nThis is an interesting observation that is difficult to explain. One possible explanations is because of its relative proximity to Nanyang Technological University, students commuting to NTU transfer via these bus stops going to school. However, we may not see the same pattern during weekday evening peak period because university students have different schedules so their time leaving the university may be more scattered throughout the day.\nA flow analysis with information on the destination bus stop may reveal more information about this.\n\n\n\n\n\n\n\n\nWhy do we seem notice the busiest areas first?\n\n\n\n\n\nAnother interesting insight about these visualizations is that we seem notice the busy areas in the map more easily than the areas that have low traffic.\nInterestingly, our biology may play into this as our eyes have more cones that are sensitive to red light than any other type. As we use red to visualized the busiest areas in our map, they are the ones that catches our eye first.\nIf we had reversed our color scheme, perhaps we would have noticed the least busy areas first.\nHowever, we have to look into more reliable scientific and psychology sources to verify if this phenomenon is true.\n\n\n\n\n\n\n\n\n\nSave point\n\n\n\n\n\nWe will use peak_trips_sf for Local Indicators of Spatial Association (LISA) analysis.\n\nwrite_rds(peak_trips_sf, \"data/rds/peak_trips_sf202310.rds\")"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html",
    "href": "Take-home_Ex1/Take-home_Ex1A.html",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "",
    "text": "The aim of this study is to uncover spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.\nThe main modes of analysis to be used here are Local Indicators of Spatial Association (GLISA) and Emerging Hot Spot Analysis (EHSA).\nIn doing these study, we will be looking at bus trips started during the hours below.\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday evening peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nMore details about the study can be found here.\nIn this part of the study, we will do data wrangling on the data sets so that they are transformed into a form that can be used for geovisualization and spatial analysis."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#preparing-the-data-sets",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#preparing-the-data-sets",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Preparing the data sets",
    "text": "Preparing the data sets\n\nGeospatial\nThis data sets are in shp format.\n\nBus Stop Locations, available publicly from LTA DataMall\n\n\n\nAspatial\nThese data sets are in csv format.\n\nMaster Plan 2019 Subzone Boundary (Web), originally from data.gov.sg but used the one provided on E-learn.\nPassenger Volume By Origin Destination Bus Stops from LTA DataMall via API (need to request for access)\n\nAugust 2023\nSeptember 2023\nOctober 2023 - we will focus on this as the main data set"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#preparing-the-data-directory",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#preparing-the-data-directory",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Preparing the data/ directory",
    "text": "Preparing the data/ directory\nBefore starting our analysis, we have to organize the data sets in a directory.\n\nGeospatial data will be located under data/geospatial\nAspatial data will be located under data/aspatial\ndata/rds to be created to store data that we can reuse and to make our code reproduceable.\n\n\n\n\n\n\n\nShow file structure\n\n\n\n\n\nTake-home_Ex1\n└── data\n    ├── aspatial\n    │   ├── origin_destination_bus_202308.csv\n    │   ├── origin_destination_bus_202309.csv\n    │   └── origin_destination_bus_202310.csv\n    ├── geospatial\n    │   ├── BusStop.cpg\n    │   ├── BusStop.dbf\n    │   ├── BusStop.lyr\n    │   ├── BusStop.prj\n    │   ├── BusStop.sbn\n    │   ├── BusStop.sbx\n    │   ├── BusStop.shp\n    │   ├── BusStop.shp.xml\n    │   ├── BusStop.shx\n    │   ├── MPSZ-2019.cpg\n    │   ├── MPSZ-2019.dbf\n    │   ├── MPSZ-2019.prj\n    │   ├── MPSZ-2019.qmd\n    │   ├── MPSZ-2019.shp\n    │   └── MPSZ-2019.shx\n    └── rds"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#setting-up-the-r-environment",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#setting-up-the-r-environment",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Setting Up the R Environment",
    "text": "Setting Up the R Environment\nAfter preparing the data sets, we can finally proceed to load the R packages needed for this study.\n\n\n\n\n\n\nR packages used\n\n\n\n\n\n\ntmap: for thematic mapping\nsf: for geospatial data handling\ntidyverse: for non-spatial data handling\nknitr:for prettifying presentation\nsfdep: for spatial analysis"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#environment-settings",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#environment-settings",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Environment Settings",
    "text": "Environment Settings\nWe will also set the default settings on for this document\n\ntmap_mode to plot: for plotting simple maps\ntmap_style to natural: for my preferred mapping style\nset seed for reproducibility of results"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#running-the-setup",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#running-the-setup",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Running the setup",
    "text": "Running the setup\nWe will label this code chunk as the setup chunk so the R runs it even after the environment restarts.\n\npacman::p_load(sf, tmap, tidyverse, knitr, sfdep)\ntmap_mode(\"plot\")\ntmap_style(\"natural\")\nset.seed(1234)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#goal-data-sets",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#goal-data-sets",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Goal data sets",
    "text": "Goal data sets\nTo enable the visualization and analysis in latter part of the study, we need to have the following data sets:\n\nHoneycomb geometry, a tessellation of hexagons covering the bus stops in Singapor\nHourly bus trips started from each hexagon cell\n\n1 for weekend, 1 for weekend/holidays\nRequired columns: HEX_ID, HOUR_OF_DAY, TRIPS\nMust contain geometry of the hexagon\nCan be used to generate a time series cube\n\n\nAs the wrangling process is expected to have a lot of intermediate steps, Save, Load, and Data clear points are available to make our data wrangling more efficient.\n\n\n\n\n\n\nSave point\n\n\n\nThis is where data is written as rds files using write_rds() for important data sets that will be used in later analysis. Examples are:\n\nThe end goal of data wrangling: Hourly bus trips started from each hexagon cell data sets\nCritical outputs of expensive calculations\n\n\n\n\n\n\n\n\n\nLoad point\n\n\n\nThis is where data is loaded from rds files using read_rds(). They were previously generated by the save point.\nTIP: Skip to the load points to progress without running the code above it\n\n\n\n\n\n\n\n\nData clear point\n\n\n\nThis is where data that will not be used anymore are cleared. The data in RStudio environment will pile up and set #| eval: false in code chunks if you want skip the clearing. For example, the code below won’t be run.\n\nmessage &lt;- \"This code chunk executed\""
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#importing-the-singapore-boundary-data",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#importing-the-singapore-boundary-data",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Importing the Singapore boundary data",
    "text": "Importing the Singapore boundary data\nWe will use the Master Plan 2019 Subzone Boundary (Web) data set that has been used in class. This is a shp file, that we will import by using st_read(). We will use this to ensure that the bus stops are within Singapore.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                    layer = \"MPSZ-2019\")\n\nReading layer `MPSZ-2019' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/Take-home_Ex1/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nCorrecting the projection\n\n\n\nThis data frame using the global GPS standard projection, WGS84. We need to convert this to SVY21 that is more appropriate for Singapore 🇸🇬 context.\n\nmpsz &lt;- mpsz %&gt;% st_transform(crs=3414)\n\n\n\n\n\n\n\n\n\nSave point\n\n\n\nLet’s save this geometry with corrected projection from plotting purposes.\n\nwrite_rds(mpsz, \"data/rds/mpsz.rds\")\n\n\n\n\n\nShow the code\ntmap_style(\"natural\")\ntm_shape(mpsz) +\n  tm_fill(\"lightgreen\", title = \"Singapore Boundary\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Map of Singapore\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#importing-the-busstop-data-set",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#importing-the-busstop-data-set",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Importing the BusStop data set",
    "text": "Importing the BusStop data set\nThe BusStop data set is a in shp format. We can import it by using st_read() from the sf package.\n\nbusstops &lt;- st_read(dsn = \"data/geospatial\",\n                    layer = \"BusStop\")\n\nReading layer `BusStop' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/Take-home_Ex1/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nCorrecting the projection\n\n\n\n\n\nWe want to use SVY21 as the projection for this study as it is the projection used for local Singaporean context.\nAfter the import, it shows that the Projected CRSis SVY21. However, checking the CRS with st_crs() tells a different story.\n\nst_crs(busstops)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAs we can see EPSG value is 9001, which correspond to WGS84. We have to fix the projection by transforming to EPSG value of 3414, which corresponds to SVY21.\n\nbusstops &lt;- st_transform(busstops, crs = 3414)\n\n\n\n\nNext, let’s take a look at the available columns to identify which columns we can use for analysis.\n\nkable(head(busstops))\n\n\n\n\n\n\n\n\n\n\nBUS_STOP_N\nBUS_ROOF_N\nLOC_DESC\ngeometry\n\n\n\n\n22069\nB06\nOPP CEVA LOGISTICS\nPOINT (13576.31 32883.65)\n\n\n32071\nB23\nAFT TRACK 13\nPOINT (13228.59 44206.38)\n\n\n44331\nB01\nBLK 239\nPOINT (21045.1 40242.08)\n\n\n96081\nB05\nGRACE INDEPENDENT CH\nPOINT (41603.76 35413.11)\n\n\n11561\nB05\nBLK 166\nPOINT (24568.74 30391.85)\n\n\n66191\nB03\nAFT CORFE PL\nPOINT (30951.58 38079.61)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom this initial look in the data, BUS_STOP_N and LOC_DESC can potentially be used to match records in the passenger volume data set.\n\n\n\n\n\n\n\n\nChanging columns to factor\n\n\n\n\n\nBUS_STOP_N has a finite set of values that we do not need to process sequentially so we will convert it as factor to make it easier to work with.\n\nbusstops$BUS_STOP_N &lt;- as.factor(busstops$BUS_STOP_N)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#generating-hexagons-from-singapore-boundary-data",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#generating-hexagons-from-singapore-boundary-data",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Generating hexagons from Singapore boundary data",
    "text": "Generating hexagons from Singapore boundary data\nFollowing the steps from https://urbandatapalette.com/post/2021-08-tessellation-sf/, we will use st_make_grid() to generate the hexagons for analysis.\nWe need to provide a value for cellsize in the function, which is defined as “for hexagonal cells the distance between opposite edges”. We need to create hexagons whose apothem is 250m, resulting in a cell size of 500m.\n\n\n\n\n\n\nWhy is cell size 500 m?\n\n\n\n\n\nApothem is defined as the perpendicular from the center of a regular polygon to one of the sides.\nThe specification is this study requires hexagons to be 250 m from the center of the hexagon to the center of one of it’s edge.\n\n\n\n\n\nAs such, this corresponds to the length of 2 opposite apothems, which is 500 m.\nThe edge length is not the same as apothem. It is 288.675m.\n\\[\n250m/cos(30) = 288.675m\n\\]\n\n\n\nWe will use the mpsz data to ensure that the honeycomb grid perfectly covers the Singapore boundaries\n\nhoneycomb &lt;-\n  st_make_grid(mpsz,\n               cellsize = 500,\n               what = \"polygon\",\n               square = FALSE) %&gt;%\n  st_sf()\n\n\n\n\n\n\n\nWe have to use st_sf() to convert the result to a data frame that can be used for the succeeding steps.\n\n\n\nChecking the generated hexagons reveals that it covers all the bus stops.\n\n\nShow the code\ntm_shape(honeycomb) +\n  tm_fill(col = \"white\", title = \"Hexagons\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Singapore with honeycomb grid\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2, bg.color = \"white\", bg.alpha = 0.5) +\n  tm_scale_bar(bg.color = \"white\", bg.alpha = 0.5) +\n  tm_shape(mpsz) +\n  tm_fill(\"green\", title = \"Singapore Boundary\", alpha = 0.5) +\n  tm_shape(busstops) +\n  tm_dots(col = \"red\", size = 0.005, title = \"Bus Stops\")\n\n\n\n\n\n\n\n\n\n\n\nChecking the scale reveals that the generated hexagons are of the expected size, 500 m from one edge to the opposite edge as there are 10 hexagons within a 5 km distance.\n\n\n\n\n\n\n\n\n\nAbout those points outside Singapore\n\n\n\n\n\nThe map shows that there are bus stops in our data set that our outside Singapore bounds (green area). We can remove these points from our busstops data by following the filtering steps from https://urbandatapalette.com/post/2021-08-tessellation-sf/.\nWe will st_intersects() to see which points in busstops intersect with mpsz, and filter those that intersect.\n\nbusstops$n_collisions = lengths(st_intersects(busstops, mpsz))\nbusstops &lt;-\n  filter(busstops, n_collisions &gt; 0) %&gt;%\n  select(, -n_collisions) # Remove n_collisions as we do not need it anymore\n\nPlotting again shows that all bus stops are now within Singapore bounds.\n\n\nShow the code\ntm_shape(honeycomb) +\n  tm_fill(col = \"white\", title = \"Hexagons\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Honeycomb grid without bus stops outside of Singapore\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2, bg.color = \"white\", bg.alpha = 0.5) +\n  tm_scale_bar(bg.color = \"white\", bg.alpha = 0.5) +\n  tm_shape(mpsz) +\n  tm_fill(\"green\", title = \"Singapore Boundary\", alpha = 0.5) +\n  tm_shape(busstops) +\n  tm_dots(col = \"red\", size = 0.005, title = \"Bus Stops\")"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#filtering-hexagons-with-bus-stops",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#filtering-hexagons-with-bus-stops",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Filtering hexagons with bus stops",
    "text": "Filtering hexagons with bus stops\nThe honeycomb grid generated from Generating hexagons from Singapore boundary data need to be filtered such that the hexagons remaining correspond to only those with bus stops.\nWe can do this by following the filtering steps from https://urbandatapalette.com/post/2021-08-tessellation-sf/. We will use st_intersects() to identify which hexagons intersect with bus stop locations.\n\nhoneycomb$n_collisions = lengths(st_intersects(honeycomb, busstops))\nhoneycomb &lt;- filter(honeycomb, n_collisions &gt; 0)\n\nLet’s generate the map again to check if we have the hexagons that correspond to bus stop locations.\n\n\nShow the code\ntm_shape(mpsz) +\n  tm_fill(\"green\", title = \"Singapore Boundary\", alpha = 0.5) +\n  tm_shape(honeycomb) +\n  tm_fill(col = \"white\", title = \"Hexagons\", alpha = 1) +\n  tm_borders(alpha = 0.2) +\n  tm_layout(main.title = \"Honeycomb grid corresponding to Singapore bus stops\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.35, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2, bg.color = \"white\", bg.alpha = 0.5) +\n  tm_scale_bar(bg.color = \"white\", bg.alpha = 0.5) +\n  tm_shape(busstops) +\n  tm_dots(col = \"red\", size = 0.001, title = \"Bus Stops\") +\n  tm_grid(alpha = 0.2)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#assigning-ids-to-each-hexagon",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#assigning-ids-to-each-hexagon",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Assigning ids to each hexagon",
    "text": "Assigning ids to each hexagon\nHere is the structure of our honeycomb data:\n\nkable(head(honeycomb, n=3))\n\n\n\n\ngeometry\nn_collisions\n\n\n\n\nPOLYGON ((3917.538 28017.41…\n1\n\n\nPOLYGON ((4417.538 30615.49…\n1\n\n\nPOLYGON ((4667.538 28450.43…\n2\n\n\n\n\n\n\n\n\n\n\n\nRemove n-collisions\n\n\n\n\n\nWe do not need n-collisions anymore so we can remove it.\n\nhoneycomb &lt;- honeycomb %&gt;% select(, -n_collisions)\n\n\n\n\nThis data is still incomplete as we need to associate the hexagons to aspatial data, which is critical to the next steps in our data wrangling.\nFor this purpose, we will assign HEX_ID with format H0000.\n\nhoneycomb$HEX_ID &lt;- sprintf(\"H%04d\", seq_len(nrow(honeycomb))) %&gt;% as.factor()\nkable(head(honeycomb)) \n\n\n\n\ngeometry\nHEX_ID\n\n\n\n\nPOLYGON ((3917.538 28017.41…\nH0001\n\n\nPOLYGON ((4417.538 30615.49…\nH0002\n\n\nPOLYGON ((4667.538 28450.43…\nH0003\n\n\nPOLYGON ((4667.538 30182.48…\nH0004\n\n\nPOLYGON ((4667.538 31048.5,…\nH0005\n\n\nPOLYGON ((4917.538 28883.44…\nH0006\n\n\n\n\n\n\n\n\n\n\n\nSave point\n\n\n\n\n\nhoneycomb is the geometry that we will use for analysis. It will be used for tasks such as identifying neighbors and calculating spatial weights.\nIs it also one of the Goal data sets we need. Hence, we will save it.\n\nwrite_rds(honeycomb, \"data/rds/honeycomb202310.rds\")\n\n\n\n\n\n\n\n\n\n\nData clear point\n\n\n\n\n\nWe do not need mpsz anymore as we have generated hexagons already. For further analysis, we will overlay the hexagons to the Singapore map with tmap_mode(\"plot\") to use interactive maps for closer inspection.\n\nrm(mpsz)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#importing-the-data-set",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#importing-the-data-set",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Importing the data set",
    "text": "Importing the data set\nThe data set is an aspatial data in csv format so we will use read_csv() to import the data.\n\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202310.csv\")\nkable(head(odbus))\n\n\n\n\n\n\n\n\n\n\n\n\n\nYEAR_MONTH\nDAY_TYPE\nTIME_PER_HOUR\nPT_TYPE\nORIGIN_PT_CODE\nDESTINATION_PT_CODE\nTOTAL_TRIPS\n\n\n\n\n2023-10\nWEEKENDS/HOLIDAY\n16\nBUS\n04168\n10051\n3\n\n\n2023-10\nWEEKDAY\n16\nBUS\n04168\n10051\n5\n\n\n2023-10\nWEEKENDS/HOLIDAY\n14\nBUS\n80119\n90079\n3\n\n\n2023-10\nWEEKDAY\n14\nBUS\n80119\n90079\n5\n\n\n2023-10\nWEEKDAY\n17\nBUS\n44069\n17229\n4\n\n\n2023-10\nWEEKENDS/HOLIDAY\n17\nBUS\n20281\n20141\n1\n\n\n\n\n\n\n\n\n\n\n\nThe relevant columns for our data study are DAY_TYPE, TIME_PER_HOUR, ORIGIN_PT_CODE, TOTAL_TRIPS\nWe do not need the DESTINATION_PT_CODE as we are only interested on when passengers get on the bus.\nFurthermore, the ORIGIN_PT_CODE can be correlated to the BUS_STOP_N column of busstops data.\n\n\n\n\n\n\n\n\n\nRecap of busstops data\n\n\n\n\n\n\nkable(head(busstops))\n\n\n\n\n\n\n\n\n\n\nBUS_STOP_N\nBUS_ROOF_N\nLOC_DESC\ngeometry\n\n\n\n\n22069\nB06\nOPP CEVA LOGISTICS\nPOINT (13576.31 32883.65)\n\n\n32071\nB23\nAFT TRACK 13\nPOINT (13228.59 44206.38)\n\n\n44331\nB01\nBLK 239\nPOINT (21045.1 40242.08)\n\n\n96081\nB05\nGRACE INDEPENDENT CH\nPOINT (41603.76 35413.11)\n\n\n11561\nB05\nBLK 166\nPOINT (24568.74 30391.85)\n\n\n66191\nB03\nAFT CORFE PL\nPOINT (30951.58 38079.61)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#cleaning-the-data",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#cleaning-the-data",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nBefore going deep in the wrangling, we will clean up the data so that we are left with a lightweight data set that R can process more easily. We will retain and rename columns below to make them more understandable and easier to join with other data sets.\n\nDAY_TYPE\nTIME_PER_HOUR -&gt; HOUR_OF_DAY\nORIGIN_PT_CODE -&gt; BUS_STOP_N\nTOTAL_TRIPS -&gt; TRIPS\n\nWe will also rename the columns to make them more understandable and will make joining with other data sets easier.\nLastly, will also convert BUS_STOP_N to factor as it has a finite set of values so we can convert it to categorical data to make it easier to work with.\n\ntrips &lt;- odbus %&gt;%\n  select(c(ORIGIN_PT_CODE, DAY_TYPE, TIME_PER_HOUR, TOTAL_TRIPS)) %&gt;%\n  rename(BUS_STOP_N = ORIGIN_PT_CODE) %&gt;%\n  rename(HOUR_OF_DAY = TIME_PER_HOUR) %&gt;%\n  rename(TRIPS = TOTAL_TRIPS)\ntrips$BUS_STOP_N &lt;- as.factor(trips$BUS_STOP_N)\nkable(head(trips))\n\n\n\n\nBUS_STOP_N\nDAY_TYPE\nHOUR_OF_DAY\nTRIPS\n\n\n\n\n04168\nWEEKENDS/HOLIDAY\n16\n3\n\n\n04168\nWEEKDAY\n16\n5\n\n\n80119\nWEEKENDS/HOLIDAY\n14\n3\n\n\n80119\nWEEKDAY\n14\n5\n\n\n44069\nWEEKDAY\n17\n4\n\n\n20281\nWEEKENDS/HOLIDAY\n17\n1\n\n\n\n\n\n\n\n\n\n\n\nselect() is used to select the columns we need.\nrename() is used to rename the columns.\n\n\n\n\n\n\n\n\n\nData clear point\n\n\n\n\n\nWe do not need odbus anymore as we will be working with the more lightweight trips from this point on.\n\nrm(odbus)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#mapping-the-bus-stops-to-hexagon",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#mapping-the-bus-stops-to-hexagon",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Mapping the bus stops to hexagon",
    "text": "Mapping the bus stops to hexagon\nIn Filtering hexagons with bus stops we were able to overlay the bus locations to our generated hexagon. While this is enough for visualization, it is not enough for the rest of the data processing we need.\nFrom Cleaning the data, we have the BUS_STOP_N in the that we can use to associate with busstops.\nWe need to create an aspatial table that contain BUS_STOP_N and HEX_ID of the hexagon containing them. We will use st_intersection().\n\n\n\n\n\n\nWhy aspatial?\n\n\n\n\n\nWe want to use generate a simple mapping here as this table will serve as a “glue” between the other aspatial data sets and our geospatial data, honeycomb.\n\n\n\n\nbs_hex &lt;- st_intersection(busstops, honeycomb) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(c(BUS_STOP_N, HEX_ID))\nkable(head(bs_hex))\n\n\n\n\n\nBUS_STOP_N\nHEX_ID\n\n\n\n\n3265\n25059\nH0001\n\n\n254\n26379\nH0002\n\n\n2566\n25751\nH0003\n\n\n2893\n25761\nH0003\n\n\n4199\n26389\nH0004\n\n\n2399\n26369\nH0005\n\n\n\n\n\n\n\n\n\n\n\nst_intersection() - find which hexagon contains the bus stop\nst_drop_geometry() - to make data aspatial\nselect() - to retain only the relevant columns: BUS_STOP_N and HEX_ID"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#adding-hex_id-information-to-bus-trips-data",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#adding-hex_id-information-to-bus-trips-data",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Adding HEX_ID information to bus trips data",
    "text": "Adding HEX_ID information to bus trips data\nTo achieve our goal of having the hourly # of bus trips per location, we need to add HEX_ID to trips data. This is so we can answer, how many bus trip originate from a certain hexagon?\nTo do this, we will do an inner_join() to join the trips data with bs_hex.\n\n\n\n\n\n\nWhy `inner_join()` instead of `left_join()`?\n\n\n\n\n\nWe will use inner_join as there are BUS_STOP_N values in trips data that are not in bs_hex.\n\ntrips$BUS_STOP_N[!(trips$BUS_STOP_N %in% bs_hex$BUS_STOP_N)] %&gt;%\n  unique() %&gt;% length()\n\n[1] 63\n\n\nThere are 57 bus stops in trips that are not in bs_hex. 5 of this can be attributed the bus stops we removed in About those points outside Singapore. Others may be due to the BusStops data set not having complete information.\nNonetheless, we have to remove these bus stops from our analysis as we do not have geospatial data to associate to the hexagons.\nTherefore, we will use inner_join to keep only the observations in trips with the matching bus stops in bs_hex.\n\n\n\n\ntrips &lt;- inner_join(trips, bs_hex)\nkable(head(trips))\n\n\n\n\nBUS_STOP_N\nDAY_TYPE\nHOUR_OF_DAY\nTRIPS\nHEX_ID\n\n\n\n\n04168\nWEEKENDS/HOLIDAY\n16\n3\nH0955\n\n\n04168\nWEEKDAY\n16\n5\nH0955\n\n\n80119\nWEEKENDS/HOLIDAY\n14\n3\nH1102\n\n\n80119\nWEEKDAY\n14\n5\nH1102\n\n\n44069\nWEEKDAY\n17\n4\nH0410\n\n\n20281\nWEEKENDS/HOLIDAY\n17\n1\nH0388"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#aggregating-trips-based-on-hex_id",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#aggregating-trips-based-on-hex_id",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Aggregating TRIPS based on HEX_ID",
    "text": "Aggregating TRIPS based on HEX_ID\nNext, we will add the TRIPS for all the bus stops within a hexagon. We will group via HEX_ID, DAY_TYPE, and HOUR_OF_DAY.\n\ntrips &lt;- trips %&gt;%\n  group_by(\n    HEX_ID,\n    DAY_TYPE,\n    HOUR_OF_DAY) %&gt;%\n  summarise(TRIPS = sum(TRIPS))\nkable(head(trips))\n\n\n\n\nHEX_ID\nDAY_TYPE\nHOUR_OF_DAY\nTRIPS\n\n\n\n\nH0001\nWEEKDAY\n7\n74\n\n\nH0001\nWEEKDAY\n8\n19\n\n\nH0001\nWEEKDAY\n9\n10\n\n\nH0001\nWEEKDAY\n10\n7\n\n\nH0001\nWEEKDAY\n16\n26\n\n\nH0001\nWEEKDAY\n17\n122\n\n\n\n\n\n\n\n\n\n\n\nSave point\n\n\n\n\n\ntrips was processed from Passenger Volume By Origin Destination Bus Stops, which has almost 6 million observations.\nWe now have a more lightweight dataset with almost 60,000 observations, which is about 100x smaller.\nLet’s save this data as an rds file so we don’t need to reprocess again later on.\n\nwrite_rds(trips, \"data/rds/trips202310.rds\")"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#generating-the-combinations",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#generating-the-combinations",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Generating the combinations",
    "text": "Generating the combinations\n\n\n\n\n\n\nHow many combinations are there? The answer is 36,456.\n\n\n\n\n\nTo satisfy the requirement of:\n\nIt must have a row for each combination of HEX_ID (location) and HOUR_OF_DAY (time)\n\nWe need to find out how many such combinations exist.\n\nThere are 1519 hexagons in our honeycomb\nThere are 24 hours in a day\n\nTherefore, there are \\(1519 \\times 24 = 36,456\\) combinations. We will use this value to verify if we have the correct space time cube.\n\n\n\nTo generate the combinations, we will use expand.grid() and for us to provide the list possible values for HEX_ID and HOUR_OF_DAY.\n\ncombos &lt;- expand_grid(\n  HEX_ID = honeycomb$HEX_ID,\n  HOUR_OF_DAY = 0:23\n)\nkable(combos[20:29,])\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\n\n\n\n\nH0001\n19\n\n\nH0001\n20\n\n\nH0001\n21\n\n\nH0001\n22\n\n\nH0001\n23\n\n\nH0002\n0\n\n\nH0002\n1\n\n\nH0002\n2\n\n\nH0002\n3\n\n\nH0002\n4\n\n\n\n\n\ncombos also has 36,456 rows, aligned with our expectations.\n\nnrow(combos)\n\n[1] 36456\n\n\nWith this generated, we can use this as a glue to generate our time series cube."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#splitting-the-data",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#splitting-the-data",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Splitting the data",
    "text": "Splitting the data\nAs we want to do separate analysis for weekdays and weekends, we will split the data. We will also remove the DAY_TYPE column as we do not need it anymore. To do this, we have to ungroup() before removing as we use DAY_TYPE as filter.\n\nWeekdayWeekend/Holidays\n\n\n\ntrips_wkdy &lt;- trips %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  ungroup() %&gt;%\n  select(, -DAY_TYPE)\nkable(trips_wkdy[20:29,])\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\n\n\n\n\nH0002\n16\n257\n\n\nH0002\n17\n159\n\n\nH0002\n18\n62\n\n\nH0002\n19\n42\n\n\nH0002\n20\n28\n\n\nH0002\n21\n2\n\n\nH0002\n22\n2\n\n\nH0003\n6\n44\n\n\nH0003\n7\n69\n\n\nH0003\n8\n72\n\n\n\n\n\n\n\n\ntrips_wknd &lt;- trips %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  ungroup() %&gt;%\n  select(, -DAY_TYPE)\nkable(trips_wknd[20:29,])\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\n\n\n\n\nH0002\n16\n42\n\n\nH0002\n17\n15\n\n\nH0002\n18\n32\n\n\nH0002\n19\n11\n\n\nH0002\n20\n13\n\n\nH0002\n21\n5\n\n\nH0002\n22\n2\n\n\nH0003\n6\n37\n\n\nH0003\n7\n40\n\n\nH0003\n8\n35\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChecking if split covers the full data\n\n\n\n\n\nLet’s check the total rows in trips_wkdy and trips_wknd add up to the number of rows in trips.\n\nnrow(trips_wkdy) + nrow(trips_wknd) == nrow(trips)\n\n[1] TRUE\n\n\nThere are no lost data so we can proceed to the next step.\n\n\n\n\n\n\n\n\n\nData clear point\n\n\n\n\n\nWe do not need trips anymore as we will be using trips_wkdy and trips_wknd from this point.\n\nrm(trips)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#filling-in-the-all-the-combos",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#filling-in-the-all-the-combos",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Filling in the all the combos",
    "text": "Filling in the all the combos\nNow that we have separate data frames for weekday and weekend/holiday, we need to make sure that our data frame as all the combination in combos. We can do that by joining trips_wkxx with combos.\n\nWeekdayWeekend/Holiday\n\n\n\ntrips_cube_wkdy &lt;- left_join(combos, trips_wkdy)\nkable(head(trips_cube_wkdy, n = 24))\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\n\n\n\n\nH0001\n0\nNA\n\n\nH0001\n1\nNA\n\n\nH0001\n2\nNA\n\n\nH0001\n3\nNA\n\n\nH0001\n4\nNA\n\n\nH0001\n5\nNA\n\n\nH0001\n6\nNA\n\n\nH0001\n7\n74\n\n\nH0001\n8\n19\n\n\nH0001\n9\n10\n\n\nH0001\n10\n7\n\n\nH0001\n11\nNA\n\n\nH0001\n12\nNA\n\n\nH0001\n13\nNA\n\n\nH0001\n14\nNA\n\n\nH0001\n15\nNA\n\n\nH0001\n16\n26\n\n\nH0001\n17\n122\n\n\nH0001\n18\n224\n\n\nH0001\n19\n38\n\n\nH0001\n20\n6\n\n\nH0001\n21\nNA\n\n\nH0001\n22\nNA\n\n\nH0001\n23\nNA\n\n\n\n\n\nCheck if the output has the same rows as combos.\n\nnrow(trips_cube_wkdy) == nrow(combos)\n\n[1] TRUE\n\n\n\n\n\ntrips_cube_wknd &lt;- left_join(combos, trips_wknd)\nkable(head(trips_cube_wknd, n = 24))\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\n\n\n\n\nH0001\n0\nNA\n\n\nH0001\n1\nNA\n\n\nH0001\n2\nNA\n\n\nH0001\n3\nNA\n\n\nH0001\n4\nNA\n\n\nH0001\n5\nNA\n\n\nH0001\n6\nNA\n\n\nH0001\n7\n28\n\n\nH0001\n8\n9\n\n\nH0001\n9\n9\n\n\nH0001\n10\n2\n\n\nH0001\n11\nNA\n\n\nH0001\n12\nNA\n\n\nH0001\n13\nNA\n\n\nH0001\n14\nNA\n\n\nH0001\n15\nNA\n\n\nH0001\n16\n3\n\n\nH0001\n17\n21\n\n\nH0001\n18\n18\n\n\nH0001\n19\n14\n\n\nH0001\n20\n1\n\n\nH0001\n21\nNA\n\n\nH0001\n22\nNA\n\n\nH0001\n23\nNA\n\n\n\n\n\nCheck if the output has the same rows as combos.\n\nnrow(trips_cube_wknd) == nrow(combos)\n\n[1] TRUE\n\n\n\n\n\n\n\n\n\n\n\nThe data frames generated now passes\n\nIt must have a row for each combination of HEX_ID (location) and HOUR_OF_DAY (time)\n\n\n\n\n\n\n\n\n\n\nThe data frames generated violate\n\nThere are no missing values in TRIPS column\n\nThis is because there are some HOUR_OF_DAY where the value of TRIPS is NA. We need to fill in these missing values.\n\n\n\n\n\n\n\n\n\nData clear point\n\n\n\n\n\nWe do not need trips_wkxx anymore as we will be using trips_cube_wkxx from this point on.\n\nrm(trips_wkdy)\nrm(trips_wknd)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1A.html#filling-in-missing-values",
    "href": "Take-home_Ex1/Take-home_Ex1A.html#filling-in-missing-values",
    "title": "Take Home Exercise 1A: Data Wrangling",
    "section": "Filling in missing values",
    "text": "Filling in missing values\nLastly, we need to fill in the missing values in TRIPS. This can be done by filtering the rows with NA and setting those to 0.\n\nWeekdayWeekend/Holiday\n\n\n\ntrips_cube_wkdy$TRIPS[is.na(trips_cube_wkdy$TRIPS)] &lt;- 0\nkable(head(trips_cube_wkdy, n = 24))\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\n\n\n\n\nH0001\n0\n0\n\n\nH0001\n1\n0\n\n\nH0001\n2\n0\n\n\nH0001\n3\n0\n\n\nH0001\n4\n0\n\n\nH0001\n5\n0\n\n\nH0001\n6\n0\n\n\nH0001\n7\n74\n\n\nH0001\n8\n19\n\n\nH0001\n9\n10\n\n\nH0001\n10\n7\n\n\nH0001\n11\n0\n\n\nH0001\n12\n0\n\n\nH0001\n13\n0\n\n\nH0001\n14\n0\n\n\nH0001\n15\n0\n\n\nH0001\n16\n26\n\n\nH0001\n17\n122\n\n\nH0001\n18\n224\n\n\nH0001\n19\n38\n\n\nH0001\n20\n6\n\n\nH0001\n21\n0\n\n\nH0001\n22\n0\n\n\nH0001\n23\n0\n\n\n\n\n\n\n\n\ntrips_cube_wknd$TRIPS[is.na(trips_cube_wknd$TRIPS)] &lt;- 0\nkable(head(trips_cube_wknd, n = 24))\n\n\n\n\nHEX_ID\nHOUR_OF_DAY\nTRIPS\n\n\n\n\nH0001\n0\n0\n\n\nH0001\n1\n0\n\n\nH0001\n2\n0\n\n\nH0001\n3\n0\n\n\nH0001\n4\n0\n\n\nH0001\n5\n0\n\n\nH0001\n6\n0\n\n\nH0001\n7\n28\n\n\nH0001\n8\n9\n\n\nH0001\n9\n9\n\n\nH0001\n10\n2\n\n\nH0001\n11\n0\n\n\nH0001\n12\n0\n\n\nH0001\n13\n0\n\n\nH0001\n14\n0\n\n\nH0001\n15\n0\n\n\nH0001\n16\n3\n\n\nH0001\n17\n21\n\n\nH0001\n18\n18\n\n\nH0001\n19\n14\n\n\nH0001\n20\n1\n\n\nH0001\n21\n0\n\n\nH0001\n22\n0\n\n\nH0001\n23\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs our data frame time series cube-friendly? The answer is YES.\n\n\n\n\n\nLet us check of our data frame can be used to create spacetime cubes.\n\nWeekend\n\nspacetime(trips_cube_wkdy, honeycomb,\n          .loc_col = \"HEX_ID\",\n          .time_col = \"HOUR_OF_DAY\") %&gt;%\n  is_spacetime_cube()\n\n[1] TRUE\n\n\n\n\nWeekend/Holiday\n\nspacetime(trips_cube_wknd, honeycomb,\n          .loc_col = \"HEX_ID\",\n          .time_col = \"HOUR_OF_DAY\") %&gt;%\n  is_spacetime_cube()\n\n[1] TRUE\n\n\n\n\n\n\n\n\n\n\n\n\nSave point\n\n\n\n\n\ntrips_cube_wkxx data is part the Goal data sets we need. Hence, we will save them.\n\nwrite_rds(trips_cube_wkdy, \"data/rds/trips_cube_wkdy202310.rds\")\nwrite_rds(trips_cube_wknd, \"data/rds/trips_cube_wknd202310.rds\")"
  },
  {
    "objectID": "Take-home_Ex1/data/geospatial/MPSZ-2019.html",
    "href": "Take-home_Ex1/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "This hands-on exercise covers Chapter 10: Local Measures of Spatial Autocorrelation\nI learned about the following:\n\nGlobal Spatial Autocorrelation (GSA) statistics\nLocal Indicator of Spatial Association (LISA) statistics\nGetis-Ord’s Gi-statistics\n\n\n\nData sets used on this exercise were downloaded from E-learn.\n\n\n\nHunan county boundary layer (shp format)\n\n\n\n\n\nHunan’s local development indicators in 2012 (csv format)\n\nNext, is putting them under the Hands-on_Ex2 directory, with the following file structure:\nHands-on_Ex2\n└── data\n    ├── aspatial\n    │   └── Hunan_2012.csv\n    └── geospatial\n        ├── Hunan.dbf\n        ├── Hunan.prj\n        ├── Hunan.qpj\n        ├── Hunan.shp\n        └── Hunan.shx\n\n\n\n\nI used the code below to install the R packages used in the exercise:\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#preparing-the-data-sets",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#preparing-the-data-sets",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "Data sets used on this exercise were downloaded from E-learn.\n\n\n\nHunan county boundary layer (shp format)\n\n\n\n\n\nHunan’s local development indicators in 2012 (csv format)\n\nNext, is putting them under the Hands-on_Ex2 directory, with the following file structure:\nHands-on_Ex2\n└── data\n    ├── aspatial\n    │   └── Hunan_2012.csv\n    └── geospatial\n        ├── Hunan.dbf\n        ├── Hunan.prj\n        ├── Hunan.qpj\n        ├── Hunan.shp\n        └── Hunan.shx"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#installing-r-packages",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#installing-r-packages",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "I used the code below to install the R packages used in the exercise:\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#importing-data-sets",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#importing-data-sets",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Importing data sets",
    "text": "Importing data sets\nI used st_read() to import the geospatial shp data.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\",\n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/Hands-on_Ex2/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the previous exercises, we transformed the data with EPSG:3414. However, that is not applicable for this data set as we are not working with Singapore 🇸🇬 data set.\n\n\nAs with the previous exercises, I used read_csv() to import aspatial csv data.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#joining-the-data-sets",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#joining-the-data-sets",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Joining the data sets",
    "text": "Joining the data sets\nIn the exercise, we have to join the 2 data sets using this code:\n\nhunan &lt;- left_join(hunan, hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\n\n\nNote\n\n\n\nWe did not specify any columns to join by but left_join detected common column, County, so it joined the 2 data sets by this column.\nAt the end of this, we are left with 7 columns, which includes GDPPC from the aspatial data, which contains data for Gross Domestic Product per Capita."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#visualizing-regional-development-indicator",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#visualizing-regional-development-indicator",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Visualizing Regional Development Indicator",
    "text": "Visualizing Regional Development Indicator\nNext, I plotted the GDPPC maps using equal interval classification and equal quantile classification.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#computing-contiguity-spatial-weights",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#computing-contiguity-spatial-weights",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Computing Contiguity Spatial Weights",
    "text": "Computing Contiguity Spatial Weights\nFirst, I built the neighbor list using Queen contiguity-based neighbors. This means the regions must share a border (minimum a point) to be considered neighbors.\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#row-standardized-weights-matrix",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#row-standardized-weights-matrix",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Row-standardized weights matrix",
    "text": "Row-standardized weights matrix\nNext, I assigned weights to each neighboring county with value 1/(# of neighbors). This could be done by using style=\"W\" to nb2listw().\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#global-spatial-autocorrelation-morans-i",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#global-spatial-autocorrelation-morans-i",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Global Spatial Autocorrelation: Moran’s I",
    "text": "Global Spatial Autocorrelation: Moran’s I\n\nMoran’s I test\nNext, I used Moran’s I statistical testing using moran.test().\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Moran’s I value is 0.30075, which is greater than 0. This means that observations are clustered, and tend to be similar.\nThe p-value is also very close to 0, which indicates high confidence on the correlation.\n\n\n\n\nComputing Monte Carlo Moran’s I\nNext, a Monte Carlo simulation was performed for the Moran’s I statistic. 1000 simulations were performed by the code below:\n\nset.seed(1234)\nbperm = moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Moran’s I value is 0.30075, same result as that of the Moran’s I test. Similarly, it means that observations are clustered, and tend to be similar.\nThe p-value is also very close to 0, which indicates high confidence on the correlation.\n\n\n\n\nVisualizing Monte Carlo Moran’s I\nFirst, I examined the statistics of the Monte Carlo Moran’s I. I checked the mean, variance, and the quantiles.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\nNext, I also plotted the histogram.\n\n\n\n\n\n\nImportant\n\n\n\nI plotted using ggplot2 as an additional challenge from the exercise.\n\n\n\nmc_results_df &lt;- data.frame(moran_i = bperm$res)\nggplot(mc_results_df, aes(x = moran_i)) +\n  geom_histogram(bins = 20, fill = \"grey\", color = \"black\") +\n  geom_vline(xintercept = 0, color = \"red\") +\n  labs(x = \"Sumilated Moran's I\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom this Monte Carlo simulations, results are skewed to the left, meaning most of the Moran’s I simulations result in negative values. It means that in most simulation results, there is dispersion so there is no spatial correlation.\nThis is quite contradictory to the statistic from moran.test.\nHowever, as this is a simulation set using seed 1234, results could be different in other simulations because the sampling is different."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#global-spatial-autocorrelation-gearys",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#global-spatial-autocorrelation-gearys",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Global Spatial Autocorrelation: Geary’s",
    "text": "Global Spatial Autocorrelation: Geary’s\nNext I used Geary’s method for spatial correlation.\n\nGeary’s C test\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\n\n\n\n\n\nTip\n\n\n\nGeary’s C statistic is 0.6907, which is less than 1. This means that observations are clustered, and tend to be similar. P-value is also very close to 0, suggesting high-confidence.\n\nIt is consistent with the conclusions in Moran’s I test.\n\n\n\n\nComputing Monte Carlo Geary’s C\nSimilarly, I did permutation test via Monte Carlo simulations.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Moran’s I value is 0.6907, same result as that of the Geary’s C test. Similarly, it means that observations are clustered, and tend to be similar.\nThe p-value is also very close to 0, which indicates high confidence on the correlation.\n\n\n\n\nVisualizing Monte Carlo Geary’s C\nFirst, I examined the statistics of the Monte Carlo Geary’s C. I checked the mean, variance, and the quantiles.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\nFinally, visualizing it.\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom this Monte Carlo simulations, results are quite balanced on 1, which makes it inconclusive as to the spatial clustering and dispersion.\nThis is quite contrary to the statistic resulting from geary.test(), which was more conclusive.\nHowever, as this is a simulation set using seed 1234, results could be different in other simulations because the sampling is different."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#compute-morans-i-correlogram",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#compute-morans-i-correlogram",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Compute Moran’s I correlogram",
    "text": "Compute Moran’s I correlogram\nFirst, I generated the correlogram for Morans’s I.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\nThis did not provide me much information and I didn’t know how to interpret it so I printed the full result.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTip\n\n\n\nFrom my understanding, since Moran’s I values are greater than 0 and highest on lag 1, it means that the spatial correlation is most significant the closer the regions are."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#compute-gearys-c-correlogram",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#compute-gearys-c-correlogram",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Compute Geary’s C correlogram",
    "text": "Compute Geary’s C correlogram\nNext, I generated the correlogram for Geary’s C.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\nNext was to print the results.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Geary’s C values are closest to 0 on the lag distance 1. Similar to Compute Moran’s I correlogram, the spatial correlation is strongest the closer the regions are.\nThe pattern is inverse of the Moran’s I correlogram, which makes sense as Moran’s I and Geary’s C trends are inverse of each other."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#computing-local-morans-i",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#computing-local-morans-i",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Computing Local Moran’s I",
    "text": "Computing Local Moran’s I\nFirst, I started with computing local Moran’s I values.\nThe code chunks below were used to compute local Moran’s I of GDPPC2012 at the county level.\n\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nThis code chunk result in a matrix with columns:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe local Moran’s I values were inspected by:\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#mapping-local-morans-i",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#mapping-local-morans-i",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Mapping local Moran’s I",
    "text": "Mapping local Moran’s I\nBefore proceeding with the mapping, I appended localMI dataframe onto the hunan dataframe.\n\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nThen, I plotted a cloropeth map of the local Moran’s I values and the p-values using tmap functions. These maps were plotted side by side for easier analysis.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#plotting-moran-scatterplot",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#plotting-moran-scatterplot",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Plotting Moran scatterplot",
    "text": "Plotting Moran scatterplot\nIn order to do this, I have to plot the Moran’s I scatterplot first. This can be via moran.plot().\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis can be be interpreted such that the counties on the upper-right quadrant (e.g., Shaosan, Ningxian, Liuyang, Wangchen, Changsa) are within an affluent region, i.e., cluster of counties with high GDP per capita.\nSome other counties of interest are Zixing and Lengshuijiang, which are more affluent than their neighbors. Lastly, Pingjian is less affluent compared to its neighbors."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#plotting-moran-scatterplot-with-standardized-variable",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#plotting-moran-scatterplot-with-standardized-variable",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Plotting Moran scatterplot with standardized variable",
    "text": "Plotting Moran scatterplot with standardized variable\nNext is to scale the plot by normalizing the axes, which should align the axes to 0. scale() was used for this purpose.\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector \n\nAfter scaling, I replotted the scatterplot.\n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#preparing-lisa-map-classes",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#preparing-lisa-map-classes",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Preparing LISA map classes",
    "text": "Preparing LISA map classes\nTo prepare LISA cluster map, I had to first create a numeric vector with the same number of elements as localMI, which is 88.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext was to compute the lag values and centering on the mean.\n\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)\n\nSimilarly, i also centered the local Moran’s I values around the mean.\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nI also set the significant value to 0.05 as per standards.\n\nsignif &lt;- 0.05\n\nThen, I defined the low-low (1), low-high (2), high-low (3) and high-high (4) categories. This corresponds to the quadrants in the scatterplot from Plotting Moran scatterplot.\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4\n\nLastly, was to add a category for non-significant Moran’s I values.\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#plotting-lisa-map",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#plotting-lisa-map",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Plotting LISA map",
    "text": "Plotting LISA map\nAfter preparing the classes, I could finally plot the LISA map. As with the other maps so far, I used tmap() functions to created this map.\nFor easier analysis, I plotted the LISA map next to the GDPPC map.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI was expecting the 2 standalone orange counties from the GDPPC map (Zixing and Lengshuijiang) to be part of the high-low category. This is because they are relatively more affluent than their neighbors.\nThey were also on the high-low quadrant in the scatterplot. Hence, this result was surprising for me.\nA possible explanation for this is that their GDPPC are just a little bit higher than 60,000, while their neighbors are in the high 50,000s. Visually, they distinct but a closer look at the number might reveal that the values are not really far-off.\n\n\nI also plotted the local Moran’s I values and p-values side by side again to find clues as to why.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs I mentioned, Zixing and Lengshuijiang were part of the high-low category as I originally expected.\nThe p-value provides a sound explanation why. This is because the p-values for these counties are 0.100 or more, which is more than the significance value that was set, which was 0.05.\nWe can say that the p-value map can be use as a filter such that those counties with p-values greater than the significance value are considered insignificant, and only those are not included in this group will be categorized."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#deriving-spatial-weight-matrix",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#deriving-spatial-weight-matrix",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Deriving spatial weight matrix",
    "text": "Deriving spatial weight matrix\n\n\n\n\n\n\nImportant\n\n\n\nThe code chunks used in this part are the same as the ones used in Hands-on Exercise 2A: Spatial Weights and Applications. I didn’t dive deep into these part as these was already learned.\nFor this exercise, binary spatial weights are used.\n\n\nIn order to calculate the spatial weights, I needed to get determine the cut-off distance first. This was done by deriving the centroids and calculating the distances to the nearest neighbor for each county.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\ncoords &lt;- cbind(longitude, latitude)\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\nThese are the same steps as in [Hands-on Exercise 2A: Spatial Weights and Applications](/Hands-on_Ex2/Hands-on_Ex2A.html#determining-cut-off-distance), where we determined the cut-off distance to be 62km.\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nFinally, the spatial weights matrix can be generated.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\nAs in the previous exercise, we could standardize the number of neighbors. This is because denser areas have more neighbors, while rural areas have less.\nThe code chunks below demonstrates how to standardize to 8 neighbors.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#gi-statistics-with-using-fixed-distance-weights",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#gi-statistics-with-using-fixed-distance-weights",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Gi statistics with using fixed distance weights",
    "text": "Gi statistics with using fixed distance weights\nContinuing from the steps above, I looked at two cases. In this part, I used the fixed distance weights.\nFirst, I calculated the Gi statistics using the fixed distance weights.\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nNext is to add the Gi statistics to the hunan data frame.\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nFinally, we could map the Gi values.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is a cluster of counties with high GDDPC in the Eastern part of China.\nA striking observation is that the other orange cities outside of this cluster are cold in the local Gi map. This means that they are surrounded by counties with low GDPPC.\nThere should be caution when interpreting the map on the right as it is not intuitive because the values are actually based on the neighbors, and not the counties themselves."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2C.html#gi-statistics-with-using-adaptive-distance-weights",
    "href": "Hands-on_Ex2/Hands-on_Ex2C.html#gi-statistics-with-using-adaptive-distance-weights",
    "title": "Hands-on Exercise 2C: Local Measures of Spatial Autocorrelation",
    "section": "Gi statistics with using adaptive distance weights",
    "text": "Gi statistics with using adaptive distance weights\nNext we calculate the Gi statistics using adaptive distance weights.\nThe steps are the same as in Gi statistics with using fixed distance weights but instead using the adaptive weights (knn_lw) instead of the fixed weights(wm62_lw).\nFirst was to calculate the Gi statistics.\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\ngi.adaptive\n\n [1]  0.274428799  0.300225037  0.030447697 -0.009771412 -0.033921570\n [6] -0.154780126  4.034649782  2.057586016  4.378892586  1.479129376\n[11]  0.761743842 -0.648205275 -0.773677838  0.589236922  1.040407601\n[16]  0.368526533 -0.604240867 -0.241840937  0.031714037 -0.110547691\n[21]  0.761314356  1.175580259 -0.884714136 -0.860993329 -1.643096490\n[26] -1.290687016 -1.422253022 -0.675281508 -1.719511109 -1.210266137\n[31] -1.300914263 -1.599085669 -1.298761870 -1.836622587  1.637619520\n[36] -0.721435309 -1.958848641 -1.665195897 -1.868014845 -1.183536130\n[41] -0.169560764 -2.084882362 -2.181780084 -2.081025645 -0.499000625\n[46]  2.194733590  2.495469794 -1.695557884 -0.745540634 -1.193763093\n[51] -1.821073681 -1.894085866 -1.570969008 -1.055766446 -1.299966539\n[56] -0.201823610  0.498063690  0.581955247 -0.876827566 -0.955484907\n[61] -0.723004897 -0.790993867 -0.183585082  1.129758266  2.271097895\n[66]  3.047193741  4.995149600  4.022126163 -0.313165513  0.384924896\n[71]  3.018245449  0.561045961  0.210102660  4.365942776 -1.210175378\n[76]  2.391729501 -1.188720061  3.068344267 -0.600223372  1.046676007\n[81] -1.427632954 -0.156355526  1.176546366  3.726230897 -0.327758027\n[86]  2.972571047 -1.009008013 -0.989393051\nattr(,\"internals\")\n              Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.09720587 0.09195402 0.0003662397  0.274428799   7.837551e-01\n [2,] 0.09769063 0.09195402 0.0003651040  0.300225037   7.640055e-01\n [3,] 0.09253816 0.09195402 0.0003680612  0.030447697   9.757100e-01\n [4,] 0.09176695 0.09195402 0.0003665281 -0.009771412   9.922037e-01\n [5,] 0.09130429 0.09195402 0.0003668767 -0.033921570   9.729397e-01\n [6,] 0.08898762 0.09195402 0.0003673079 -0.154780126   8.769947e-01\n [7,] 0.16751891 0.09195402 0.0003507748  4.034649782   5.468380e-05\n [8,] 0.13054918 0.09195402 0.0003518436  2.057586016   3.962989e-02\n [9,] 0.17277103 0.09195402 0.0003406253  4.378892586   1.192839e-05\n[10,] 0.12001759 0.09195402 0.0003599760  1.479129376   1.391057e-01\n[11,] 0.10633361 0.09195402 0.0003563487  0.761743842   4.462129e-01\n[12,] 0.07951853 0.09195402 0.0003680448 -0.648205275   5.168522e-01\n[13,] 0.07714548 0.09195402 0.0003663568 -0.773677838   4.391213e-01\n[14,] 0.10311529 0.09195402 0.0003587953  0.589236922   5.557024e-01\n[15,] 0.11178796 0.09195402 0.0003634216  1.040407601   2.981506e-01\n[16,] 0.09902122 0.09195402 0.0003677535  0.368526533   7.124807e-01\n[17,] 0.08068910 0.09195402 0.0003475655 -0.604240867   5.456835e-01\n[18,] 0.08732412 0.09195402 0.0003665092 -0.241840937   8.089034e-01\n[19,] 0.09256190 0.09195402 0.0003673900  0.031714037   9.747001e-01\n[20,] 0.08984049 0.09195402 0.0003655276 -0.110547691   9.119750e-01\n[21,] 0.10653391 0.09195402 0.0003667585  0.761314356   4.464693e-01\n[22,] 0.11447605 0.09195402 0.0003670374  1.175580259   2.397626e-01\n[23,] 0.07508563 0.09195402 0.0003635312 -0.884714136   3.763108e-01\n[24,] 0.07555112 0.09195402 0.0003629457 -0.860993329   3.892417e-01\n[25,] 0.06043622 0.09195402 0.0003679474 -1.643096490   1.003630e-01\n[26,] 0.06742593 0.09195402 0.0003611483 -1.290687016   1.968122e-01\n[27,] 0.06478946 0.09195402 0.0003647974 -1.422253022   1.549528e-01\n[28,] 0.07912867 0.09195402 0.0003607191 -0.675281508   4.994969e-01\n[29,] 0.05932898 0.09195402 0.0003599915 -1.719511109   8.552135e-02\n[30,] 0.06893033 0.09195402 0.0003618998 -1.210266137   2.261768e-01\n[31,] 0.06724327 0.09195402 0.0003608067 -1.300914263   1.932878e-01\n[32,] 0.06134370 0.09195402 0.0003664310 -1.599085669   1.098016e-01\n[33,] 0.06714525 0.09195402 0.0003648812 -1.298761870   1.940257e-01\n[34,] 0.05762358 0.09195402 0.0003493969 -1.836622587   6.626563e-02\n[35,] 0.12317148 0.09195402 0.0003633868  1.637619520   1.015011e-01\n[36,] 0.07825698 0.09195402 0.0003604615 -0.721435309   4.706417e-01\n[37,] 0.05490035 0.09195402 0.0003578169 -1.958848641   5.013052e-02\n[38,] 0.06013762 0.09195402 0.0003650661 -1.665195897   9.587368e-02\n[39,] 0.05649408 0.09195402 0.0003603425 -1.868014845   6.176000e-02\n[40,] 0.06958160 0.09195402 0.0003573248 -1.183536130   2.365967e-01\n[41,] 0.08870667 0.09195402 0.0003667818 -0.169560764   8.653556e-01\n[42,] 0.05226797 0.09195402 0.0003623370 -2.084882362   3.707998e-02\n[43,] 0.05058836 0.09195402 0.0003594662 -2.181780084   2.912577e-02\n[44,] 0.05256094 0.09195402 0.0003583316 -2.081025645   3.743156e-02\n[45,] 0.08249954 0.09195402 0.0003589829 -0.499000625   6.177789e-01\n[46,] 0.13351191 0.09195402 0.0003585448  2.194733590   2.818271e-02\n[47,] 0.13980943 0.09195402 0.0003677540  2.495469794   1.257905e-02\n[48,] 0.05972453 0.09195402 0.0003613115 -1.695557884   8.996964e-02\n[49,] 0.07779955 0.09195402 0.0003604495 -0.745540634   4.559450e-01\n[50,] 0.06933428 0.09195402 0.0003590369 -1.193763093   2.325707e-01\n[51,] 0.05717238 0.09195402 0.0003647919 -1.821073681   6.859566e-02\n[52,] 0.05561872 0.09195402 0.0003680088 -1.894085866   5.821361e-02\n[53,] 0.06225124 0.09195402 0.0003574860 -1.570969008   1.161898e-01\n[54,] 0.07183294 0.09195402 0.0003632178 -1.055766446   2.910749e-01\n[55,] 0.06738016 0.09195402 0.0003573408 -1.299966539   1.936124e-01\n[56,] 0.08811771 0.09195402 0.0003613143 -0.201823610   8.400546e-01\n[57,] 0.10147288 0.09195402 0.0003652580  0.498063690   6.184392e-01\n[58,] 0.10310390 0.09195402 0.0003670801  0.581955247   5.605968e-01\n[59,] 0.07526754 0.09195402 0.0003621606 -0.876827566   3.805803e-01\n[60,] 0.07370784 0.09195402 0.0003646671 -0.955484907   3.393325e-01\n[61,] 0.07823737 0.09195402 0.0003599264 -0.723004897   4.696769e-01\n[62,] 0.07683091 0.09195402 0.0003655412 -0.790993867   4.289476e-01\n[63,] 0.08846487 0.09195402 0.0003612141 -0.183585082   8.543390e-01\n[64,] 0.11362359 0.09195402 0.0003678997  1.129758266   2.585781e-01\n[65,] 0.13552322 0.09195402 0.0003680335  2.271097895   2.314105e-02\n[66,] 0.15029172 0.09195402 0.0003665206  3.047193741   2.309888e-03\n[67,] 0.18713548 0.09195402 0.0003630845  4.995149600   5.879018e-07\n[68,] 0.16912010 0.09195402 0.0003680793  4.022126163   5.767515e-05\n[69,] 0.08597972 0.09195402 0.0003639373 -0.313165513   7.541549e-01\n[70,] 0.09930460 0.09195402 0.0003646621  0.384924896   7.002931e-01\n[71,] 0.14976364 0.09195402 0.0003668522  3.018245449   2.542429e-03\n[72,] 0.10267460 0.09195402 0.0003651229  0.561045961   5.747662e-01\n[73,] 0.09598415 0.09195402 0.0003679379  0.210102660   8.335875e-01\n[74,] 0.17564058 0.09195402 0.0003674137  4.365942776   1.265756e-05\n[75,] 0.06894940 0.09195402 0.0003613546 -1.210175378   2.262116e-01\n[76,] 0.13777971 0.09195402 0.0003671080  2.391729501   1.676920e-02\n[77,] 0.06924543 0.09195402 0.0003649397 -1.188720061   2.345498e-01\n[78,] 0.15052389 0.09195402 0.0003643681  3.068344267   2.152485e-03\n[79,] 0.08060684 0.09195402 0.0003573967 -0.600223372   5.483574e-01\n[80,] 0.11191592 0.09195402 0.0003637301  1.046676007   2.952490e-01\n[81,] 0.06473996 0.09195402 0.0003633737 -1.427632954   1.533975e-01\n[82,] 0.08896972 0.09195402 0.0003643008 -0.156355526   8.757528e-01\n[83,] 0.11452640 0.09195402 0.0003680752  1.176546366   2.393766e-01\n[84,] 0.15719339 0.09195402 0.0003065349  3.726230897   1.943644e-04\n[85,] 0.08568420 0.09195402 0.0003659344 -0.327758027   7.430946e-01\n[86,] 0.14892272 0.09195402 0.0003672891  2.972571047   2.953169e-03\n[87,] 0.07271488 0.09195402 0.0003635650 -1.009008013   3.129708e-01\n[88,] 0.07310269 0.09195402 0.0003630331 -0.989393051   3.224709e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = knn_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThen attaching the Gi statistics to the hunan data frame.\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\nAnd finally, mapping it.\n\ngdppc&lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWith this plot, the clusters are a lot more clearer and we can clearly see that the East side is the hot spot in terms on GDDPC.\nHowever, we can also notice that on the Southwest, the most affluent county is a cold spot in the Gi map because it is surrounded by less affluent counties.\n\n\nAnother observation is that the Gi map using adaptive distance weights are less scattered and bigger compared to the Gi map using fixed distance weights."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "",
    "text": "This hands-on exercise covers Chapter 8: Spatial Weights and Applications\nI learned about the following:\n\nCalculating spatial weights\nCalculating spatially lagged variables"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html#preparing-the-data-sets",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html#preparing-the-data-sets",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "Preparing the data sets",
    "text": "Preparing the data sets\nData sets used on this exercise were downloaded from E-learn.\n\nGeospatial\n\nHunan county boundary layer (shp format)\n\n\n\nAspatial\n\nHunan’s local development indicators in 2012 (csv format)\n\nNext, is putting them under the Hands-on_Ex2 directory, with the following file structure:\nHands-on_Ex2\n└── data\n    ├── aspatial\n    │   └── Hunan_2012.csv\n    └── geospatial\n        ├── Hunan.dbf\n        ├── Hunan.prj\n        ├── Hunan.qpj\n        ├── Hunan.shp\n        └── Hunan.shx"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html#installing-r-packages",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html#installing-r-packages",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "Installing R packages",
    "text": "Installing R packages\nI used the code below to install the R packages used in the exercise:\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html#importing-data-sets",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html#importing-data-sets",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "Importing data sets",
    "text": "Importing data sets\nI used st_read() to import the geospatial shp data.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\",\n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/Hands-on_Ex2/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the previous exercises, we transformed the data with EPSG:3414. However, that is not applicable for this data set as we are not working with Singapore 🇸🇬 data set.\n\n\nAs with the previous exercises, I used read_csv() to import aspatial csv data.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html#joining-the-data-sets",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html#joining-the-data-sets",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "Joining the data sets",
    "text": "Joining the data sets\nIn the exercise, we have to join the 2 data sets using this code:\n\nhunan &lt;- left_join(hunan, hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\n\n\nNote\n\n\n\nWe did not specify any columns to join by but left_join detected common column, County, so it joined the 2 data sets by this column.\nAt the end of this, we are left with 7 columns, which includes GDPPC from the aspatial data, which contains data for Gross Domestic Product per Capita."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html#computing-queen-contiguity-based-neighbors",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html#computing-queen-contiguity-based-neighbors",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "Computing (QUEEN) contiguity based neighbors",
    "text": "Computing (QUEEN) contiguity based neighbors\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThis showed that there are 2 least connected regions, 30 and 65. Furthermore, there is 1 county is most connected, 85.\nBelow I analyzed these counties of interest\n\nLeast connected counties\nFirst, I checked the names of the least connected counties.\n\nhunan$County[c(30, 65)]\n\n[1] \"Xinhuang\" \"Linxiang\"\n\n\nThe least connected counties are Xinhuang in the West and Linxiang in the Northeast.\nIt makes sense for these counties to be least connected as they are counties that only have 1 neighbors each, according to the map.\n\nhunan$County[c(\n  wm_q[[30]],\n  wm_q[[65]]\n)]\n\n[1] \"Zhijiang\" \"Yueyang\" \n\n\nXinhuang borders Zhijiang to the East, while Linxiang borders Yueyang to the Southwest.\n\n\nMost connected county\n\nhunan$County[85]\n\n[1] \"Taoyuan\"\n\n\nThe most connected county is Taoyuan with 11 neighbors. It’s neighbors are:\n\nhunan$County[wm_q[[85]]]\n\n [1] \"Anxiang\"  \"Hanshou\"  \"Jinshi\"   \"Linli\"    \"Shimen\"   \"Yuanling\"\n [7] \"Anhua\"    \"Nan\"      \"Cili\"     \"Sangzhi\"  \"Taojiang\"\n\n\nThis makes perfect sense as Taoyuan is a relatively large, inner county."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html#creating-rook-contiguity-based-neighbors",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html#creating-rook-contiguity-based-neighbors",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "Creating (ROOK) contiguity based neighbors",
    "text": "Creating (ROOK) contiguity based neighbors\n\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThis operation resulted in 8 fewer non-zero links. The most connected region, Taoyuan, has one less neighbor. However, the least connected regions stayed the same.\n\nsetdiff(hunan$County[wm_q[[85]]], hunan$County[wm_r[[85]]])\n\n[1] \"Nan\"\n\n\nNan is not considered a neighbor of Taoyuan using the Rook method. I check the documentation of poly2nb() to understand why.\n\n\n\n\n\n\nNote\n\n\n\nWhen setting queen=false, it requires boundaries to be more that just one point. On the other hand, with queen=true, it requires the objects to shared only a single point.\nAs such, having 8 less links means 8 pairs of counties only share a single point in their boundaries.\n\n\nLooking at the map, Nan indeed only touches Taoyuan at a single point:"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html#visualising-contiguity-weights",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html#visualising-contiguity-weights",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "Visualising contiguity weights",
    "text": "Visualising contiguity weights\nTo plot the contiguity, we need to get the centroids of each county region. To get this for a single county, the following code can be used.\n\nhunan$geometry[1] %&gt;% st_centroid(.x)\n\nGeometry set for 1 feature \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 112.1531 ymin: 29.44346 xmax: 112.1531 ymax: 29.44346\nGeodetic CRS:  WGS 84\n\n\nHowever, we needed to plot each longitude and latitude separately and create a new data frame for centroid coordinates from those. In order to do that, I copied the code chunks from the exercise.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\ncoords &lt;- cbind(longitude, latitude)\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\nPlotting contiguity based neighbors map\nI plotted the Queen and Rooks maps on the same plot instead of the recommended way in the exercise. This is so I could see which neighbors where present in the Queen method but were not present in the Rook method.\nThey are the red lines in the map.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\", main=\"Queen Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"blue\", main=\"Rook Contiguity\")"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html#determining-cut-off-distance",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html#determining-cut-off-distance",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "Determining cut-off distance",
    "text": "Determining cut-off distance\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\n\n\n\n\n\n\nNote\n\n\n\nThis means that the (centroids of) closest neighbors are 24.79 km apart while the farthest neighbors are 61.79 km apart.\nTo ensure that all counties will have at least one neighbors, we set the cut-off distance to the maximum distance, or 61.79 km."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html#computing-fixed-distance-weight-matrix",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html#computing-fixed-distance-weight-matrix",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "Computing fixed distance weight matrix",
    "text": "Computing fixed distance weight matrix\nTo figure out the neighbors within the 62km distance (rounded out from the previous result), we use dnearneigh() .\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\n\n\n\n\n\nTip\n\n\n\nThe average number of links here correspond to the average number of neighbors each county has.\nThat means for every county in China, there are 3.681818 other counties within 62 km of them, on average.\n\n\nThe example below gives a glimpse of neighbors each county has.\n\nstr(wm_d62)\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnother observation here is that Taoyuan, which had 11 contiguity-based neighbors, now only has 2 neighbors when using distance-based methods.\n\nwm_d62[88]\n\n[[1]]\n[1] 59 87"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html#plotting-distance-based-matrix",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html#plotting-distance-based-matrix",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "Plotting distance-based matrix",
    "text": "Plotting distance-based matrix\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n💡 I found out that plotting the red lines first before the black lines would just display black lines.\nThe technique of rendering the superset before the subset is a good technique to display the difference in the different plots.\nAfter realizing this, I applied the same technique in the Queen and Rook maps in [##Visualising contiguity weights]."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html#computing-adaptive-distance-weight-matrix",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html#computing-adaptive-distance-weight-matrix",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "Computing adaptive distance weight matrix",
    "text": "Computing adaptive distance weight matrix\nThere are cases in which knowing the k-nearest neighbors is useful. It can be done by passing k to knearneigh:\n\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nPlotting this in a map and overlapping with the wm_d62 map, we can see that more neighbor links (in red) were added so that each county has 6 neighbors.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, add=TRUE, col=\"red\", length=0.08)\nplot(wm_d62, coords, add=TRUE)"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html#row-standardized-weights-matrix",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html#row-standardized-weights-matrix",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "Row-standardized weights matrix",
    "text": "Row-standardized weights matrix\nNext we assign the weight of 1/(# of neighbors) to each neighbor.\n\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nNext I inspected some weights values to check if the results are consistent with our expectations.\n\nrswm_q$weights[c(1, 10, 30, 85)]\n\n[[1]]\n[1] 0.2 0.2 0.2 0.2 0.2\n\n[[2]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n[[3]]\n[1] 1\n\n[[4]]\n [1] 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909\n [7] 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909\n\n\nAs expected, their values are equal to 1/(# of neighbors).\nNext, the same was also done to derive a row standardised distance weight matrix.\n\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\nChecking some of the matrix values:\n\nrswm_ids$weights[c(1, 10, 30, 85)]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[3]]\n[1] 0.02090587\n\n[[4]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n\n\n\n\n\n\n\nNote\n\n\n\nResults seem to be the same as when using nbdists() and lapply() in Weights based on IDW.\n\n\nFinally, we get some summary of the values.\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html#spatial-lag-with-row-standardized-weights",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html#spatial-lag-with-row-standardized-weights",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "Spatial lag with row-standardized weights",
    "text": "Spatial lag with row-standardized weights\nFirst, I computed the spatially lagged values for each polygon.\n\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nThe spatially lagged GDPPC values were appended to the Hunan data using the code below:\n\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, the GDPPC and spatial lag GDPPC were plotted for comparison\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe spatial correlation seems to appear more positive among counties in the East."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html#spatial-lag-as-a-sum-of-neighboring-values",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html#spatial-lag-as-a-sum-of-neighboring-values",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "Spatial lag as a sum of neighboring values",
    "text": "Spatial lag as a sum of neighboring values\nThe spatial lag as a sum of neighboring values was calculated by assigning binary weights.\n\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nThen, these weights were applied to the GDPPC values, and appending the lag_sum data to thehunan data set.\n\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\nhunan &lt;- left_join(hunan, lag.res)\n\nLastly, I plotted the map.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe lag_sum plot looks more scattered compared to the lag plot."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html#spatial-window-average",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html#spatial-window-average",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "Spatial window average",
    "text": "Spatial window average\nFirst, I added the diagonal element to the neighbor list.\n\nwm_qs &lt;- include.self(wm_q)\n\nNext, I calculated the weights for the new list.\n\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nThen, I creates the lag variable from the weight structure and GDPPC variable.\n\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, hunan$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nSubsequently, I processed the data for further analysis.\n\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\nI inspected the different lag values to figure out if there was any pattern. It was hard to do by eye on this table.\n\nhunan %&gt;%\n  select(\"County\", \"lag GDPPC\", \"lag_window_avg GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nAfter all the data processing, I could finally plot the spatial window average.\n\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe range of values became narrower, from 10,000 - 60,000 to 10,000 - 50,000. Furthermore, the map looks “cleaner” for the lag_window_average."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2A.html#spatial-window-sum",
    "href": "Hands-on_Ex2/Hands-on_Ex2A.html#spatial-window-sum",
    "title": "Hands-on Exercise 2A: Spatial Weights and Applications",
    "section": "Spatial window sum",
    "text": "Spatial window sum\nFirst, I added the diagonal element to the neighbor list.\n\nwm_qs &lt;- include.self(wm_q)\n\nThen, binary weights were calculated from this new list.\n\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\n\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, data was processed for further analysis.\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\nNext, I compared the lag_sum and w_sum values to check for patterns. Hard to see in this table format.\n\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nFinally, I plotted the maps.\n\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3.html",
    "href": "Hands-on_Ex3/Hands-on_Ex3.html",
    "title": "Hands-on Exercise 3: Processing and Visualizing Flow Data",
    "section": "",
    "text": "We will visualize flow data\n\n\n\n\n\n\nI am running through most of the code here as the wrangling steps are very similar to the wrangling done in Take Home Ex1. No need to analyze each and every step as I already learned those.\n\n\n\n\n\nWe will load the library used for this exercise.\n\npacman::p_load(tmap, sf, DT, stplanr,\n               performance,\n               ggpubr, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3.html#getting-started",
    "href": "Hands-on_Ex3/Hands-on_Ex3.html#getting-started",
    "title": "Hands-on Exercise 3: Processing and Visualizing Flow Data",
    "section": "",
    "text": "We will load the library used for this exercise.\n\npacman::p_load(tmap, sf, DT, stplanr,\n               performance,\n               ggpubr, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3.html#importing-of-data",
    "href": "Hands-on_Ex3/Hands-on_Ex3.html#importing-of-data",
    "title": "Hands-on Exercise 3: Processing and Visualizing Flow Data",
    "section": "Importing of data",
    "text": "Importing of data\n\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202310.csv\")\n\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE) \n\n\nhead(odbus)\n\n# A tibble: 6 × 7\n  YEAR_MONTH DAY_TYPE   TIME_PER_HOUR PT_TYPE ORIGIN_PT_CODE DESTINATION_PT_CODE\n  &lt;chr&gt;      &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;fct&gt;          &lt;fct&gt;              \n1 2023-10    WEEKENDS/…            16 BUS     04168          10051              \n2 2023-10    WEEKDAY               16 BUS     04168          10051              \n3 2023-10    WEEKENDS/…            14 BUS     80119          90079              \n4 2023-10    WEEKDAY               14 BUS     80119          90079              \n5 2023-10    WEEKDAY               17 BUS     44069          17229              \n6 2023-10    WEEKENDS/…            17 BUS     20281          20141              \n# ℹ 1 more variable: TOTAL_TRIPS &lt;dbl&gt;"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3.html#extracting-the-study-data",
    "href": "Hands-on_Ex3/Hands-on_Ex3.html#extracting-the-study-data",
    "title": "Hands-on Exercise 3: Processing and Visualizing Flow Data",
    "section": "Extracting the study data",
    "text": "Extracting the study data\n\nodbus6_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE,\n           DESTINATION_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\ndatatable(odbus6_9)\n\n\n\n\n\n\n\nwrite_rds(odbus6_9, \"data/rds/odbus6_9.rds\")\nodbus6_9 &lt;- read_rds(\"data/rds/odbus6_9.rds\")"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3.html#importing-geospatial-data",
    "href": "Hands-on_Ex3/Hands-on_Ex3.html#importing-geospatial-data",
    "title": "Hands-on Exercise 3: Processing and Visualizing Flow Data",
    "section": "Importing geospatial data",
    "text": "Importing geospatial data\n\nbusstop &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/Hands-on_Ex3/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `/Users/kjcpaas/Documents/Grad School/ISSS624/Project/ISSS624/Hands-on_Ex3/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nmpsz &lt;- write_rds(mpsz, \"data/rds/mpsz.rds\")"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3.html#combining-busstop-and-mpsz",
    "href": "Hands-on_Ex3/Hands-on_Ex3.html#combining-busstop-and-mpsz",
    "title": "Hands-on Exercise 3: Processing and Visualizing Flow Data",
    "section": "Combining Busstop and mpsz",
    "text": "Combining Busstop and mpsz\n\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\n\n\ndatatable(busstop_mpsz)\n\n\n\n\n\n\n\nwrite_rds(busstop_mpsz, \"data/rds/busstop_mpsz.rds\")  \n\n\nod_data &lt;- left_join(odbus6_9 , busstop_mpsz,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C,\n         DESTIN_BS = DESTINATION_PT_CODE)\n\n\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\nod_data &lt;- unique(od_data)\n\n\nod_data &lt;- left_join(od_data , busstop_mpsz,\n            by = c(\"DESTIN_BS\" = \"BUS_STOP_N\")) \n\n\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\nod_data &lt;- unique(od_data)\n\n\nod_data &lt;- od_data %&gt;%\n  rename(DESTIN_SZ = SUBZONE_C) %&gt;%\n  drop_na() %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;%\n  summarise(MORNING_PEAK = sum(TRIPS))\n\n\nwrite_rds(od_data, \"data/rds/od_data.rds\")\nod_data &lt;- read_rds(\"data/rds/od_data.rds\")"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3.html#removing-intra-zonal-flows",
    "href": "Hands-on_Ex3/Hands-on_Ex3.html#removing-intra-zonal-flows",
    "title": "Hands-on Exercise 3: Processing and Visualizing Flow Data",
    "section": "Removing intra-zonal flows",
    "text": "Removing intra-zonal flows\n\nod_data1 &lt;- od_data[od_data$ORIGIN_SZ!=od_data$DESTIN_SZ,]"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3.html#creating-desire-lines",
    "href": "Hands-on_Ex3/Hands-on_Ex3.html#creating-desire-lines",
    "title": "Hands-on Exercise 3: Processing and Visualizing Flow Data",
    "section": "Creating desire lines",
    "text": "Creating desire lines\nIn this code chunk below, od2line() of stplanr package is used to create the desire lines.\n\nflowLine &lt;- od2line(flow = od_data1, \n                    zones = mpsz,\n                    zone_code = \"SUBZONE_C\")"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3.html#visualizing-the-desire-lines",
    "href": "Hands-on_Ex3/Hands-on_Ex3.html#visualizing-the-desire-lines",
    "title": "Hands-on Exercise 3: Processing and Visualizing Flow Data",
    "section": "Visualizing the desire lines",
    "text": "Visualizing the desire lines\nTo visualise the resulting desire lines, the code chunk below is used.\n\ntm_shape(mpsz) +\n  tm_polygons() +\nflowLine %&gt;%  \ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5,\n           col = \"red\")\n\n\n\n\n\ntm_shape(mpsz) +\n  tm_polygons() +\nflowLine %&gt;%  \n  filter(MORNING_PEAK &gt;= 5000) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5,\n           col = \"red\")"
  }
]