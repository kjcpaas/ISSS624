---
title: "In-class Exercise 3: Calibrating Spatial Interaction Models with R"
author: "Kristine Joy Paas"
date: "1 December 2023"
date-modified: "last-modified"
format: html
execute: 
  echo: true
  eval: true
  warning: false
---

# Overview

# Getting Started

-   reshape2 - will handle matrix form better

```{r}
pacman::p_load(tmap, sf, sp, DT, performance, reshape2, ggpubr, units, tidyverse)
```

# Computing distance Matrix

Contains distance from the centroid of a region to the centroid of the other region.

```{r}
mpsz <- read_rds("data/rds/mpsz.rds")
mpsz
```

## **Converting from sf data.table to SpatialPolygonsDataFrame**

```{r}
mpsz_sp <- as(mpsz, "Spatial")
mpsz_sp
```

Sample operation in mpsz_sp on how to read the data table. (Check after class how this is done because it didn't work. I might have copied wrongly)

```{r eval=FALSE}
mpsz_sp_selected <- mpsz_sp %>%
  selected(mpsz@data$SUBZONE)
```

## Computing the distance matrix

```{r}
dist <- spDists(mpsz_sp, 
                longlat = FALSE)
head(dist, n=c(10, 10))
```

## **Labelling column and row heanders of a distance matrix**

```{r}
sz_names <- mpsz$SUBZONE_C
```

```{r}
colnames(dist) <- paste0(sz_names)
rownames(dist) <- paste0(sz_names)
```

## **Pivoting distance value by SUBZONE_C**

This will generate a list of pair of location1 and location2.

Number of rows should be:

$$
n_{loc}^2 = 332^2 = 110224
$$

```{r}
distPair <- melt(dist) %>%
  rename(dist = value)
head(distPair, 10)
```

```{r}
nrow(distPair)
```

# **Updating intra-zonal distances**

First we need to find out the minimum value for **non-zero distances**

```{r}
distPair %>%
  filter(dist > 0) %>%
  summary()
```

Next, a constant distance value of 50m is added into intra-zones distance.

::: callout-important
## Why 50?

*Need to ask this on Piazza since I do not fully understand why this has to be specifically 50 m.*

Min dist is **173.8 ,**

I understand intra-zone data **must be less than this**.

However why 50 specifically?

173.8m is shortest distance centroid to centroid so centroid to edge is **86.9 m** (assuming equal edge to edge data)

*Is 50 m just an arbitrary value less than this?*
:::

```{r}
distPair$dist[distPair$dist == 0] <- 50
```

```{r}
summary(distPair)
```

Rename column names

```{r}
distPair <- distPair %>%
  rename(orig = Var1,
         dest = Var2)
head(distPair)
```

```{r}
write_rds(distPair, "data/rds/distPair.rds") 
```

# Preparing Flow Data

```{r}
od_data <- read_rds("data/rds/od_data.rds")
```

Next, we will compute the total passenger trip between and within planning subzones by using the code chunk below. The output is all *flow_data*.

```{r}
flow_data <- od_data %>%
  group_by(ORIGIN_SZ, DESTIN_SZ) %>% 
  summarize(TRIPS = sum(MORNING_PEAK)) 
head(flow_data, n = 10)
```

## **Separating intra-flow from passenger volume df**

```{r}
flow_data$FlowNoIntra <- ifelse(
  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, 
  0, flow_data$TRIPS)
flow_data$offset <- ifelse(
  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, 
  0.000001, 1)
```

## **Combining passenger volume data with distance value**

```{r}
flow_data$ORIGIN_SZ <- as.factor(flow_data$ORIGIN_SZ)
flow_data$DESTIN_SZ <- as.factor(flow_data$DESTIN_SZ)
```

```{r}
flow_data1 <- flow_data %>%
  left_join (distPair,
             by = c("ORIGIN_SZ" = "orig",
                    "DESTIN_SZ" = "dest"))
```

# **Preparing Origin and Destination Attributes**

## **Importing population data**

```{r}
pop <- read_csv("data/aspatial/pop.csv")
```

## **Geospatial data wrangling**

```{r}
pop <- pop %>%
  left_join(mpsz,
            by = c("PA" = "PLN_AREA_N",
                   "SZ" = "SUBZONE_N")) %>%
  select(1:6) %>%
  rename(SZ_NAME = SZ,
         SZ = SUBZONE_C)
```

## Preparing origin attribute

```{r}
flow_data1 <- flow_data1 %>%
  left_join(pop,
            by = c(ORIGIN_SZ = "SZ")) %>%
  rename(ORIGIN_AGE7_12 = AGE7_12,
         ORIGIN_AGE13_24 = AGE13_24,
         ORIGIN_AGE25_64 = AGE25_64) %>%
  select(-c(PA, SZ_NAME))
```

## Preparing destination attribute

```{r}
flow_data1 <- flow_data1 %>%
  left_join(pop,
            by = c(DESTIN_SZ = "SZ")) %>%
  rename(DESTIN_AGE7_12 = AGE7_12,
         DESTIN_AGE13_24 = AGE13_24,
         DESTIN_AGE25_64 = AGE25_64) %>%
  select(-c(PA, SZ_NAME))
```

```{r}
write_rds(flow_data1, "data/rds/SIM_data.rds")
```

# **Calibrating Spatial Interaction Models**

## **Importing the modelling data**

```{r}
SIM_data <- read_rds("data/rds/SIM_data.rds")
```

## **Visualising the dependent variable**

Dependent variable = `TRIPS`

```{r}
ggplot(data = SIM_data,
       aes(x = TRIPS)) +
  geom_histogram()
```

This is highly skewed, not resembling "bell curve" distribution.

```{r}
ggplot(data = SIM_data,
       aes(x = dist,
           y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)
```

Plotting `log` axis resembles linear relationship more

```{r}
ggplot(data = SIM_data,
       aes(x = log(dist),
           y = log(TRIPS))) +
  geom_point() +
  geom_smooth(method = lm)
```

## **Checking for variables with zero values**

Poisson regression uses `log` of values and since `log(0)` is `undefined`, we must ensure that there are no zeroes in `SIM_data`

```{r}
summary(SIM_data)
```

The print report above reveals that variables `ORIGIN_AGE7_12`, `ORIGIN_AGE13_24`, `ORIGIN_AGE25_64`,`DESTIN_AGE7_12`, `DESTIN_AGE13_24`, `DESTIN_AGE25_64` consist of 0 values.

In view of this, code chunk below will be used to replace zero values to **0.99**.

```{r}
SIM_data$ORIGIN_AGE7_12[SIM_data$ORIGIN_AGE7_12 == 0] <- 0.99
SIM_data$ORIGIN_AGE13_24[SIM_data$ORIGIN_AGE13_24 == 0] <- 0.99
SIM_data$ORIGIN_AGE25_64[SIM_data$ORIGIN_AGE25_64 == 0] <- 0.99
SIM_data$DESTIN_AGE7_12[SIM_data$DESTIN_AGE7_12 == 0] <- 0.99
SIM_data$DESTIN_AGE13_23[SIM_data$DESTIN_AGE13_24 == 0] <- 0.99
SIM_data$DESTIN_AGE25_64[SIM_data$DESTIN_AGE25_64 == 0] <- 0.99
```

Verify the values

```{r}
summary(SIM_data)
```

## **Unconstrained Spatial Interaction Model**

The general formula of Unconstrained Spatial Interaction Model

$$
\lambda_{ij} = exp(k + \mu lnV_i + \alpha lnW_j + \beta ln d_{ij})
$$

Calibrate model

```{r}
uncSIM <- glm(formula = TRIPS ~ 
                log(ORIGIN_AGE25_64) + 
                log(DESTIN_AGE25_64) +
                log(dist),
              family = poisson(link = "log"),
              data = SIM_data,
              na.action = na.exclude)
uncSIM
```

## **R-squared function**

*Read on this since I do not fully understand what this is for. i know we uses function since this is a model*

```{r}
CalcRSquared <- function(observed,estimated){
  r <- cor(observed,estimated)
  R2 <- r^2
  R2
}
```

```{r}
CalcRSquared(uncSIM$data$TRIPS, uncSIM$fitted.values)
```

```{r}
r2_mcfadden(uncSIM)
```

## **Origin (Production) constrained SIM**

The general formula of Origin Constrained Spatial Interaction Model

$$
\lambda_{ij} = exp(k + \mu_i + \alpha lnW_j + \beta ln d_{ij})
$$

*Code below takes a while to run so be **patient***

```{r}
#| eval: false
# Set eval = true when need to recompute
orcSIM <- glm(formula = TRIPS ~ 
                 ORIGIN_SZ +
                 log(DESTIN_AGE25_64) +
                 log(dist),
              family = poisson(link = "log"),
              data = SIM_data,
              na.action = na.exclude)
write_rds(orcSIM, "data/rds/orcSIM.rds")
```

```{r}
orcSIM <- read_rds("data/rds/orcSIM.rds")
summary(orcSIM)
```

```{r}
CalcRSquared(orcSIM$data$TRIPS, orcSIM$fitted.values)
```

## **Destination constrained**

The general formula of Destination Constrained Spatial Interaction Model

$$
\lambda_{ij} = exp(k + \mu lnV_i + \alpha_j + \beta ln d_{ij})
$$

*This takes a while to run so be **patient***

```{r}
#| eval: false
# Set eval = true when need to recompute
decSIM <- glm(formula = TRIPS ~ 
                DESTIN_SZ + 
                log(ORIGIN_AGE25_64) + 
                log(dist),
              family = poisson(link = "log"),
              data = SIM_data,
              na.action = na.exclude)
write_rds(decSIM, "data/rds/decSIM.rds") # This is 52 MB file
```

```{r}
decSIM <-read_rds("data/rds/decSIM.rds")
summary(decSIM)
```

```{r}
CalcRSquared(decSIM$data$TRIPS, decSIM$fitted.values)
```

## **Doubly constrained**

The general formula of Doubly Constrained Spatial Interaction Model

$$
\lambda_{ij} = exp(k + \mu_i + \alpha_j + \beta ln d_{ij})
$$

*This code chunk takes a while to run (longer than previous ones) so be **extrapatient*****.** I'll save thre results so no need to rerun it when I rered

```{r}
#| eval: false
# Set eval: true when we need to recompute
dbcSIM <- glm(formula = TRIPS ~ 
                ORIGIN_SZ + 
                DESTIN_SZ + 
                log(dist),
              family = poisson(link = "log"),
              data = SIM_data,
              na.action = na.exclude)
# Save so we do not recompute again when rendering
write_rds(dbcSIM, "data/rds/dbcSIM.rds") # 82MB file
```

```{r}
dbcSIM <- read_rds("data/rds/dbcSIM.rds")
summary(dbcSIM)
```

```{r}
CalcRSquared(dbcSIM$data$TRIPS, dbcSIM$fitted.values)
```

## **Model comparison**

Compare performance.

```{r}
model_list <- list(unconstrained=uncSIM,
                   originConstrained=orcSIM,
                   destinationConstrained=decSIM,
                   doublyConstrained=dbcSIM)
```

```{r}
compare_performance(model_list,
                    metrics = "RMSE")
```

Best model has the **smallest RMSE** so it is **doubleConstrained**

## **Visualising fitted**

Unconstrained

```{r}
df <- as.data.frame(uncSIM$fitted.values) %>%
  round(digits = 0)
```

```{r}
SIM_data <- SIM_data %>%
  cbind(df) %>%
  rename(uncTRIPS = "uncSIM$fitted.values")
```

Origin constrained

```{r}
df <- as.data.frame(orcSIM$fitted.values) %>%
  round(digits = 0)
```

```{r}
SIM_data <- SIM_data %>%
  cbind(df) %>%
  rename(orcTRIPS = "orcSIM$fitted.values")
```

Destination constrained

```{r}
df <- as.data.frame(decSIM$fitted.values) %>%
  round(digits = 0)
```

```{r}
SIM_data <- SIM_data %>%
  cbind(df) %>%
  rename(decTRIPS = "decSIM$fitted.values")
```

Doubly constrained

```{r}
df <- as.data.frame(dbcSIM$fitted.values) %>%
  round(digits = 0)
```

```{r}
SIM_data <- SIM_data %>%
  cbind(df) %>%
  rename(dbcTRIPS = "dbcSIM$fitted.values")
```

Generating plots

```{r}
unc_p <- ggplot(data = SIM_data,
                aes(x = uncTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

orc_p <- ggplot(data = SIM_data,
                aes(x = orcTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

dec_p <- ggplot(data = SIM_data,
                aes(x = decTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

dbc_p <- ggplot(data = SIM_data,
                aes(x = dbcTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

ggarrange(unc_p, orc_p, dec_p, dbc_p,
          ncol = 2,
          nrow = 2)
```

Double constrained looks less scattered and more fitted to the line, consistent with RMSE results

# Reflections

-   To be honest I was quite distracted in class earlier as I was revising my Take home ex 1, and my notes are not so coherent. I will revise after I finish my Take Home Ex 1

-   However, I find this lesson interesting as we finally get to learn some flow analysis. I had some questions I wanted to answer during Take Home Exercise that I discovered couldn't be provided by EHSA and LISA e.g.

    -   Were people really coming from home on mornings whether it is weekday or weekend, or

    -   Is the flow of people coming home in the evenings during weekdays more scattered through the rest of the day compare to traffic of people going out of their houses in the morning?

        -   My hypothesis is that in the morning, people go straight to office/school but in the evening people do some recreational activities before going home

-   Lastly, I was a Math major in my undergrad so this activity reminded me why I fell in love with Math in the first place. I want to explore about the models more and I would've enjoyed the class if we had more time. I know that DT students in MITB have a reputations of do not like technical classes like this but I am enjoying and learning a lot so far although it requires a lot of effort. I guess my Math training prepared me for the persistence required for rigorous learning like this.
